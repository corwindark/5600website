---
title: "ARMA ARIMA SARIMA MODELS"
---

Loading packages

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```


``` {r message = FALSE}
library(tidyverse)
library(quantmod)
library(forecast)
library(tseries)
library(fpp2)
library(lubridate)
```



Bringing the data into this tab as well:

``` {r}
#| code-summary: "Show Code"

spyIn <- quantmod::getSymbols("SPY", from = as.Date("2021/01/01"), to = as.Date("2023/09/30"), periodicity = "daily", src = "yahoo", auto.assign = FALSE)
qqqIn <- quantmod::getSymbols("QQQ", from = as.Date("2021/01/01"), to = as.Date("2023/09/30"), periodicity = "daily", src = "yahoo", auto.assign = FALSE)
iwmIn <- quantmod::getSymbols("IWM", from = as.Date("2021/01/01"), to = as.Date("2023/09/30"), periodicity = "daily", src = "yahoo", auto.assign = FALSE)


spyIn$spyRange <- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open
qqqIn$qqqRange <- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open
iwmIn$iwmRange <- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open


diff1SPY <- diff(spyIn$spyRange)
diff1QQQ <- diff(qqqIn$qqqRange)
diff1IWM <- diff(iwmIn$iwmRange)

```



## Stationarity of the Time Series

Based on previous results, and the fact that I am using "pseudo-differenced data" in that I am taking the percentage range in prices, in addition to a single differencing, means that the time series are stationary. As evidenced by Dickey-Fuller Tests.


## Building ARIMA Model

### QQQ
Since I did some of this work with SPY data on the EDA tab, I will focus on QQQ range data here, after it has been differenced.

``` {r}
#print(diff1QQQ)
acf(diff1QQQ, na.action = na.exclude, main = "Differenced QQQ Daily Range ACF Plot")
pacf(diff1QQQ, na.action = na.exclude, main = "Differenced QQQ Daily Range PACF Plot")

```

Based on these charts the order I would pick for QQQ is: ARIMA(2,1,0)

``` {r}

modelQQQ1 <- arima(diff1QQQ, order = c(2,1,0))
#summary(modelQQQ1)
```

5. The equation for an ARIMA model with the above formula is x = -1.0034x(t-1) - 0.4832x(t-2) + error.

6. Model Diagnostic:
``` {r}

stats::tsdiag(modelQQQ1)

```

The Ljung Box statistics look good, although the ACF of the residuals does have 1 significant term.I originally tried a (4,1,2) model, however the ljung box statistics were highly correlated, and I suspected overfitting. After reducing the parametrization greatly, the new model performed mnuch better.

7. Comparing the Auto ARIMA method:
``` {r}
autoQQQ <- auto.arima(diff1QQQ)
#summary(autoQQQ)
```

The auto.arima method chose an ARIMA(1,0,1) model. However, this model did not perform as well in terms of AIC, with the Auto arima model having a score of -4799 while my model had a score of -4292.


8. Forecasting with my model

``` {r}
plot(forecast(modelQQQ1, 10), xlim = c(650,750))   

```


Forecasting with auto arima model
``` {r}
plot(forecast(autoQQQ, 10), xlim = c(650,750))
```

Overall, my model has a slightly more dynamic prediction than the auto arima function, which quicly levels out to 0. However, my model also has a much wider uncertainty band.


9. Compare ARIMA model with benchmarks
```{r}

naiveModelQQQ <- naive(diff1QQQ, h=1)
snaiveModelQQQ <- snaive(diff1QQQ, h=1)

#summary(naiveModelQQQ)
#summary(snaiveModelQQQ)
#summary(modelQQQ1)

```

I fit a naive and seasonal naive model. On RMSE my model had the best performance, with 0.011, while the naive and snaive models had 0.017 rmse each (since there was no seasonal period I realized they were the same model). On MAE my arima model had 0.008 while the seasonal naive models had 0.0122.

Let's compare forecasts:
```{r}
plot(forecast(modelQQQ1, 10), xlim = c(650,750)) 
```

```{r}
plot(forecast(naiveModelQQQ, 10), xlim = c(650,750))   
```


Here, the naive method can only forecast 1 observation into the future, since the seasonal period is one. Which is an advantage to my model, but realistically means the naive model should be evaluated with cross validation.

### SPY


Now, let's gather ARIMA models for the intraday range. First we declare the helper function
```{r}
arimaResults <- function(data) {
    
    i=1
    temp= data.frame()
    ls=matrix(rep(NA,6*100),nrow=100) # roughly nrow = 3x4x2


    for (p in 1:6)# p=1,2,3 : 3
    {
        for(q in 1:6)# q=1,2,3,4 :4
        {
            for(d in 1:2) {
                
                if(p-1+d - 1+q-1<=8) #usual threshold
                {
                    
                    model<- Arima(data,order=c(p-1,d-1,q-1),include.drift=FALSE) 
                    ls[i,]= c(p-1,d-1,q-1,model$aic,model$bic,model$aicc)
                    i=i+1
                    #print(i)
                    
                }

            }
                
        }
    }


    temp= as.data.frame(ls)
    names(temp)= c("p","d","q","AIC","BIC","AICc")
    return(temp[which.min(temp$AIC),])
    #return(temp)
}

```

Plotting the daily ranges:

SPY
```{r}

plot(spyIn$spyRange, main = "SPY Daily Range")

```

The SPY data certainly appears to have clustered volatility, as there are large swings in close proximity to eachother. But it also has noticeable trend, with the intraday ranges being higher during the middle of the time series before dropping down again towards the end. To combat this, let's look at the differenced series:

```{r}
plot(spyIn$spyRange %>% diff(), main = "Differenced SPY Daily Range")
```

The differenced data looks noisy, and perhaps over differenced. As we saw on the previous page. Let's see which arima models had the lowest AIC and BIC scores.

Now, let's find good-fitting models starting with SPY:
```{r}
arimaResults(spyIn$spyRange)
```

The best model returned by the function is ARMA(3,0,3). It is surprising that the models which differenced the data didn't perform better, but I manually reviewed those options and they all had worse AIC and BIC scores.

### IWM

Let's check IWM:
```{r}
plot(iwmIn$iwmRange)
```

IWM's intraday range looks slightly more stationary than the other 2 time series, and also has more extreme changes in volatility over time. However, we can still see somewhat of a trend, such that a moving average would be obviously nonstationary, so once again we difference the series.

```{r}
plot(iwmIn$iwmRange %>% diff())
```

While the variance looked elevated in the non-differenced plot, it is actually less hesteroskedastic in the differenced plot. This suggests IWM may not have as much autocorrelation in the variance as the other two indices. An ARIMA model alone might suffice here, lets look for the parameters that yield the lowest AIC and BIC.

For IWM:
```{r}
arimaResults(iwmIn$iwmRange)
```

Fitting the pattern of the best models for the financial instruments being of the form ARMA(N, 0, N), the model with the lowest AIC for IWM was ARMA(4,0,4).




## SARIMA 

Let's look for a seasonal affect in the ACF plots, using the weather events data. First we prepare the data:
```{r}
weather_data <- read.csv('data/storms_clean.csv')


weather_data$month <- weather_data$BEGIN_YEARMONTH %% 100

weather_data <- weather_data %>%
    mutate(realdate = make_date(YEAR, month, BEGIN_DAY)) %>%
    mutate(DAMAGE_PROPERTY =  str_replace(DAMAGE_PROPERTY, "K", "") )  %>%
    mutate(DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)) %>%
    mutate(DAMAGE_PROPERTY = replace_na(DAMAGE_PROPERTY, 0))


# Daily Event Number
daily_events <- weather_data %>%
    group_by(realdate) %>%
    summarize(events = length(EPISODE_ID))

# Daily Property Damage
daily_property <-  weather_data %>%
    group_by(realdate) %>%
    summarize(pdam = sum(DAMAGE_PROPERTY))

# Daily Daily Casualties
daily_casualties <-  weather_data %>%
    group_by(realdate) %>%
    summarize(casualties = sum(INJURIES_DIRECT) + sum(INJURIES_INDIRECT) + sum(DEATHS_DIRECT) + sum(DEATHS_INDIRECT))

# Daily Hurricanes
daily_hurricanes <- weather_data %>%
    filter(EVENT_TYPE == "Hurricane") %>%
    group_by(realdate) %>%
    summarize(hurricaneWarnings = length(EPISODE_ID))


# Daily joined data

weather_merged <- full_join(daily_events, daily_property, by = "realdate")
weather_merged <- full_join(weather_merged, daily_casualties, by = 'realdate')
weather_merged <- full_join(weather_merged, daily_hurricanes, by = 'realdate')

head(weather_merged)

```

Now, lets look at the acf plot:
```{r}
acf(weather_merged$events, lag.max = 365, main = "ACF, Daily Weather Events")

```

With a 365 lag plot (as we are looking at weather data), we can see that for about 1/4 of the 365 lags, there is some positive correlation in the residuals (the same season), then for 1/2 the lags after that there is negative correlation (the opposite seasons), and then a return to significant positive correlation about 3/4 of the way through the data. This appears to show a clear seasonal effect of about 365. So let's seasonally difference the data.

```{r}

seasonDiff <- weather_merged$events %>% diff(lag = 365)
acf(seasonDiff, lag.max = 365, main = "ACF, 365 Day Differenced Weather Events")
pacf(seasonDiff, main = "PACF, 365 Differenced Weather Events")
```

After seasonal differencing, this plot looks much much better, without noticeable season-to-season correlations in the lags, although there is still some short-term correlation. And some repeating period which appears to be almost weekly in the residuals.


Based on the ACF and PACF plots, I would consider p of 1, d of 0, and q of 3. Then for P and Q I might consider 0, D would be 1 since we seasonally differenced. But let's run some code to see the AIC of different values:

```{r}
SARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){
  
temp=c()
d=1
D=1
s=12
 
i=1
temp= data.frame()
ls=matrix(rep(NA,9*378),nrow=378)
 
for (p in p1:p2)
  {
    for(q in q1:q2)
    {
      for(P in P1:P2)
      {
        for(Q in Q1:Q2)
        {
          for(d in d1:d2)
       
        {
          if(p+d+q+P+D+Q<=8)
          {
            
            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))
            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)
            i=i+1
            #print(i)
            
          }
          
        }
      }
    }
    
  }
  
  }
  temp= as.data.frame(ls)
  names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")
  
  temp
  
}

#SARIMA.c(p1=1,p2=5,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=weather_merged$events) %>% filter(!is.na(p))

#SARIMA.c(p1=1,p2=5,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=seasonDiff) %>% filter(!is.na(p))
```

Based on the results of the function, the minimum AIC and BIC are for the model: (0,1,2)(0,1,0). If I run the SARIMA function on the 365 differenced data, then it returns (1,1,1,)(0,1,0) as the model with the lowest AIC. So I will compare these two models for the series, using diagnostics.

```{r}
mod1 <- Arima(weather_merged$events,order=c(0,1,2),seasonal=c(0,1,0))
mod2 <- Arima(weather_merged$events,order=c(1,1,1),seasonal=c(0,1,0))

checkresiduals(mod1 )

```

The residuals for model 1 (0,1,2)(0,1,0) show some clustering of volatility. In addition, they appear to be skewed to the right, as the right tail of the residual distribution is fatter than the left tail and has more outlying values. Finally, the ACF plot of the residuals looks good, with little visible correlation and no values crossing the significance line. In addition, the Ljung-Box test returns p=0.43, suggesting we can reject the idea that there is autocorrelation in the residuals.


```{r}
checkresiduals(mod2 )
```

The residual diagnostics for model 2 (1,1,1)(0,1,0) are similar to model 1, except that they have even less correlatoin visible in the residual plots. The residual's distribution is still skewed to the right, with a fatter positive tail. The ACF plot, however, has even less correlation visible, with only two values even coming marginally close to the significance line. There is still some heteroskedacticity in the plot over time, however, suggesting clustering of volatility.


Now, let's use an Auto Arima function to determine the correct model:
```{r}
aaData <- ts(weather_merged$events)
mod3 <- auto.arima(aaData, seasonal = TRUE, trace = FALSE)

```

Regardless of the frequency fed into the model, Auto.arima only wants to fit a (2,0,1) ARIMA model. I tried 365, 90, 60, 40, 14, and 7 day frequencies, and in each case the seasonal term was not chosen for the data. I think the reason that it doesn't recognize the seasonality is because it doesn't worok with 365, which should be the best frequency for the data. 


Forecast with a confidence band: Model 1 (0,1,2)(0,1,0)
```{r}

plot(forecast(mod2), xlim = c(850,925))

```

Model 2: (1,1,1)(0,1,0)
```{r}
plot(forecast(mod1), xlim = c(850,925))
```

I think the forecasts are interesting, because model 2 has a more dynamic forecast, with a decrease over several days before leveling out its prediction. Model 1, meanwhile, predicts that the series will hardly change after its first prediction. While the series does not have a lot of trend going into the prediction interval, weather events are dynamic and I would tend to believe the model which includes more variation as opposed to constant numbers of events. Hence, I would select the (1,1,1)(0,1,0) SARIMA model.

Benchmark Comparison

We will use two benchmark methods: A seasonal naive forecast and a mean forecast.
```{r}

base1 <- snaive(aaData, 10)
base2 <- meanf(aaData, 10)

plot(base1, xlim = c(850,925))
```

Here we can see the plot for the seasonal naive model's forecasts (10 days out), which show a predicted value close to the last observed value in the series. It has a high degree of uncertainty as shown by the prediction interval, which is quite wide.


```{r}
plot(base2, xlim = c(850,925) ) 

```

The forecast for the meanf model (above) departs further from the previously observed values, as the mean of the series is substantially below recently observed values. However, the model has a smaller prediction interval than the seasonal naive forecast, which is a slight advantage.

Now let's look at the accuracy of the three forcasts:
```{r}


accuracy(snaive(aaData))
accuracy(meanf(aaData))
accuracy(mod2)

```

The accuracy statistics were a mixed result between the seasonal naive forecast and the SARIMA(1,1,1)(0,1,0) model. The mean forecast did not perform better on any metric than the other two models, and could be discarded. On Root Mean Squared Error, the SARIMA model beat the Seasonal Naive model with a value of 182.4 vs. 222.3, respectively. On Mean Absolute Error, the SARIMA model also performed better, with 120.3 compared to the Seasonal Naive model's 134.7. On the Mean Absolute Percentage Error, however, the Seasonal Naive model performed better than the SARIMA model, achieving 141.1 vs. 179.3 for the SARIMA model. 

Overall, it seems like the accuracy metrics might favor the seasonal naive model, while the prediction forecasts look more accurate for the SARIMA model.

Cross Validation:
Let's do a seasonal cross-validation with 1 and 10-step-ahead forecasts. 
```{r}

    # I add a 100 day buffer to my period of 365 days to have enough data
    test <- 100 
    trainnum <- length(aaData) - test
    rmse1 <- vector(mode = 'numeric', length = 100)
    rmse2 <- vector(mode = 'numeric', length = 100)
    rmse361 <- vector(mode = 'numeric', length = 100)
    rmse362 <- vector(mode = 'numeric', length = 100)

    for(i in 1:100) {

        
        xtrain <- aaData[c(1:(trainnum + i - 1))]
        xtest <-  aaData[c((trainnum + i +1):(trainnum+i+10))]
        
    
        ######## model ###########
        fit2 <- arima(xtrain, order = c(1,1,1), seasonal = list(order = c(0,1,0)))
        fcast2 <- predict(fit2, n.ahead = 10)$pred
        
        # Errors

        rmse2[i]  <-sqrt((fcast2[1]-xtest[1])^2)
        rmse362[i]  <- mean( sqrt((fcast2 -xtest)^2) )
        
    }
    
# create index
index <- c(1:100)
ggplot() +
    geom_line(aes(x = index, y = rmse2, color = '1 Step Ahead') ) + 
    geom_line(aes(x = index, y = rmse362, color = '10 Steps Ahead')) + 
    labs(title = "1 vs. 10 Steps Ahead Forecasts", color = "Forecast", 
    x = "Days", y = "Predicted Number of Weather Events" ) +  theme(text = element_text(size=14))
```

The chart above shows the RMSE for the cross validated forecasts with windows 1 and 10. I had to use 10 for my seasonal window as my computer was unable to handle the 365 window, and could not produce results. But my data also had short-term seasonality so I relied upon that here. The red line represents the 10-step ahead forecast average RMSE and the blue line represents the 1-step ahead forecast RMSE. Overall you can see that both forecasts perform better and worse around the same time, except that the short-term forecast has a lagged reaction to the same periods where the long-term forecast performed poorly. In some cases, however, the 1-step RMSE does exceed the 10-step RMSE, suggesting poor short term performance.




