[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series. A time series is a sequence of data points or observations collected or recorded over a period of time at specific, equally spaced intervals. Each data point in a time series is associated with a particular timestamp or time period, making it possible to analyze and study how a particular variable or phenomenon changes over time. Time series data can be found in various domains and can represent a wide range of phenomena, including financial data, economic indicators, weather measurements, stock prices, sales figures, and more.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\nKey characteristics of time series data include:\nTemporal Order: Time series data is ordered chronologically, with each data point representing an observation at a specific point in time. The order of data points is critical for understanding trends and patterns over time.\nEqually Spaced Intervals: In most cases, time series data is collected at regular intervals, such as hourly, daily, weekly, monthly, or yearly. However, irregularly spaced time series data can also exist.\nDependency: Time series data often exhibits temporal dependency, meaning that the value at a given time is influenced by or related to the values at previous times. This dependency can take various forms, including trends, seasonality. This serial correlation is called as autocorrelation.\nComponents: Time series data can typically be decomposed into various components, including:\nTrend: The long-term movement or direction in the data. Seasonality: Repeating patterns or cycles that occur at fixed intervals. Noise/Irregularity: Random fluctuations or variability in the data that cannot be attributed to the trend or seasonality.\nApplications: Time series data is widely used for various applications, including forecasting future values, identifying patterns and anomalies, understanding underlying trends, and making informed decisions based on historical data.\nAnalyzing time series data involves techniques like time series decomposition, smoothing, statistical modeling, and forecasting. This class will cover but not be limited to traditional time series modeling including ARIMA, SARIMA, the multivariate Time Series modeling including; ARIMAX, SARIMAX, and VAR models, Financial Time Series modeling including; ARCH, GARCH models, and E-GARCH, M-GARCH..ect, Bayesian structural time series (BSTS) models, Spectral Analysis and Deep Learning Techniques for Time Series. Researchers and analysts use software tools like Python, R, and specialized time series libraries to work with and analyze time series data effectively.\nTime series analysis is essential in fields such as finance, economics, epidemiology, environmental science, engineering, and many others, as it provides insights into how variables change over time and allows for the development of predictive models to forecast future trends and outcomes."
  },
  {
    "objectID": "Exploratory Data Analysis.html",
    "href": "Exploratory Data Analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Time Series Analysis\n\nLet’s start with reading in the intraday range data. First with the S&P 500.\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(quantmod)\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nLoading required package: TTR\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\n#decomposedSPY &lt;- decompose(spyIn$spyRange)\n\n##decompedSPY = HoltWinters(spyIn$spyRange,beta = FALSE,gamma = FALSE)\n#plot(decompedSPY)\n\nautoplot(spyIn$spyRange)\n\n\n\nacf(spyIn$spyRange)\n\n\n\npacf(spyIn$spyRange)\n\n\n\n\nUpon initial review of the SPY intraday range data, it appears that there is some trend, but no seasonality. The decomposition function would not work on the data as it could not recognize periodicity. As a next step, we can difference the data.\n\nlibrary(forecast)\n\n\ndiff1 &lt;- diff(spyIn$spyRange)\n\n\n\n#write.csv(diff1SPY, \"/data/spyDiff.csv\")\n#auto.arima(spyIn$spyRange)\nplot(diff1)\n\n\n\n\nAfter differencing, the data appears to have lost its trend. However, there is still visible heteroskedacticity in the differences between intraday ranges.\n\nacf(diff1,    na.action = na.exclude)\n\n\n\npacf(diff1,    na.action = na.exclude)\n\n\n\n\nAfter differencing, we see 2 lags being significant in the ACF plot. In the PACF plot, we see about 4 lags being significant. This is a marked departure from the original plots, where the ACF showed clear non-stationarity and the PACF had many significant lags.\n\nlibrary(tseries)\n\ndiffnoNA &lt;- diff1$spyRange[!is.na(diff1$spyRange)]\nadf.test(diffnoNA)\n\nWarning in adf.test(diffnoNA): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNA\nDickey-Fuller = -12.455, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n#\n\nWith dickey fuller test result of 0.01, we can reject the null hypothesis and conclude the series is stationary.\n\nMoving Average Smoothing\n\n\nspyDiff1 &lt;- as.ts(diffnoNA)\n\nspyAvg3Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/3, 3))\nspyAvg5Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/5, 5))\nspyAvg30Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/30, 30))\nspyAvg50Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/50, 50))\n\nspyAvg100Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/100, 100))\n\n\nautoplot(spyAvg3Diff1)\n\n\n\nautoplot(spyAvg5Diff1)\n\n\n\nautoplot(spyAvg30Diff1)\n\n\n\nautoplot(spyAvg50Diff1)\n\n\n\nautoplot(spyAvg100Diff1)\n\n\n\n\nOverall the effect of the moving average at higher numbers is to reveal seasonality in the data. At lower smoothing levels (3 and 5 days), the time series looked almost unchanged, and still highly variant. Only at a 30 day average window did the seasonality start to appear, with clear periods in the data and a repeating pattern. Notably, even with a repeating pattern the data still showed obvious heteroskedasticity, with periods of increased varianced. The 50 and 100 day moving average windows began to obfuscate the periods, while keeping the heteroskedacticity, although they did have smaller variations and were nearer to 0 on average. In adition, the 50 and 100 days had discernable trends in the data."
  },
  {
    "objectID": "Data Sources.html",
    "href": "Data Sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "S&P 500, Nasdaq 100, and Russel 2000, Daily Data Yahoo Finance\n\nhttps://finance.yahoo.com/quote/%5EGSPC/history?p=%255EGSPC\nFinancial data from tickers SPY, QQQ, and IWM. Data includes the open and close of stock prices for each day, as well as the highest and lowest price recorded for the day. These indices are widely used and will be a relevant outcome variable for the project to study. The main output value thaht will be calculated is the high-low range of the day.\n\nVIX Data, Yahoo Finance\n\nhttps://finance.yahoo.com/quote/%5EVIX/history?p=%255EVIX\nThe Chicago Board of Exchange (CBOE) VIX index is a widely-used tool to measure investor sentiment. The indicator itself represents the degree of volatility perceived by investors in the next month. It ranges from near zero up to about 60 in recent years, with scores at different intervals representing different levels of perceived volatility.\n\nWork Stoppages\n\nhttps://www.bls.gov/wsp/\nThis dataset contains all of the major strikes recorded by the bureau of labor statistics, such that I will be able to use the number of striking workers on any given day as a variable for analysis.\n\nDaily Yield Curve Next 3 Months\n\nhttps://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=2023\nWhile I originally considered the actual federal funds rate, the values did not change much day to day. Instead I would like to use the expectation of yield rates, measured through the pricing of bonds in the yield curve over the next few months. The data includes the prices of treasury bills 1, 2, and 3 months out on each date.\n\nStorm Events by Day\n\nhttps://www.ncdc.noaa.gov/stormevents/ftp.jsp\nNOAA maintains records on each storm warning they send out for the entire country. This dataset includes many types of storm warnings, such as for hurricanes, blizzards, flash floods, and tornadoes. Importantly, the dataset includes the day in which each weather event took place, as well as the property damage and bodily injury caused by the weather event."
  },
  {
    "objectID": "ARMA ARIMA SARIMA Models.html",
    "href": "ARMA ARIMA SARIMA Models.html",
    "title": "ARMA ARIMA SARIMA MODELS",
    "section": "",
    "text": "Loading packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(tseries)\n\n\nBringing the data into this tab as well:\n\n\nShow Code\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\ndiff1SPY &lt;- diff(spyIn$spyRange)\ndiff1QQQ &lt;- diff(qqqIn$qqqRange)\ndiff1IWM &lt;- diff(iwmIn$iwmRange)\n\n\n\nStationarity of the time series\n\nBased on previous results, and the fact that I am using “pseudo-differenced data” in that I am taking the percentage range in prices, in addition to a single differencing, means that the time series are stationary\n\nBuilding ARIMA Model\n\nSince I did some of this work with SPY data on the EDA tab, I will focus on QQQ range data here.\n\n\nCode\n#print(diff1QQQ)\nacf(diff1QQQ, na.action = na.exclude)\n\n\n\n\n\nCode\npacf(diff1QQQ, na.action = na.exclude)\n\n\n\n\n\nBased on these charts the order I would pick for QQQ is: ARIMA(2,1,0)\n\n\nCode\nmodelQQQ1 &lt;- arima(diff1QQQ, order = c(2,1,0))\nsummary(modelQQQ1)\n\n\n\nCall:\narima(x = diff1QQQ, order = c(2, 1, 0))\n\nCoefficients:\n          ar1      ar2\n      -1.0035  -0.4832\ns.e.   0.0334   0.0334\n\nsigma^2 estimated as 0.0001131:  log likelihood = 2149.08,  aic = -4292.16\n\nTraining set error measures:\n                       ME       RMSE         MAE      MPE     MAPE      MASE\nTraining set 4.630039e-05 0.01062913 0.007805518 87.46782 258.6253 0.6398223\n                   ACF1\nTraining set -0.1944037\n\n\n\nEquation is x = -1.0034x(t-1) - 0.4832x(t-2) + error\nModel Diagnostic:\n\n\n\nCode\nstats::tsdiag(modelQQQ1)\n\n\n\n\n\nThe Ljung Box statistics look cood, although the ACF of the residuals does have 1 significant term.\nI originally tried a (4,1,2) model, however the ljung box statistics were highly correlated, and I suspected overfitting. After reducing the parametrization greatly, the new model performed mnuch better.\n\n\n\n\n\nCode\nautoQQQ &lt;- auto.arima(diff1QQQ)\nsummary(autoQQQ)\n\n\nSeries: diff1QQQ \nARIMA(1,0,1) with zero mean \n\nCoefficients:\n          ar1      ma1\n      -0.0008  -0.8122\ns.e.   0.0524   0.0361\n\nsigma^2 = 5.483e-05:  log likelihood = 2402.77\nAIC=-4799.55   AICc=-4799.51   BIC=-4785.94\n\nTraining set error measures:\n                        ME       RMSE         MAE      MPE     MAPE     MASE\nTraining set -6.694728e-05 0.00739421 0.005455453 283.3672 549.0245 0.446905\n                    ACF1\nTraining set 0.003755607\n\n\nThe auto.arima method chose an ARIMA(1,0,1) model. However, this model did not perform as well in terms of AIC, with the Auto arima model having a score of -4799 while my model had a score of -4292.\n\nForecasting with my model\n\n\n\nCode\nplot(forecast(modelQQQ1, 10), xlim = c(650,750))   \n\n\n\n\n\nForecasting with auto arima model\n\n\nCode\nplot(forecast(autoQQQ, 10), xlim = c(650,750))\n\n\n\n\n\nOverall, my model has a slightly more dynamic prediction than the auto arima function, which quicly levels out to 0. However, my model also has a much wider uncertainty band.\n\nCompare ARIMA model with benchmarks\n\n\n\nCode\nnaiveModelQQQ &lt;- naive(diff1QQQ, h=1)\nsnaiveModelQQQ &lt;- snaive(diff1QQQ, h=1)\n\nsummary(naiveModelQQQ)\n\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = diff1QQQ, h = 1) \n\nResidual sd: 0.0165 \n\nError measures:\n                       ME     RMSE        MAE      MPE     MAPE MASE       ACF1\nTraining set 2.387746e-05 0.016488 0.01219951 309.6279 564.6879    1 -0.6746326\n\nForecasts:\n    Point Forecast       Lo 80      Hi 80       Lo 95      Hi 95\n691   -0.003168135 -0.02429836 0.01796209 -0.03548403 0.02914776\n\n\nCode\nsummary(snaiveModelQQQ)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = diff1QQQ, h = 1) \n\nResidual sd: 0.0165 \n\nError measures:\n                       ME     RMSE        MAE      MPE     MAPE MASE       ACF1\nTraining set 2.387746e-05 0.016488 0.01219951 309.6279 564.6879    1 -0.6746326\n\nForecasts:\n    Point Forecast       Lo 80      Hi 80       Lo 95      Hi 95\n691   -0.003168135 -0.02429836 0.01796209 -0.03548403 0.02914776\n\n\nCode\nsummary(modelQQQ1)\n\n\n\nCall:\narima(x = diff1QQQ, order = c(2, 1, 0))\n\nCoefficients:\n          ar1      ar2\n      -1.0035  -0.4832\ns.e.   0.0334   0.0334\n\nsigma^2 estimated as 0.0001131:  log likelihood = 2149.08,  aic = -4292.16\n\nTraining set error measures:\n                       ME       RMSE         MAE      MPE     MAPE      MASE\nTraining set 4.630039e-05 0.01062913 0.007805518 87.46782 258.6253 0.6398223\n                   ACF1\nTraining set -0.1944037\n\n\nI fit a naive and seasonal naive model. On RMSE my model had the best performance, with 0.011, while the naive and snaive models had 0.017 rmse each (since there was no seasonal period I realized they were the same model). On MAE my arima model had 0.008 while the seasonal naive models had 0.0122.\nLet’s compare forecasts:\n\n\nCode\nplot(forecast(modelQQQ1, 10), xlim = c(650,750)) \n\n\n\n\n\n\n\nCode\nplot(forecast(naiveModelQQQ, 10), xlim = c(650,750))   \n\n\n\n\n\nHere, the naive method can only forecast 1 observation into the future, since the seasonal period is one. Which is an advantage to my model, but realistically means the naive model should be evaluated with cross validation."
  },
  {
    "objectID": "about me.html",
    "href": "about me.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Data Visualization.html",
    "href": "Data Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The central question of this research project is about stock market volatility. As such the following visualizations will explore this subject in different areas. To start with, let’s define the outcome variables in question:\nFor each security: S&P 500, QQQ Pro-Shares ETF, and Russell 200 index, we have several metrics 1. Price 2. Standard deviation (10 day window) 3. True range (intraday difference between highest and lowest price/previous closing value) 4. Trading volume\nThe window of our investigation is January 1st 2021 - September 30th 2023.\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(quantmod)\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nLoading required package: TTR\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\nhead(spyIn)\n\n           SPY.Open SPY.High SPY.Low SPY.Close SPY.Volume SPY.Adjusted\n2021-01-04   375.31   375.45  364.82    368.79  110210800     354.1974\n2021-01-05   368.10   372.50  368.05    371.33   66426200     356.6368\n2021-01-06   369.71   376.98  369.12    373.55  107997700     358.7691\n2021-01-07   376.10   379.90  375.91    379.10   68766800     364.0994\n2021-01-08   380.59   381.49  377.10    381.26   71677200     366.1740\n2021-01-11   377.85   380.58  377.72    378.69   51034700     363.7057\n\nhead(qqqIn)\n\n           QQQ.Open QQQ.High QQQ.Low QQQ.Close QQQ.Volume QQQ.Adjusted\n2021-01-04   315.11   315.29  305.18    309.31   45305900     304.2444\n2021-01-05   308.29   312.14  308.29    311.86   29323400     306.7526\n2021-01-06   307.00   311.88  305.98    307.54   52809600     302.5034\n2021-01-07   310.28   315.84  310.25    314.98   30394800     309.8215\n2021-01-08   317.34   319.39  315.08    319.03   33955800     313.8051\n2021-01-11   315.98   317.19  313.75    314.42   32746400     309.2706\n\nhead(iwmIn)\n\n           IWM.Open IWM.High IWM.Low IWM.Close IWM.Volume IWM.Adjusted\n2021-01-04   197.54   197.89  190.94    193.50   33664200     186.8582\n2021-01-05   193.09   197.62  193.07    196.49   27442900     189.7457\n2021-01-06   199.48   206.78  199.16    204.53   52952200     197.5097\n2021-01-07   205.71   208.52  205.70    208.17   24031400     201.0247\n2021-01-08   209.32   209.77  204.66    207.72   29017000     200.5902\n2021-01-11   205.09   208.12  204.83    207.54   20945100     200.4163\n\n\nLet’s start by simply seeing the absolute prices of these securities over time:\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\ncombinedCloses &lt;- cbind(spyIn$SPY.Close, qqqIn$QQQ.Close, iwmIn$IWM.Close)\n\n\nts.plot(combinedCloses, gpars = list(col = rainbow(3)))\n\nlegend(\"topleft\", legend = c(\"SPY\", \"QQQ\", \"IWM\"), col = 1:3, lty = 1)\n\n\n\n\nIn these charts, we can see that the overall trend for all three securities has been somewhat of a decline since the start of the time window. We can also see that QQQ and IWM have moved further in their trends than SPY, which makes sense as that is the most stable index of the three.\nNow, let us look at the intraday range volatility measure.\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\ncombinedIntradayPercentRange &lt;- cbind(spyIn$spyRange, qqqIn$qqqRange, iwmIn$iwmRange)\n\nhead(qqqIn$pctRange)\n\nNULL\n\nts.plot(combinedIntradayPercentRange, gpars = list(col = rainbow(3)))\n\nlegend(\"topleft\", legend = c(\"SPY\", \"QQQ\", \"IWM\"), col = 1:3, lty = 1)\n\ntitle(\"Daily High-Low Range as Percent of Open Price\")\n\n\n\n\nIn this chart, we can see that the three indices tended to move together, with the midpoint of the time window having a higher average range in daily prices than the beginning or end for all of the securities. However, we can also see again that volatility is not even between them, with SPY and IWM having higher daily ranges than QQQ on average (at least according to the naked eye).\nNow, let’s review the trends in the federal funds rate:\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndff &lt;- read.csv(\"./dff.csv\")\n\ndff &lt;- dff %&gt;%\n    mutate(DATE = ymd(DATE)) %&gt;%\n    filter(DATE &gt;= ymd(\"2021-01-01\"))\n\nplot3 &lt;- ggplot(dff, aes(x = DATE, y = DFF)) + geom_line() + labs(y = \"Daily Federal Fund Rate\", x = \"Time\", title = \"Daily Federal Funds Rate over Time\")\nggplotly(plot3)"
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nLoading required package: TTR\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n\n\n\nThis chart shows the extreme volatility of Moderna’s stock price compared to its peers. The stock has gone from the cheapest of the 5 per share, to the most expensive by a factor of 2, back down to the 2nd most expensive, in the 3-year window visible. Meanwhile, the other 4 stocks have followed a roughly slow and steady upward trend, with Abbivie and Amgen having the greatest progression during the time period.\n\n\n\n\n\n\nHere, we see a steep downward trend in BTC price before Winter 2022, however BTC price has recovered from the low and roughly stabalized after that inflection point.\n\n\n\n\n\n\nThe candlestick chart shows how volatile bitcoins price can be in each day of the window studied. The candlesticks show wide intraday ranges in price.\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n      STATION                      NAME       DATE DAPR MDPR PRCP SNOW SNWD\n1 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-06   NA   NA    0    0   NA\n2 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-07   NA   NA    0    0   NA\n3 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-08   NA   NA    0    0   NA\n4 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-09   NA   NA    0    0   NA\n5 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-10   NA   NA    0    0   NA\n6 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-11   NA   NA    0    0   NA\n  TMAX TMIN TOBS WESD\n1   NA   NA   NA   NA\n2   NA   NA   NA   NA\n3   NA   NA   NA   NA\n4   NA   NA   NA   NA\n5   NA   NA   NA   NA\n6   NA   NA   NA   NA\n\n\nWarning: Removed 69 rows containing missing values (position_stack).\n\n\n\n\n\n\nthis chart shows the total precipitation recorded at 7 stations by month of the year. We can see the USC00182325 has a substantially higher total precipitation amount in August and September than all the other months. It is dubious, however, that the same station records no precipitation from October to December, and it leads me to wonder about whether the data is faulty. Secondly, we can see August has the highest total amount of precipitation recorded in the dataset.\n\n\n\n\n\n\nHere, we can see the CPI index has increased steadily since the 1940s, with a slower period of increase between 1960 and 1970, and a higher period of increase between 1970 and 1980, followed by a marked jump after 2020."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "For this project the time series I will be studying are major stock market indices, specifically the S&P 500, the Russel 2000, and the Nasdaq 100 (QQQ). The purpose of this project is to understand the factors driving the price of these indices, looking at climate events such as wildfires and hurricanes, investor confidence, macroeconomic data, and the impacts of COVID-19. I will use the statistical models we learn in class to investigate the relationships of these factors on stock market prices."
  },
  {
    "objectID": "Introduction.html#topic-explanation",
    "href": "Introduction.html#topic-explanation",
    "title": "Introduction",
    "section": "",
    "text": "For this project the time series I will be studying are major stock market indices, specifically the S&P 500, the Russel 2000, and the Nasdaq 100 (QQQ). The purpose of this project is to understand the factors driving the price of these indices, looking at climate events such as wildfires and hurricanes, investor confidence, macroeconomic data, and the impacts of COVID-19. I will use the statistical models we learn in class to investigate the relationships of these factors on stock market prices."
  },
  {
    "objectID": "Introduction.html#the-big-picture",
    "href": "Introduction.html#the-big-picture",
    "title": "Introduction",
    "section": "The Big Picture:",
    "text": "The Big Picture:\nFinancial markets are directly important to the well-being of everyday Americans. While the stock market is often seen as exclusive to a small circle of elite investors, in reality millions of Americans have exposure to the markets through retirement funds, which are made up of many different kinds of investments. If we can become better at predicting the course of financial markets, particularly in relation to external forces like climate change, it could promise to reduce volatility and inefficiency in the market. In turn, this would help everyday people build up their savings to enjoy a better quality of life, retire sooner, or have more security.\nFurther, understanding the movements of financial markets is important in controlling sentiment. In cases like the onset of the COVID-19 pandemic, stock market crashes could snowball into businesses cutting more jobs than needed, which would again directly impact everyday peoples’ livelihoods. With the hope of understanding these phenomena better, this project will look directly at investor sentiment and the impact of COVID-19.\n\n\n\nBig Picture"
  },
  {
    "objectID": "Introduction.html#analytical-angles",
    "href": "Introduction.html#analytical-angles",
    "title": "Introduction",
    "section": "Analytical Angles:",
    "text": "Analytical Angles:\nFirst, looking at stock prices as individual univariate time series. Conceptually, we are studying any intrinsic characteristics or patterns that publicly-traded stocks might have. This focuses on the properties of stocks themselves, and not how they react to external stimuli. Second, understanding stock prices as objects which are reflective of individuals actions and preferences. In this case, the individuals in question are investors, and we will look at investor sentiment and see how it might affect prices in financial markets. Third, viewing changes in stock prices in the context of external stimuli. From this analytical perspective, the stock market is a closed system, which is disrupted by external forces from time-to-time that cause unexpected behaviour. Studying the impact of climate events and economic data releasing will pertain to this point. Fourth, using stocks at output variables, which are a tool to investigate the impact of serious events like COVID-19. In this case, we are interested in looking at how the onset of COVID rippled through the stock market, and what we could learn from stock prices to help ameliorate similar disasters in the future."
  },
  {
    "objectID": "Introduction.html#literature-review",
    "href": "Introduction.html#literature-review",
    "title": "Introduction",
    "section": "Literature Review:",
    "text": "Literature Review:\nMany of the factors this project seeks to investigate have previously been reviewed by academics. For instance, the consequence of investor sentiment on the stock market has been widely studied, in terms of how it impacts stock market crises (Zouaoui, Nouyrigat, and Beer 2011), trading volume (So and Lei 2015), and especially returns (Smales 2017). Further, authors have previously found an impact of climate events, such as hurricanes (Liu, Ferreira, and Karali 2021) on stock prices of particular companies. The fact that these issues have been studied previously is encouraging, because in each case the authors chose a specific group of time and company limited stock market data. This project might extend their results to other stock market datasets, and might hope to incorporate these relationships into new statistical models"
  },
  {
    "objectID": "Introduction.html#guiding-questions",
    "href": "Introduction.html#guiding-questions",
    "title": "Introduction",
    "section": "Guiding Questions:",
    "text": "Guiding Questions:\n\nHow do markets react in anticipation of the release of macroeconomic data?\nHow do markets react when their predictions about macroeconomic data are more or less accurate?\nHow does investor confidence impact markets?\nCan time series data from financial markets tell us anything about future levels of investor confidence?\nHow does the stock market react to the increasing prevalence of climate events such as hurricanes, wildfires?\nWhich securities are most impacted by climate events?\nHow has COVID changed stock prices in the months and years after the onset of the pandemic?\nCan we predict the impact of future black swan events on the stock market by looking at COVID’s example?\nHow do the prices of different securities impact eachother?\nHow have the relationships between the prices of securities changed over time?"
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html",
    "href": "ARIMAX SARIMAX VAR.html",
    "title": "Data Sources",
    "section": "",
    "text": "Planning Models\n\nWe have the following independent variables: 1. Interest Rate Expectation Changes - 3 Months 2. Interest Rate Expectation Changes - 6 Months 3. Interest Rate Expectation Changes - 1 Year 4. Extreme Weather Events - Daily Event Number 5. Extreme Weather Events - Daily Property Damage 6. Extreme Weather Events - Daily Casualties 7. Extreme Weather Events - Hurricanes 8. Expected Volatility (VIX) - Value 9. Expected Volatility (VIX) - Daily Change 10. Work Stoppages - Daily Striking Worker Total 11. Work Stoppages - Daily New Strike Beggining 12. Work Stoppages - Daily New Workers Striking\nWe will combine these 12 predictors into 5 models, for SPY, QQQ, and IWM intraday volatility: 1. (ARIMAX) SPY ~ Interest Rate 1-Year + Daily Weather Property Damage + Daily VIX Change + Daily Striking Worker Total 2. (VAR) SPY ~ Interest Rate 6-Months + Daily Weather Event Number + Daily VIX Value + Daily Striking Workers 3. (ARIMAX) IWM ~ Daily New Strikes Beggining + Casualties + Hurricanes + Interest Rate 3-Months 4. (VAR/ARIMAX) QQQ ~ All Interest Rates + All Extreme Weather Events + Daily VIX Change + Daily Striking Workers 5. (ARIMAX) QQQ ~ Interest Rate 3 months + Daily Property Damage + Daily VIX Change + Daily New Striking Workers\n\nPreparing Exogenous Data\n\nFirst we need to create all 12 predictors, then we can combine them to estimate the models as needed."
  }
]