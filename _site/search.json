[
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In this project I will study the daily volatility of major US stock indices over time, specifically the Standard and Poors 500, the Russel 2000, and the NASDAQ 100. By volatility, I specifically mean the intraday range in prices that these indices take on the open market. I will both review these intraday ranges as univariate time series, and also attempt to shed light on variables that impact volatility by incorporating external factors. I will look at warnings of extreme weather events, investor confidence metrics, future bond rate expectations, and work stoppage data on strikes and striking workers. Using the statistical models we have learned in class, I hope to investigate the relationships between these factors and stock market volatility."
  },
  {
    "objectID": "Introduction.html#topic-explanation",
    "href": "Introduction.html#topic-explanation",
    "title": "Introduction",
    "section": "",
    "text": "In this project I will study the daily volatility of major US stock indices over time, specifically the Standard and Poors 500, the Russel 2000, and the NASDAQ 100. By volatility, I specifically mean the intraday range in prices that these indices take on the open market. I will both review these intraday ranges as univariate time series, and also attempt to shed light on variables that impact volatility by incorporating external factors. I will look at warnings of extreme weather events, investor confidence metrics, future bond rate expectations, and work stoppage data on strikes and striking workers. Using the statistical models we have learned in class, I hope to investigate the relationships between these factors and stock market volatility."
  },
  {
    "objectID": "Introduction.html#the-big-picture",
    "href": "Introduction.html#the-big-picture",
    "title": "Introduction",
    "section": "The Big Picture:",
    "text": "The Big Picture:\nFinancial markets are directly important to the well-being of everyday Americans. While the stock market is often seen as exclusive to a small circle of elite investors, in reality millions of Americans have exposure to the markets through retirement funds, which are made up of many different kinds of investments. If we can become better at predicting the course of financial markets, particularly in relation to external forces like climate change, it could promise to reduce volatility and inefficiency in the market. In turn, this would help everyday people build up their savings to enjoy a better quality of life, retire sooner, and have more financial security.\nFurther, understanding the movements of financial markets is important in controlling sentiment amongst investors. In cases like the onset of the COVID-19 pandemic, stock market crashes could snowball into businesses cutting more jobs than needed, which could directly impact peoples’ livelihoods. By looking at extreme events such as major strikes and severe weather warnings, I hope to elucidate the effect of these changes.\n\n\n\nBig Picture"
  },
  {
    "objectID": "Introduction.html#analytical-angles",
    "href": "Introduction.html#analytical-angles",
    "title": "Introduction",
    "section": "Analytical Angles:",
    "text": "Analytical Angles:\nThe price of stock market indices are considered to be noisy and near-impossible to predict. Related values such as intraday range are also expected to be difficult to predict with any accuracy (given that any such prediction could yield profit and should be closed under the efficient market hypothesis).\nFirst, looking at intraday volatility as univariate time series. Conceptually, this means studying the securities in a vacumn, and assuming no external information will determine their future while. While this approach might seem naive, it actually mirrors how some financial institutions might manage risk, where you look to a stocks past volatility to determine future volatility.\nSecond, understanding intraday volatility as a series of values which are reflective of individuals actions and preferences. In this case, the individuals in question are investors, and we will look at investor sentiment and see how it might affect prices in financial markets.\nThird, viewing intraday volatility in the context of external stimuli. From this analytical perspective, the stock market is a closed system, which is disrupted by external forces from time-to-time that cause unexpected behaviour. Studying the impact of extreme weather events and work stoppages relate to this point.\nFourth, understanding intraday volatility as an output variable of larger expectations in the financial system. By looking at changes in future bond prices (i.e. predictions for future bond rates), we can understand how indices react to shifting financial conditions overall."
  },
  {
    "objectID": "Introduction.html#literature-review",
    "href": "Introduction.html#literature-review",
    "title": "Introduction",
    "section": "Literature Review:",
    "text": "Literature Review:\nMany of the factors this project seeks to investigate have previously been reviewed by academics. For instance, the consequence of investor sentiment on the stock market has been widely studied, in terms of how it impacts stock market crises (Zouaoui, Nouyrigat, and Beer 2011), trading volume (So and Lei 2015), and especially returns (Smales 2017). Further, authors have previously found an impact of climate events, such as hurricanes (Liu, Ferreira, and Karali 2021) on stock prices of particular companies. The fact that these issues have been studied previously is encouraging, because in each case the authors chose a specific group of time and company limited stock market data. This project might extend their results to other stock market datasets, and might hope to incorporate these relationships into new statistical models."
  },
  {
    "objectID": "Introduction.html#guiding-questions",
    "href": "Introduction.html#guiding-questions",
    "title": "Introduction",
    "section": "Guiding Questions:",
    "text": "Guiding Questions:\n\nIs the intraday volatility of major stock indices truly random, or does it contain patterns that can be modelled?\nDo the major US stock indices differ greatly in the structure of their volatility over time?\nHow do markets react in response to chaining expectations about macroeconomic conditions?\nHow do markets incorporate extreme weather event warnings into their pricing of major indices, and how does this affect volatility?\nHow do markets react in response to major strikes and labor stoppages, and does this differ based on the size of the strike or the types of companies in question?\nHow does investor confidence impact markets?\nDo the major US stock indices differ in their response to external stimuli, and are some more sensitive to external volitility?\nAre the patterns exhibited by intraday volatility in major indices best-suited to traditional statistical models or deep learning? If deep learning models fit better, then what does that say about patterns in volatility?\nHave the relationships between the intraday ranges in the prices of securities and external factors changed over time?\nHow well can we explain variations in intraday volatility, when synthesizing as much external data and models as possible?"
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html",
    "href": "Financial Time Series Models (ARCH GARCH).html",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "",
    "text": "On this page, I will look to see whether ARCH and GARCH models may be a good fit for the change in intraday range of popular stock market indices. While these models would usually be fitted on the returns of financial intstruments, the day over day change in intraday percent range also exhibits volatility clustering, and intraday range is the exact subject of my project. This means that modelling these time series successfully would be complementary with the other models I have tried to fit, and I am eager to investigate whether these methods will work well at answering my research questions. I will look to fit 3 models: 1 each for the daily range of SPY, QQQ, and IWM."
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html#introduction",
    "href": "Financial Time Series Models (ARCH GARCH).html#introduction",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "",
    "text": "On this page, I will look to see whether ARCH and GARCH models may be a good fit for the change in intraday range of popular stock market indices. While these models would usually be fitted on the returns of financial intstruments, the day over day change in intraday percent range also exhibits volatility clustering, and intraday range is the exact subject of my project. This means that modelling these time series successfully would be complementary with the other models I have tried to fit, and I am eager to investigate whether these methods will work well at answering my research questions. I will look to fit 3 models: 1 each for the daily range of SPY, QQQ, and IWM."
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html#stationarity-and-volatility",
    "href": "Financial Time Series Models (ARCH GARCH).html#stationarity-and-volatility",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "Stationarity and Volatility",
    "text": "Stationarity and Volatility\nLoad packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(quantmod)\n\n\nRead in data\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\nPlotting the daily ranges:\nSPY\n\n\nCode\nplot(spyIn$spyRange, main = \"Daily SPY Range (Proportion of Open)\")\n\n\n\n\n\nThe SPY data certainly appears to have clustered volatility, as there are large swings in close proximity to eachother. But it also has noticeable trend, with the intraday ranges being higher during the middle of the time series before dropping down again towards the end. To combat this, let’s look at the differenced series:\n\n\nCode\nplot(spyIn$spyRange %&gt;% diff(), main = \"Daily Difference in SPY Range\")\n\n\n\n\n\nNow, we see a plot that looks very similar to the daily returns of a stock and promising for ARCH/GARCH modeling, which means it is stationary but has clustered volatility. To adjust for this, we can fit an ARIMA model on the SPY ranges, and then fit a GARCH model on the residuals, which will come from differenced data that is now stationary.\nRepeating the process for QQQ:\n\n\nCode\nplot(qqqIn$qqqRange, main = \"Daily QQQ Range (Proportion of Open)\")\n\n\n\n\n\nOnce again we see clear non-stationarity in the data, so let’s look at the differenced values:\n\n\nCode\nplot(qqqIn$qqqRange %&gt;% diff(), main = \"Daily Difference in QQQ Range\")\n\n\n\n\n\nSimilar to the SPY data, the time series is now stationary, although there is heteroskedasticity. So we might expect to model QQQ and SPY similarly, with an ARIMA + ARCH/GARCH model.\nLet’s check IWM:\n\n\nCode\nplot(iwmIn$iwmRange, main = \"Daily IWM Range (Proportion of Open)\")\n\n\n\n\n\nIWM’s intraday range looks slightly more stationary than the other 2 time series, and also has more extreme changes in volatility over time. However, we can still see somewhat of a trend, such that a moving average would be obviously nonstationary, so once again we difference the series.\n\n\nCode\nplot(iwmIn$iwmRange %&gt;% diff(), main = \"Daily Difference in IWM Range\")\n\n\n\n\n\nWhile the variance looked elevated in the non-differenced plot, it is actually less hesteroskedastic in the differenced plot. This suggests IWM may not have as much autocorrelation in the variance as the other two indices. An ARIMA model alone might suffice here."
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html#past-arima-models-regerenced",
    "href": "Financial Time Series Models (ARCH GARCH).html#past-arima-models-regerenced",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "Past ARIMA Models Regerenced",
    "text": "Past ARIMA Models Regerenced\nNow, let’s gather ARIMA models for the intraday range. First we declare the helper function\n\n\nCode\narimaResults &lt;- function(data) {\n    \n    i=1\n    temp= data.frame()\n    ls=matrix(rep(NA,6*100),nrow=100) # roughly nrow = 3x4x2\n\n    for (p in 1:6)# p=1,2,3 : 3\n    {\n        for(q in 1:6)# q=1,2,3,4 :4\n        {\n            for(d in 1:2) {\n                \n                if(p-1+d - 1+q-1&lt;=8) #usual threshold\n                {\n                    \n                    model&lt;- Arima(data,order=c(p-1,d-1,q-1),include.drift=FALSE) \n                    ls[i,]= c(p-1,d-1,q-1,model$aic,model$bic,model$aicc)\n                    i=i+1\n                    #print(i)\n                    \n                }\n\n            }\n                \n        }\n    }\n\n    temp= as.data.frame(ls)\n    names(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n    return(temp[which.min(temp$AIC),])\n    #return(temp)\n}\n\n\nNow, let’s find good-fitting models starting with SPY:\n\n\nCode\narimaResults(spyIn$spyRange)\n\n\n   p d q       AIC       BIC      AICc\n43 3 0 3 -5139.956 -5103.662 -5139.744\n\n\nThe best model returned by the function is ARMA(3,0,3). It is surprising that the models which differenced the data didn’t perform better, but I manually reviewed those options and they all had worse AIC and BIC scores.\nFor QQQ:\n\n\nCode\narimaResults(qqqIn$qqqRange)\n\n\n   p d q      AIC       BIC      AICc\n15 1 0 1 -4814.81 -4796.663 -4814.751\n\n\nFor QQQ, the model with the lowest AIC score was ARMA(1,0,1). Once again I expected a differenced model to perform better, but I will continue with the residuals from this model.\nFor IWM:\n\n\nCode\narimaResults(iwmIn$iwmRange)\n\n\n   p d q       AIC       BIC      AICc\n56 4 0 4 -4857.373 -4812.006 -4857.049\n\n\nFitting the pattern of the best models for the financial instruments being of the form ARMA(N, 0, N), the model with the lowest AIC for IWM was ARMA(4,0,4)."
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html#reviewing-residuals-to-identify-archgarch-models",
    "href": "Financial Time Series Models (ARCH GARCH).html#reviewing-residuals-to-identify-archgarch-models",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "Reviewing Residuals to Identify ARCH/GARCH Models",
    "text": "Reviewing Residuals to Identify ARCH/GARCH Models\nNow, lets look at the residuals of all of these models, to identify clustering and see if we need to fit ARCH/GARCH models on the residuals.\nDeclaring the models\n\n\nCode\nspyARMA &lt;- arima(spyIn$spyRange, order = c(3,0,3))\nqqqARMA &lt;- arima(qqqIn$qqqRange, order = c(1,0,1))\niwmARMA &lt;- arima(iwmIn$iwmRange, order = c(4,0,4))\n\n\nSPY range residuals:\n\n\nCode\ncheckresiduals(spyARMA)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(3,0,3) with non-zero mean\nQ* = 5.2511, df = 4, p-value = 0.2625\n\nModel df: 6.   Total lags used: 10\n\n\nThe residuals show definite clustering around timestamps 260 and 450, which suggests a ARCH/GARCH model will be a good fit. Now lets look at ACF and PACF plots of the residuals and squared residuals:\n\n\nCode\nacf(spyARMA$residuals, main = \"SPY ARMA(3,0,3) Residuals ACF\")\n\n\n\n\n\nCode\npacf(spyARMA$residuals, main = \"SPY ARMA(3,0,3) Residuals PACF\")\n\n\n\n\n\nCode\nacf(spyARMA$residuals^2, main = \"SPY ARMA(3,0,3) Residuals Squared ACF\")\n\n\n\n\n\nCode\npacf(spyARMA$residuals^2, main = \"SPY ARMA(3,0,3) Residuals Squared PACF\")\n\n\n\n\n\nCode\n#install.packages('FinTS')\nlibrary(FinTS)\nArchTest(spyIn$spyRange)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  spyIn$spyRange\nChi-squared = 204.01, df = 12, p-value &lt; 2.2e-16\n\n\nCode\n#ArchTest(spyARMA$residuals)\n\n\nBased on the ACF and PACF plots, it seems like an ARCH model of (1) might be the best fit for SPY residual data. An ARCH test of the original range data confirms that there is an ARCH effect in the dataset.\nQQQ Residuals:\n\n\nCode\nacf(qqqARMA$residuals, main = \"QQQ ARMA(1,0,1) Residuals ACF\")\n\n\n\n\n\nCode\npacf(qqqARMA$residuals, main = \"QQQ ARMA(1,0,1) Residuals PACF\")\n\n\n\n\n\nCode\nacf(qqqARMA$residuals^2, main = \"QQQ ARMA(1,0,1) Residuals Squared ACF\")\n\n\n\n\n\nCode\npacf(qqqARMA$residuals^2, main = \"QQQ ARMA(1,0,1) Residuals Squared PACF\")\n\n\n\n\n\nCode\n#install.packages('FinTS')\nlibrary(FinTS)\nArchTest(qqqIn$qqqRange)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  qqqIn$qqqRange\nChi-squared = 173.7, df = 12, p-value &lt; 2.2e-16\n\n\nCode\nArchTest(qqqARMA$residuals)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  qqqARMA$residuals\nChi-squared = 17.618, df = 12, p-value = 0.1278\n\n\nThe QQQ residuals suggest GARCH values up to (1,3). The squared residuals suggest more similar to a ARCH(1) model. ARCH tests of the original range data confirm an ARCH effect.\nIWM Residuals:\n\n\nCode\nacf(iwmARMA$residuals, main = \"IWM ARMA(4,0,4) Residuals ACF\")\n\n\n\n\n\nCode\npacf(iwmARMA$residuals, main = \"IWM ARMA(4,0,4) Residuals PACF\")\n\n\n\n\n\nCode\nacf(iwmARMA$residuals^2, main = \"IWM ARMA(4,0,4) Residuals Squared ACF\")\n\n\n\n\n\nCode\npacf(iwmARMA$residuals^2, main = \"IWM ARMA(4,0,4) Residuals Squared PACF\")\n\n\n\n\n\nCode\n#install.packages('FinTS')\nlibrary(FinTS)\nArchTest(iwmIn$iwmRange)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  iwmIn$iwmRange\nChi-squared = 174.39, df = 12, p-value &lt; 2.2e-16\n\n\nCode\nArchTest(iwmARMA$residuals)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  iwmARMA$residuals\nChi-squared = 28.872, df = 12, p-value = 0.004116\n\n\nThe IWM residuals look similar to the spy residuals, but suggest an ARCH (1) model. The squared residuals suggest GARCH(1,1)."
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html#finding-and-evaluating-arch-and-garch-fits",
    "href": "Financial Time Series Models (ARCH GARCH).html#finding-and-evaluating-arch-and-garch-fits",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "Finding and evaluating ARCH and GARCH Fits",
    "text": "Finding and evaluating ARCH and GARCH Fits\nLet’s start by declaring a GARCH helper function that checks possible values:\n\n\nCode\nlibrary(tseries)\ngarchResults &lt;- function(data) {\n    model &lt;- list() ## set counter\n    cc &lt;- 1\n    for (p in 1:7) {\n        for (q in 1:7) {\n            if(p + q &lt; 8) {\n                model[[cc]] &lt;- garch(data,order=c(q-1,p),trace=F)\n                cc &lt;- cc + 1\n            }\n        }\n    } \n\n    ## get AIC values for model evaluation\n    GARCH_AIC &lt;- sapply(model, AIC) ## model with lowest AIC is the best\n    #which(GARCH_AIC == min(GARCH_AIC))\n    ## [1] 24\n    model[[which(GARCH_AIC == min(GARCH_AIC))]]\n}"
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html#spy-garch-model-fitting-diagnostics-and-equation",
    "href": "Financial Time Series Models (ARCH GARCH).html#spy-garch-model-fitting-diagnostics-and-equation",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "SPY GARCH Model Fitting, Diagnostics, and Equation",
    "text": "SPY GARCH Model Fitting, Diagnostics, and Equation\nLet’s check our ARCH(1) model hypothesis for SPY:\n\n\nCode\nlibrary(vars)\nlibrary(fGarch)\n\nspyMod &lt;- garchResults(spyARMA$residuals)\nsummary(spyMod)\n\n\n\nCall:\ngarch(x = data, order = c(q - 1, p), trace = F)\n\nModel:\nGARCH(4,1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9251 -0.6944 -0.1773  0.4743  5.7716 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 1.295e-06   5.529e-07    2.342   0.0192 *  \na1 2.677e-01   4.417e-02    6.060 1.36e-09 ***\nb1 2.309e-01   9.399e-02    2.457   0.0140 *  \nb2 3.670e-09   1.336e-01    0.000   1.0000    \nb3 8.880e-02   1.380e-01    0.643   0.5199    \nb4 4.160e-01   9.804e-02    4.244 2.20e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 692.91, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.46517, df = 1, p-value = 0.4952\n\n\nCode\n#checkresiduals(spyARMA)\ncheckresiduals(spyMod)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 2.669, df = 10, p-value = 0.9882\n\nModel df: 0.   Total lags used: 10\n\n\nThe automatic function returns a surprise: The GARCH model with the lowest AIC is acutally GARCH(4,1). The diagnostics for this model are also promising, the 4th and 1st lag variance are highly significant (p &lt; 0.05), as is the 1st residual error. The Box-Ljung test is decidedly above the threshold at which we can reject the idea that there is no autocorrelation in the residuals (p = 0.495) which is another encouraging sign for the model fit. This suggests that the model has explained most of the signal in the data, with little correlation left in the residuals. It also shows that the results have improved from the ARMA(3,0,3) model alone, which had a Ljung-Box test result of 0.26, which suggests less correlation is left in the residuals after fitting a GARCH(4,1) model.\n\n\nCode\ngarchFit(formula = ~arma(3,3) + garch(4,1), data = spyIn$spyRange, trace = FALSE)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(3, 3) + garch(4, 1), data = spyIn$spyRange, \n    trace = FALSE) \n\nMean and Variance Equation:\n data ~ arma(3, 3) + garch(4, 1)\n&lt;environment: 0x0000000038afa6b8&gt;\n [data = spyIn$spyRange]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1          ar2          ar3          ma1          ma2  \n 2.4917e-05   9.7804e-01   5.9471e-01  -5.7502e-01  -6.6483e-01  -5.8857e-01  \n        ma3        omega       alpha1       alpha2       alpha3       alpha4  \n 2.9376e-01   3.2859e-07   5.3270e-02   1.0000e-08   1.0000e-08   1.0000e-08  \n      beta1  \n 9.3784e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      2.492e-05   3.759e-05    0.663 0.507405    \nar1     9.780e-01   1.474e-01    6.634 3.28e-11 ***\nar2     5.947e-01   1.858e-01    3.200 0.001374 ** \nar3    -5.750e-01   8.150e-02   -7.055 1.73e-12 ***\nma1    -6.648e-01   1.629e-01   -4.082 4.46e-05 ***\nma2    -5.886e-01   1.573e-01   -3.742 0.000182 ***\nma3     2.938e-01   8.403e-02    3.496 0.000473 ***\nomega   3.286e-07   2.829e-07    1.161 0.245454    \nalpha1  5.327e-02   4.160e-02    1.281 0.200369    \nalpha2  1.000e-08   4.400e-02    0.000 1.000000    \nalpha3  1.000e-08   1.060e-01    0.000 1.000000    \nalpha4  1.000e-08   8.994e-02    0.000 1.000000    \nbeta1   9.378e-01   2.959e-02   31.699  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2619.748    normalized:  3.796736 \n\nDescription:\n Thu Dec 07 01:56:28 2023 by user: corwi \n\n\nThe final evaluation of all the parameters together, however, suggests that the GARCH model should be (0,1). SO I will write the final correct formula as:\nFinal formula is: Rt = 0.000025 + 0.978 * R(t-1) + 0.595 * R(t-2) - 0.575 * R(t-3) - 0.665 * W(t-1) - 0.589 * W(t-2) + 0.294 * W(t-3) + At\nAt = Sigma(t) * Epsilon(t)\nSigma(t) ^ 2 = 0.00000033 + 0.94 * Sigma(t-1)^2"
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html#qqq-garch-model-fitting-diagnostics-and-equation",
    "href": "Financial Time Series Models (ARCH GARCH).html#qqq-garch-model-fitting-diagnostics-and-equation",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "QQQ GARCH Model Fitting, Diagnostics, and Equation",
    "text": "QQQ GARCH Model Fitting, Diagnostics, and Equation\n\n\nCode\nqqqMod &lt;- garchResults(qqqARMA$residuals)\nsummary(qqqMod)\n\n\n\nCall:\ngarch(x = data, order = c(q - 1, p), trace = F)\n\nModel:\nGARCH(1,1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9569 -0.6967 -0.1981  0.4453  5.4864 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 3.515e-07   1.633e-07    2.152   0.0314 *  \na1 3.527e-02   5.331e-03    6.615 3.72e-11 ***\nb1 9.582e-01   6.218e-03  154.113  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 591.42, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.070318, df = 1, p-value = 0.7909\n\n\nCode\n#qqqMod\n\n#checkresiduals(qqqARMA)\n#checkresiduals(qqqMod)\n\n\nThe GARCH model with the lowest AIC for the QQQ residuals is GARCH(1,1), which is within the range of the (1,3) we suspected, but still a different value than we anticipated. All of the terms in this model a significant at the p&lt;0.05 level, both the residual error lag term and the lag variance term. In addition, the Ljung-Box test score improves from 0.1825 in the ARMA(1,0,1) model alone, to 0.51 in the GARCH model. This suggests the model has lowered the autocorrelation in the residuals.\n\n\nCode\ngarchFit(formula = ~arma(1,1) + garch(1,1), data = qqqIn$qqqRange, trace = FALSE)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(1, 1) + garch(1, 1), data = qqqIn$qqqRange, \n    trace = FALSE) \n\nMean and Variance Equation:\n data ~ arma(1, 1) + garch(1, 1)\n&lt;environment: 0x000000003acec0c8&gt;\n [data = qqqIn$qqqRange]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1          ma1        omega       alpha1        beta1  \n 6.1100e-04   9.6203e-01  -7.4073e-01   3.5130e-07   3.5178e-02   9.5819e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      6.110e-04   2.413e-04    2.532   0.0113 *  \nar1     9.620e-01   1.488e-02   64.636  &lt; 2e-16 ***\nma1    -7.407e-01   3.685e-02  -20.099  &lt; 2e-16 ***\nomega   3.513e-07   2.017e-07    1.741   0.0816 .  \nalpha1  3.518e-02   7.556e-03    4.656 3.23e-06 ***\nbeta1   9.582e-01   7.993e-03  119.873  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2441.621    normalized:  3.538581 \n\nDescription:\n Thu Dec 07 01:56:28 2023 by user: corwi \n\n\nFinal formula is: Rt = 0.00061 + 0.962 * R(t-1) - 0.741 * W(t-1) + At\nAt = Sigma(t) * Epsilon(t)\nSigma(t) ^ 2 = 0.00000035 + .034 * a(t-1)^2 + 0.96 * Sigma(t-1)^2"
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html#iwm-garch-model-fitting-diagnostics-and-equation",
    "href": "Financial Time Series Models (ARCH GARCH).html#iwm-garch-model-fitting-diagnostics-and-equation",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "IWM GARCH Model Fitting, Diagnostics, and Equation",
    "text": "IWM GARCH Model Fitting, Diagnostics, and Equation\n\n\nCode\niwmMod &lt;- garchResults(iwmARMA$residuals)\nsummary(iwmMod)\n\n\n\nCall:\ngarch(x = data, order = c(q - 1, p), trace = F)\n\nModel:\nGARCH(1,1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7824 -0.7249 -0.1962  0.4915  5.1795 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 2.153e-06   8.987e-07    2.395 0.016602 *  \na1 6.089e-02   1.693e-02    3.597 0.000322 ***\nb1 8.969e-01   2.903e-02   30.900  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 338.27, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 8.6335e-05, df = 1, p-value = 0.9926\n\n\nCode\n# checkresiduals(iwmARMA)\n\ncheckresiduals(iwmMod)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 4.2445, df = 10, p-value = 0.9356\n\nModel df: 0.   Total lags used: 10\n\n\nFor IWM, the squared residuals showed a GARCH(1,1) fit, and this is also what the function returned as the model with the lowest AIC value. All of the terms were significant at the p &lt; 0.01 threshold, and the Ljung-Box test returns 0.498, improving from 0.088 in the ARMA(4,0,4) model alone. This suggests the GARCH fit has done a very good job at removing autocorrelation from the resdiuals.\n\n\nCode\ngarchFit(formula = ~arma(4,2) + garch(1,1), data = iwmIn$iwmRange, trace = FALSE)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(4, 2) + garch(1, 1), data = iwmIn$iwmRange, \n    trace = FALSE) \n\nMean and Variance Equation:\n data ~ arma(4, 2) + garch(1, 1)\n&lt;environment: 0x000000002d7fd900&gt;\n [data = iwmIn$iwmRange]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1          ar2          ar3          ar4          ma1  \n 3.4855e-04   6.6410e-01   5.6820e-01  -1.7672e-01  -7.5180e-02  -3.9551e-01  \n        ma2        omega       alpha1        beta1  \n-4.7133e-01   1.7350e-06   5.8615e-02   9.0767e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      3.485e-04   3.509e-04    0.993  0.32058    \nar1     6.641e-01   1.613e-01    4.118 3.83e-05 ***\nar2     5.682e-01   2.229e-01    2.549  0.01080 *  \nar3    -1.767e-01   5.433e-02   -3.253  0.00114 ** \nar4    -7.518e-02   6.747e-02   -1.114  0.26516    \nma1    -3.955e-01   1.592e-01   -2.484  0.01299 *  \nma2    -4.713e-01   1.791e-01   -2.631  0.00851 ** \nomega   1.735e-06   1.000e-06    1.734  0.08287 .  \nalpha1  5.861e-02   1.936e-02    3.027  0.00247 ** \nbeta1   9.077e-01   3.244e-02   27.979  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2457.18    normalized:  3.56113 \n\nDescription:\n Thu Dec 07 01:56:29 2023 by user: corwi \n\n\nWhen pulling all of the variables together, the final model diagnostic suggests that the moving average terms 3 and 4 are not significant. As such, I will write the equation for the final selected model of ARMA(4,2) + GARCH(1,1):\nFinal formula is: Rt = 0.00035 + 0.664 * R(t-1) + 0.568 * R(t-2) - 0.177 * R(t-3) - 0.075 * R(t-4) - 0.396 * W(t-1) - 0.471 * W(t-2) + At\nAt = Sigma(t) * Epsilon(t)\nSigma(t) ^ 2 = 0.00000017 + .0586 * a(t-1)^2 + 0.908 * Sigma(t-1)^2"
  },
  {
    "objectID": "Deep Learning for TS.html",
    "href": "Deep Learning for TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nimport subprocess\n\nimport sys\nimport os \n\ndef install(name):\n    subprocess.call([sys.executable, '-m', 'pip', 'install', name])\n\n#install('yfinance')\n\n\nWARNING:tensorflow:From C:\\Users\\corwi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead."
  },
  {
    "objectID": "Deep Learning for TS.html#declare-useful-functions",
    "href": "Deep Learning for TS.html#declare-useful-functions",
    "title": "Deep Learning for TS",
    "section": "Declare Useful Functions",
    "text": "Declare Useful Functions\nHere, I would like to mention that this code is based on the deep learning lab code we went over in class. I have adapted it here to apply it to my data, with modifications in some areas as my data functions slightly differently than the examples in the lab.\n\n\nCode\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "Deep Learning for TS.html#read-in-data",
    "href": "Deep Learning for TS.html#read-in-data",
    "title": "Deep Learning for TS",
    "section": "Read in Data",
    "text": "Read in Data\n\n\nCode\nimport yfinance as yf\n\nspy = yf.download('SPY',start = \"2021-01-01\", end = \"2023-09-30\")\niwm = yf.download('QQQ',start = \"2021-01-01\", end = \"2023-09-30\")\nqqq = yf.download('IWM',start = \"2021-01-01\", end = \"2023-09-30\")\n\nspyRange = (spy['High'] - spy['Low']) / spy['Close']\niwmRange = (iwm['High'] - iwm['Low']) / iwm['Close']\nqqqRange = (qqq['High'] - qqq['Low']) / qqq['Close']\n\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed"
  },
  {
    "objectID": "Deep Learning for TS.html#normalize",
    "href": "Deep Learning for TS.html#normalize",
    "title": "Deep Learning for TS",
    "section": "Normalize",
    "text": "Normalize\n\n\nCode\n# Slice training set\nspy_train = pd.DataFrame( spyRange[range(0,490)] )\nspy_train = spy_train.reset_index(drop = True)\n\n# Slice validation set\nspy_val =  pd.DataFrame( spyRange[range(490,590)] )\nspy_val = spy_val.reset_index(drop = True)\n\n# Slice test set\nspy_test =  pd.DataFrame( spyRange[range(590,690)] )\nspy_test = spy_test.reset_index(drop = True)\n\n# Save training normalization to use on test and validation sets\nspy_train_mean = spy_train.mean()\nspy_train_std = spy_train.std()\n\n# Normalize\nspy_train=(spy_train-spy_train_mean)/spy_train_std\nspy_val=(spy_val-spy_train_mean)/spy_train_std\nspy_test=(spy_test-spy_train_mean)/spy_train_std"
  },
  {
    "objectID": "Deep Learning for TS.html#fit-helper-function-to-create-data-in-desired-format",
    "href": "Deep Learning for TS.html#fit-helper-function-to-create-data-in-desired-format",
    "title": "Deep Learning for TS",
    "section": "Fit Helper Function to Create Data in Desired Format",
    "text": "Fit Helper Function to Create Data in Desired Format\n\n\nCode\n# code from lab\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n\n    # verbose=True --&gt; report and plot for debugging\n    # unique=True --&gt; don't re-sample:\n    # x1,x2,x3 --&gt; x4 then x4,x5,x6 --&gt; x7 instead of x2,x3,x4 --&gt; x5\n    # initialize\n    i_start=0; count=0;\n\n    # initialize output arrays with samples\n    x_out=[]\n    y_out=[]\n\n    # sequentially build mini-batch samples\n    while i_start+lookback+delay&lt; x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n    \n        # report if desired\n        if verbose and count&lt;2: print(\"indice range:\",i_start,i_stop,\"--&gt;\", i_pred)\n\n        # define arrays:\n        # method-1: buggy due to indexing from left\n        # numpy's slicing --&gt; start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while j&gt;=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n        \n        # create mini-batch sample\n        xtmp=x.iloc[indices_to_keep,:] # isolate relevant indices\n        \n        xtmp=xtmp.iloc[:,feature_columns] # isolate desire features\n        ytmp=x.iloc[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp);\n        \n        # report if desired\n        if verbose and count&lt;2: print(xtmp, \"--&gt;\",ytmp)\n        if verbose and count&lt;2: print(\"shape:\",xtmp.shape, \"--&gt;\",ytmp.shape)\n        \n        if verbose and count&lt;2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n        \n        # UPDATE START POINT\n        if unique: i_start+=lookback\n        i_start+=1; count+=1\n\n    return np.array(x_out),np.array(y_out)\n\n\nDiagnostic plotting functions from the lab\n\n\nCode\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n\n# UTILITY FUNCTION\ndef regression_report(yt,ytp,yv,yvp):\n\n    print(\"---------- Regression report ----------\")\n\n    print(\"TRAINING:\")\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n    title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]);\n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n    print(\"VALIDATION:\")\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n    title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\n\nUse function to reformat data. Train will be 1-490, Validation 491-590, and Test 591-690\n\n\nCode\n#spy_train = generator(spyRange, lookback=lookback, delay=delay, min_index=0, max_index=489, shuffle=True, step=step, batch_size=batch_size)\n#spy_val = generator(spyRange, lookback=lookback, delay=delay, min_index=490, max_index=589, step=step, batch_size=batch_size)\n#spy_test = generator(spyRange, lookback=lookback, delay=delay, min_index=590, max_index=690, step=step, batch_size=batch_size)\n\n\n\nL = 30\nS = 1\nD = 1\nbatch_size = 10 \n\n\nspy_x,spy_y=form_arrays(spy_train,lookback=L,delay=D,step=S,unique=False,verbose=False)\n\nspy_val_x, spy_val_y = form_arrays(spy_val,lookback=L,delay=D,step=S,unique=False,verbose=False)\n\n\nReshape data\n\n\nCode\n# RESHAPE INTO A DATA FRAME\nspy_t_1 = spy_x.reshape(spy_x.shape[0],spy_x.shape[1]*spy_x.shape[2])\nspy_v_1 = spy_val_x.reshape(spy_val_x.shape[0],spy_val_x.shape[1]*spy_val_x.shape[2])\n\n\ninput_shape = (spy_t_1.shape[1],)\nrnn_input_shape = (spy_x.shape[1], spy_x.shape[2])\n\n# NEW SIZES\nprint(\"train: \", spy_x.shape,\"--&gt;\",spy_t_1.shape)\nprint(\"validation: \", spy_val_x.shape,\"--&gt;\",spy_v_1.shape)\n\n\ntrain:  (459, 31, 1) --&gt; (459, 31)\nvalidation:  (69, 31, 1) --&gt; (69, 31)"
  },
  {
    "objectID": "Deep Learning for TS.html#fit-gru",
    "href": "Deep Learning for TS.html#fit-gru",
    "title": "Deep Learning for TS",
    "section": "Fit GRU",
    "text": "Fit GRU\nNo regularlization\n\n\nCode\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_1_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=30, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\nWARNING:tensorflow:From C:\\Users\\corwi\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From C:\\Users\\corwi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n\n\nLet’s plot results\n\n\nCode\nhistory_plot(history_spy_1_noreg)\n\n\n\n\n\nRegularlization\n\n\nCode\nfrom tensorflow.keras import regularizers\nL1=0\nL2=1e-3\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape,kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n# layer that reads the recurent layer\nmodel.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5,kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_1_reg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=30, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\nLet’s look at the diagnostic plot\n\n\nCode\nhistory_plot(history_spy_1_reg)\n\n\n\n\n\nCompared to the model without regularization, we can see that the training set error decreases more steadily, without regressing higher very often. The consequence of this is that the raining error goes lower than the validation error faster than without regularization, suggesting regularization is helping the model to learn more purposefully and successfully."
  },
  {
    "objectID": "Deep Learning for TS.html#fit-rnn",
    "href": "Deep Learning for TS.html#fit-rnn",
    "title": "Deep Learning for TS",
    "section": "Fit RNN",
    "text": "Fit RNN\n\n\nCode\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.SimpleRNN(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_2_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n\n\nCode\nhistory_plot(history_spy_2_noreg)\n\n\n\n\n\nWe can see the validation loss of the simple RNN model is actually almost equivalent to both the regularized and non-regularized GRU models above. Specifically, all 3 models tended to initialize around 0.6 loss for the validation set, and then slowly work down to about 0.55. If anything, the RNN model might perform slightly better, with validation loss decreasing below 0.55 at the later-stage epochs."
  },
  {
    "objectID": "Deep Learning for TS.html#fit-lstm",
    "href": "Deep Learning for TS.html#fit-lstm",
    "title": "Deep Learning for TS",
    "section": "Fit LSTM",
    "text": "Fit LSTM\n\n\nCode\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.LSTM(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_3_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n\n\nCode\nhistory_plot(history_spy_3_noreg)\n\n\n\n\n\nWith one LSTM layer, the model already seems better than the RNN and GRU competitiors. The validation loss starts out around 0.56 and stays in the lower end of the 0.55-0.6 range. Let’s try adding another layer:\n\n\nCode\n#from sklearn.neural_network import MLPRegressor\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer= regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer = regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer = regularizers.L1L2(l1=L1, l2=L2)))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(32, activation='relu',kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.Dense(1))\n#model.add(MLPRegressor())\n\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\n#  trying diff shape\n\nhistory_spy_4_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n\n\nCode\nspy_4_t_pred = model.predict(spy_x)\nspy_4_t_pred = spy_4_t_pred\n\nspy_4_v_pred = model.predict(spy_val_x)\nspy_4_v_pred.shape\n\n\n 1/15 [=&gt;............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/15 [=================&gt;............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/15 [==============================] - 1s 7ms/step\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3/3 [==============================] - 0s 7ms/step\n\n\n(69, 31, 1)\n\n\n\n\nCode\nhistory_plot(history_spy_4_noreg)\n\n\n\n\n\nWith a much larger model of 3x64 neuron LSTM layers, we can see that the validation loss is still around 55. However, the training loss has a consistent curve down to the validation loss. It is interesting that even in the later epochs, the validation loss remains lower than the training."
  },
  {
    "objectID": "Deep Learning for TS.html#comparing-results-of-deep-learning-models",
    "href": "Deep Learning for TS.html#comparing-results-of-deep-learning-models",
    "title": "Deep Learning for TS",
    "section": "Comparing Results of Deep Learning Models",
    "text": "Comparing Results of Deep Learning Models\nOf the three neural network models, the LSTM provided the most consistent performance, while the RNN provided the best performance. The GRU model performed slightly worse than the other two, as measured by its validation set performance. Interestingly, the complexity required for the model to avoid underfitting was much higher with the LSTM model, where 3 hidden layers of 64 neurons each were needed before I felt the training plot showed excess training performance over validation performance. With smaller models, the LSTM performed somewhat poorly on the training set in comparison to the validation set. In terms of accuracy, the RNN model was notable for sometimes reaching below 0.55 loss in performance on the validation set, while the LSTM model usually reached around 0.56 and the GRU model between 0.56 and 0.6. In terms of predictive power (extrapolating outside the validation set) I would tend to trust the LSTM model, because it had the most consistent performance once it was trained over a number of epochs, and even boasted the lowest loss on a particular epoch, sometimes reaching as low as 0.51 in the validation set, although these results were not the norm.\nRegularization was an important element across the models. It had two main benefits: First, the models achieved better performance faster because they had more consistent improvement and less variation in performance between epochs. Models which included regularization (both L1L2 and dropout regularization) had almost monotonically decreasing loss in the training set, and clear trends toward improvement in the validation set.\nThe deep learning models will only have the performance described above when they predict 1 observation into the future. This is because they take in the past 30 observations and return 1 prediction. All of the loss estimates are for this 1-step ahead prediction. If we were to predict further into the future with the models, there would be drift or compound error, where the models loss from a previous prediction would be inherited in future predictions that relied upon that past prediction as an input into the model.\nThe deep learning modelling is more complicated than the univariate modeling from HW3, because there are a range of hyperparameters introduced that make the process of selecting the best model more complicated. Unlike a simple ARIMA model, it is not possible to iterate through all combinations of hidden layer sizes and complexities. As such, I think there is more subjective elements introduced to neural network modeling."
  },
  {
    "objectID": "Deep Learning for TS.html#comparing-deep-learning-and-traditional-ts-models",
    "href": "Deep Learning for TS.html#comparing-deep-learning-and-traditional-ts-models",
    "title": "Deep Learning for TS",
    "section": "Comparing Deep Learning and Traditional TS Models",
    "text": "Comparing Deep Learning and Traditional TS Models\nWe identified ARIMA(3,0,3) as a good model for the SPY data. Lets look at the one step ahead precition of the ARIMA and best-performing (LSTM) neural network model in the validation data:\n\n\nCode\nfrom statsmodels.tsa.arima.model import ARIMA\n\nmodelA = ARIMA(spy_train, order = (3,0,3))\nmodelA = modelA.fit()\n\nforecasts_arima = modelA.forecast(100)\n\npredict_arima = ARIMA(spy_val, order = (3,0,3))\npredict_arima = predict_arima.fit()\n\narimapreds = list()\n\n\n\nindexvals = pd.array(range(0,69))\narimavals = pd.array(predict_arima.get_prediction().predicted_mean)\narimavals = arimavals[0:69]\nnnvals = spy_4_v_pred[:,0,0]\n\n\nplt.plot(indexvals, arimavals, label='ARIMA Prediction')\nplt.plot(indexvals, nnvals, label='NN Prediction')\nplt.plot(indexvals, spy_val_y, label='Real Values')\nplt.legend()\nplt.title(\"One Step Ahead Forecasts in Validation (NN vs. ARIMA)\")\nplt.show()\n\nrealVals = spy_val_y\n\ndef rmse(pred, tar):\n    return np.sqrt(((pred - tar) ** 2).mean())\n\n\nprint(\"arima: \", rmse(arimavals, spy_val_y))\nprint(\"Neural Net: \", rmse(nnvals, spy_val_y))\n\n\nC:\\Users\\corwi\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\nC:\\Users\\corwi\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n\n\n\n\n\narima:  0.7763029583389425\nNeural Net:  0.7090497569990377\n\n\nLooking at a chart of our best-performing Neural Network Model (LSTM) vs. my best performing ARIMA model (3,0,3) from earlier work, vs. the real observed values, a few things become clear. One, neither model type does a perfect job, especially of capturing extremely high or low values. Two, the ARIMA predictions increase and decrease more (have a higher range) thant he NN predictions. However, the ARIMA predictions tend to vary further from 0, while the NN predictions stay close to zero and only alter slightly. Because of this, the RSME of the neural network is actually lower than the ARIMA model, with the neural network having 0.71 RMSE and the ARIMA model having 0.78 RMSE.\nHowever, the neural network is still not the best model I have tested in this project, because the VAR model achieved an RMSE of 0.68 on the same dataset. This suggests that while Neural networks are powerful tools for univariate prediction, the most accurate model predictions may still be those that include exogenous variables."
  },
  {
    "objectID": "Data Sources.html",
    "href": "Data Sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Financial data on the prices of different stocks are readily available in both R and Python through various packages, and amongst these the Yahoo Finance package is a particularly popular option. To approximate the daily range in value of major indices, I chose to look at popular low-cost ETFs which attempt to track the value of stocks contained in these indices. For the S&P 500, this is SPY, run by State Street, for the Russel 2000 this is iShare’s IWM, and for the NASDAQ 100 it is the ever-popular Invesco QQQ fund. The data I haave gathered for these tickers includes the open and close of stock prices for each day, as well as the highest and lowest price recorded for the day. These indices are widely used and will be a relevant outcome variable for the project to study. The main output value that will be calculated is the high-low range of the day.\n\n\n\nSPY Historical Data\nIWM Historical Data\nQQQ Historical Data"
  },
  {
    "objectID": "Data Sources.html#united-states-stock-indices---intraday-volatility",
    "href": "Data Sources.html#united-states-stock-indices---intraday-volatility",
    "title": "Data Sources",
    "section": "",
    "text": "Financial data on the prices of different stocks are readily available in both R and Python through various packages, and amongst these the Yahoo Finance package is a particularly popular option. To approximate the daily range in value of major indices, I chose to look at popular low-cost ETFs which attempt to track the value of stocks contained in these indices. For the S&P 500, this is SPY, run by State Street, for the Russel 2000 this is iShare’s IWM, and for the NASDAQ 100 it is the ever-popular Invesco QQQ fund. The data I haave gathered for these tickers includes the open and close of stock prices for each day, as well as the highest and lowest price recorded for the day. These indices are widely used and will be a relevant outcome variable for the project to study. The main output value that will be calculated is the high-low range of the day.\n\n\n\nSPY Historical Data\nIWM Historical Data\nQQQ Historical Data"
  },
  {
    "objectID": "Data Sources.html#investor-confidence---cboes-vix",
    "href": "Data Sources.html#investor-confidence---cboes-vix",
    "title": "Data Sources",
    "section": "Investor Confidence - CBOE’s VIX",
    "text": "Investor Confidence - CBOE’s VIX\nhttps://finance.yahoo.com/quote/%5EVIX/history?p=%255EVIX\nThe Chicago Board of Exchange (CBOE) VIX index is a widely-used tool to measure investor sentiment. The indicator itself represents the degree of volatility perceived by investors in the next month. It ranges from near zero up to about 60 in recent years, with scores at different intervals representing different levels of perceived volatility.\n\nSources\n\nMethodology of the VIX\nVIX Historical Data"
  },
  {
    "objectID": "Data Sources.html#work-stoppages",
    "href": "Data Sources.html#work-stoppages",
    "title": "Data Sources",
    "section": "Work Stoppages",
    "text": "Work Stoppages\nThis dataset contains all of the major strikes recorded by the bureau of labor statistics, such that I will be able to use the number of striking workers on any given day as a variable for analysis.\n\nSources\n\nLabor Deparment Data"
  },
  {
    "objectID": "Data Sources.html#daily-3-month-yield-curve",
    "href": "Data Sources.html#daily-3-month-yield-curve",
    "title": "Data Sources",
    "section": "Daily 3 Month Yield Curve",
    "text": "Daily 3 Month Yield Curve\nWhile I originally considered the actual federal funds rate, the values did not change much day to day. Instead I would like to use the expectation of yield rates, measured through the pricing of bonds in the yield curve over the next few months. The data includes the prices of treasury bills 1, 2, and 3 months out on each date.\n\nSources\n\nTreasury Department Data"
  },
  {
    "objectID": "Data Sources.html#storm-events-by-day",
    "href": "Data Sources.html#storm-events-by-day",
    "title": "Data Sources",
    "section": "Storm Events by Day",
    "text": "Storm Events by Day\nThe National Oceanic and Atmospheric Administration (NOAA) maintains records on each storm warning they send out for the entire country. This dataset includes many types of storm warnings, such as for hurricanes, blizzards, flash floods, and tornadoes. Importantly, the dataset includes the day in which each weather event took place, as well as the property damage and bodily injury caused by the weather event.\n\nNOAA Data"
  },
  {
    "objectID": "ARMA ARIMA SARIMA Models.html",
    "href": "ARMA ARIMA SARIMA Models.html",
    "title": "ARMA ARIMA SARIMA MODELS",
    "section": "",
    "text": "Loading packages\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(lubridate)\nBringing the data into this tab as well:\nShow Code\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\ndiff1SPY &lt;- diff(spyIn$spyRange)\ndiff1QQQ &lt;- diff(qqqIn$qqqRange)\ndiff1IWM &lt;- diff(iwmIn$iwmRange)"
  },
  {
    "objectID": "ARMA ARIMA SARIMA Models.html#stationarity-of-the-time-series",
    "href": "ARMA ARIMA SARIMA Models.html#stationarity-of-the-time-series",
    "title": "ARMA ARIMA SARIMA MODELS",
    "section": "Stationarity of the Time Series",
    "text": "Stationarity of the Time Series\nBased on previous results, and the fact that I am using “pseudo-differenced data” in that I am taking the percentage range in prices, in addition to a single differencing, means that the time series are stationary. As evidenced by Dickey-Fuller Tests."
  },
  {
    "objectID": "ARMA ARIMA SARIMA Models.html#building-arima-model",
    "href": "ARMA ARIMA SARIMA Models.html#building-arima-model",
    "title": "ARMA ARIMA SARIMA MODELS",
    "section": "Building ARIMA Model",
    "text": "Building ARIMA Model\n\nQQQ\nSince I did some of this work with SPY data on the EDA tab, I will focus on QQQ range data here, after it has been differenced.\n\n\nCode\n#print(diff1QQQ)\nacf(diff1QQQ, na.action = na.exclude, main = \"Differenced QQQ Daily Range ACF Plot\")\n\n\n\n\n\nCode\npacf(diff1QQQ, na.action = na.exclude, main = \"Differenced QQQ Daily Range PACF Plot\")\n\n\n\n\n\nBased on these charts the order I would pick for QQQ is: ARIMA(2,1,0)\n\n\nCode\nmodelQQQ1 &lt;- arima(diff1QQQ, order = c(2,1,0))\n#summary(modelQQQ1)\n\n\n\nThe equation for an ARIMA model with the above formula is x = -1.0034x(t-1) - 0.4832x(t-2) + error.\nModel Diagnostic:\n\n\n\nCode\nstats::tsdiag(modelQQQ1)\n\n\n\n\n\nThe Ljung Box statistics look good, although the ACF of the residuals does have 1 significant term.I originally tried a (4,1,2) model, however the ljung box statistics were highly correlated, and I suspected overfitting. After reducing the parametrization greatly, the new model performed mnuch better.\n\nComparing the Auto ARIMA method:\n\n\n\nCode\nautoQQQ &lt;- auto.arima(diff1QQQ)\n#summary(autoQQQ)\n\n\nThe auto.arima method chose an ARIMA(1,0,1) model. However, this model did not perform as well in terms of AIC, with the Auto arima model having a score of -4799 while my model had a score of -4292.\n\nForecasting with my model\n\n\n\nCode\nplot(forecast(modelQQQ1, 10), xlim = c(650,750))   \n\n\n\n\n\nForecasting with auto arima model\n\n\nCode\nplot(forecast(autoQQQ, 10), xlim = c(650,750))\n\n\n\n\n\nOverall, my model has a slightly more dynamic prediction than the auto arima function, which quicly levels out to 0. However, my model also has a much wider uncertainty band.\n\nCompare ARIMA model with benchmarks\n\n\n\nCode\nnaiveModelQQQ &lt;- naive(diff1QQQ, h=1)\nsnaiveModelQQQ &lt;- snaive(diff1QQQ, h=1)\n\n#summary(naiveModelQQQ)\n#summary(snaiveModelQQQ)\n#summary(modelQQQ1)\n\n\nI fit a naive and seasonal naive model. On RMSE my model had the best performance, with 0.011, while the naive and snaive models had 0.017 rmse each (since there was no seasonal period I realized they were the same model). On MAE my arima model had 0.008 while the seasonal naive models had 0.0122.\nLet’s compare forecasts:\n\n\nCode\nplot(forecast(modelQQQ1, 10), xlim = c(650,750)) \n\n\n\n\n\n\n\nCode\nplot(forecast(naiveModelQQQ, 10), xlim = c(650,750))   \n\n\n\n\n\nHere, the naive method can only forecast 1 observation into the future, since the seasonal period is one. Which is an advantage to my model, but realistically means the naive model should be evaluated with cross validation.\n\n\nSPY\nNow, let’s gather ARIMA models for the intraday range. First we declare the helper function\n\n\nCode\narimaResults &lt;- function(data) {\n    \n    i=1\n    temp= data.frame()\n    ls=matrix(rep(NA,6*100),nrow=100) # roughly nrow = 3x4x2\n\n    for (p in 1:6)# p=1,2,3 : 3\n    {\n        for(q in 1:6)# q=1,2,3,4 :4\n        {\n            for(d in 1:2) {\n                \n                if(p-1+d - 1+q-1&lt;=8) #usual threshold\n                {\n                    \n                    model&lt;- Arima(data,order=c(p-1,d-1,q-1),include.drift=FALSE) \n                    ls[i,]= c(p-1,d-1,q-1,model$aic,model$bic,model$aicc)\n                    i=i+1\n                    #print(i)\n                    \n                }\n\n            }\n                \n        }\n    }\n\n    temp= as.data.frame(ls)\n    names(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n    return(temp[which.min(temp$AIC),])\n    #return(temp)\n}\n\n\nPlotting the daily ranges:\nSPY\n\n\nCode\nplot(spyIn$spyRange, main = \"SPY Daily Range\")\n\n\n\n\n\nThe SPY data certainly appears to have clustered volatility, as there are large swings in close proximity to eachother. But it also has noticeable trend, with the intraday ranges being higher during the middle of the time series before dropping down again towards the end. To combat this, let’s look at the differenced series:\n\n\nCode\nplot(spyIn$spyRange %&gt;% diff(), main = \"Differenced SPY Daily Range\")\n\n\n\n\n\nThe differenced data looks noisy, and perhaps over differenced. As we saw on the previous page. Let’s see which arima models had the lowest AIC and BIC scores.\nNow, let’s find good-fitting models starting with SPY:\n\n\nCode\narimaResults(spyIn$spyRange)\n\n\n   p d q       AIC       BIC      AICc\n43 3 0 3 -5139.956 -5103.662 -5139.744\n\n\nThe best model returned by the function is ARMA(3,0,3). It is surprising that the models which differenced the data didn’t perform better, but I manually reviewed those options and they all had worse AIC and BIC scores.\n\n\nIWM\nLet’s check IWM:\n\n\nCode\nplot(iwmIn$iwmRange)\n\n\n\n\n\nIWM’s intraday range looks slightly more stationary than the other 2 time series, and also has more extreme changes in volatility over time. However, we can still see somewhat of a trend, such that a moving average would be obviously nonstationary, so once again we difference the series.\n\n\nCode\nplot(iwmIn$iwmRange %&gt;% diff())\n\n\n\n\n\nWhile the variance looked elevated in the non-differenced plot, it is actually less hesteroskedastic in the differenced plot. This suggests IWM may not have as much autocorrelation in the variance as the other two indices. An ARIMA model alone might suffice here, lets look for the parameters that yield the lowest AIC and BIC.\nFor IWM:\n\n\nCode\narimaResults(iwmIn$iwmRange)\n\n\n   p d q       AIC       BIC      AICc\n56 4 0 4 -4857.373 -4812.006 -4857.049\n\n\nFitting the pattern of the best models for the financial instruments being of the form ARMA(N, 0, N), the model with the lowest AIC for IWM was ARMA(4,0,4)."
  },
  {
    "objectID": "ARMA ARIMA SARIMA Models.html#sarima",
    "href": "ARMA ARIMA SARIMA Models.html#sarima",
    "title": "ARMA ARIMA SARIMA MODELS",
    "section": "SARIMA",
    "text": "SARIMA\nLet’s look for a seasonal affect in the ACF plots, using the weather events data. First we prepare the data:\n\n\nCode\nweather_data &lt;- read.csv('data/storms_clean.csv')\n\n\nweather_data$month &lt;- weather_data$BEGIN_YEARMONTH %% 100\n\nweather_data &lt;- weather_data %&gt;%\n    mutate(realdate = make_date(YEAR, month, BEGIN_DAY)) %&gt;%\n    mutate(DAMAGE_PROPERTY =  str_replace(DAMAGE_PROPERTY, \"K\", \"\") )  %&gt;%\n    mutate(DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)) %&gt;%\n    mutate(DAMAGE_PROPERTY = replace_na(DAMAGE_PROPERTY, 0))\n\n\n# Daily Event Number\ndaily_events &lt;- weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(events = length(EPISODE_ID))\n\n# Daily Property Damage\ndaily_property &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(pdam = sum(DAMAGE_PROPERTY))\n\n# Daily Daily Casualties\ndaily_casualties &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(casualties = sum(INJURIES_DIRECT) + sum(INJURIES_INDIRECT) + sum(DEATHS_DIRECT) + sum(DEATHS_INDIRECT))\n\n# Daily Hurricanes\ndaily_hurricanes &lt;- weather_data %&gt;%\n    filter(EVENT_TYPE == \"Hurricane\") %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(hurricaneWarnings = length(EPISODE_ID))\n\n\n# Daily joined data\n\nweather_merged &lt;- full_join(daily_events, daily_property, by = \"realdate\")\nweather_merged &lt;- full_join(weather_merged, daily_casualties, by = 'realdate')\nweather_merged &lt;- full_join(weather_merged, daily_hurricanes, by = 'realdate')\n\nhead(weather_merged)\n\n\n# A tibble: 6 x 5\n  realdate   events  pdam casualties hurricaneWarnings\n  &lt;date&gt;      &lt;int&gt; &lt;dbl&gt;      &lt;int&gt;             &lt;int&gt;\n1 2021-01-01    643  918           1                NA\n2 2021-01-02     75  189.          0                NA\n3 2021-01-03     80   63           3                NA\n4 2021-01-04     44   20           0                NA\n5 2021-01-05     18    0           2                NA\n6 2021-01-06     27  850           2                NA\n\n\nNow, lets look at the acf plot:\n\n\nCode\nacf(weather_merged$events, lag.max = 365, main = \"ACF, Daily Weather Events\")\n\n\n\n\n\nWith a 365 lag plot (as we are looking at weather data), we can see that for about 1/4 of the 365 lags, there is some positive correlation in the residuals (the same season), then for 1/2 the lags after that there is negative correlation (the opposite seasons), and then a return to significant positive correlation about 3/4 of the way through the data. This appears to show a clear seasonal effect of about 365. So let’s seasonally difference the data.\n\n\nCode\nseasonDiff &lt;- weather_merged$events %&gt;% diff(lag = 365)\nacf(seasonDiff, lag.max = 365, main = \"ACF, 365 Day Differenced Weather Events\")\n\n\n\n\n\nCode\npacf(seasonDiff, main = \"PACF, 365 Differenced Weather Events\")\n\n\n\n\n\nAfter seasonal differencing, this plot looks much much better, without noticeable season-to-season correlations in the lags, although there is still some short-term correlation. And some repeating period which appears to be almost weekly in the residuals.\nBased on the ACF and PACF plots, I would consider p of 1, d of 0, and q of 3. Then for P and Q I might consider 0, D would be 1 since we seasonally differenced. But let’s run some code to see the AIC of different values:\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \ntemp=c()\nd=1\nD=1\ns=12\n \ni=1\ntemp= data.frame()\nls=matrix(rep(NA,9*378),nrow=378)\n \nfor (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n#SARIMA.c(p1=1,p2=5,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=weather_merged$events) %&gt;% filter(!is.na(p))\n\n#SARIMA.c(p1=1,p2=5,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=seasonDiff) %&gt;% filter(!is.na(p))\n\n\nBased on the results of the function, the minimum AIC and BIC are for the model: (0,1,2)(0,1,0). If I run the SARIMA function on the 365 differenced data, then it returns (1,1,1,)(0,1,0) as the model with the lowest AIC. So I will compare these two models for the series, using diagnostics.\n\n\nCode\nmod1 &lt;- Arima(weather_merged$events,order=c(0,1,2),seasonal=c(0,1,0))\nmod2 &lt;- Arima(weather_merged$events,order=c(1,1,1),seasonal=c(0,1,0))\n\ncheckresiduals(mod1 )\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,2)\nQ* = 8.0235, df = 8, p-value = 0.4312\n\nModel df: 2.   Total lags used: 10\n\n\nThe residuals for model 1 (0,1,2)(0,1,0) show some clustering of volatility. In addition, they appear to be skewed to the right, as the right tail of the residual distribution is fatter than the left tail and has more outlying values. Finally, the ACF plot of the residuals looks good, with little visible correlation and no values crossing the significance line. In addition, the Ljung-Box test returns p=0.43, suggesting we can reject the idea that there is autocorrelation in the residuals.\n\n\nCode\ncheckresiduals(mod2 )\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,1)\nQ* = 2.7198, df = 8, p-value = 0.9507\n\nModel df: 2.   Total lags used: 10\n\n\nThe residual diagnostics for model 2 (1,1,1)(0,1,0) are similar to model 1, except that they have even less correlatoin visible in the residual plots. The residual’s distribution is still skewed to the right, with a fatter positive tail. The ACF plot, however, has even less correlation visible, with only two values even coming marginally close to the significance line. There is still some heteroskedacticity in the plot over time, however, suggesting clustering of volatility.\nNow, let’s use an Auto.Arima function to determine the correct model:\n\n\nCode\naaData &lt;- ts(weather_merged$events)\nmod3 &lt;- auto.arima(aaData, seasonal = TRUE, trace = FALSE)\n\n\nRegardless of the frequency fed into the model, Auto.arima only wants to fit a (2,0,1) ARIMA model. I tried 365, 90, 60, 40, 14, and 7 day frequencies, and in each case the seasonal term was not chosen for the data. I think the reason that it doesn’t recognize the seasonality is because it doesn’t worok with 365, which should be the best frequency for the data.\nForecast with a confidence band: Model 1 (0,1,2)(0,1,0)\n\n\nCode\nplot(forecast(mod2), xlim = c(850,925))\n\n\n\n\n\nModel 2: (1,1,1)(0,1,0)\n\n\nCode\nplot(forecast(mod1), xlim = c(850,925))\n\n\n\n\n\nI think the forecasts are interesting, because model 2 has a more dynamic forecast, with a decrease over several days before leveling out its prediction. Model 1, meanwhile, predicts that the series will hardly change after its first prediction. While the series does not have a lot of trend going into the prediction interval, weather events are dynamic and I would tend to believe the model which includes more variation as opposed to constant numbers of events. Hence, I would select the (1,1,1)(0,1,0) SARIMA model.\nBenchmark Comparison\nWe will use two benchmark methods: A seasonal naive forecast and a mean forecast.\n\n\nCode\nbase1 &lt;- snaive(aaData, 10)\nbase2 &lt;- meanf(aaData, 10)\n\nplot(base1, xlim = c(850,925))\n\n\n\n\n\nHere we can see the plot for the seasonal naive model’s forecasts (10 days out), which show a predicted value close to the last observed value in the series. It has a high degree of uncertainty as shown by the prediction interval, which is quite wide.\n\n\nCode\nplot(base2, xlim = c(850,925) ) \n\n\n\n\n\nThe forecast for the meanf model (above) departs further from the previously observed values, as the mean of the series is substantially below recently observed values. However, the model has a smaller prediction interval than the seasonal naive forecast, which is a slight advantage.\nNow let’s look at the accuracy of the three forcasts:\n\n\nCode\naccuracy(snaive(aaData))\n\n\n                    ME     RMSE      MAE       MPE    MAPE MASE       ACF1\nTraining set -0.232967 222.2549 134.6615 -92.97186 141.104    1 -0.3466138\n\n\nCode\naccuracy(meanf(aaData))\n\n\n                       ME     RMSE      MAE       MPE     MAPE     MASE\nTraining set 4.269057e-15 195.8141 137.4244 -231.3227 259.5254 1.020517\n                  ACF1\nTraining set 0.3527022\n\n\nCode\naccuracy(mod2)\n\n\n                   ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 1.865123 182.4393 120.2761 -152.6725 179.3078 0.8931731\n                    ACF1\nTraining set 0.006055532\n\n\nThe accuracy statistics were a mixed result between the seasonal naive forecast and the SARIMA(1,1,1)(0,1,0) model. The mean forecast did not perform better on any metric than the other two models, and could be discarded. On Root Mean Squared Error, the SARIMA model beat the Seasonal Naive model with a value of 182.4 vs. 222.3, respectively. On Mean Absolute Error, the SARIMA model also performed better, with 120.3 compared to the Seasonal Naive model’s 134.7. On the Mean Absolute Percentage Error, however, the Seasonal Naive model performed better than the SARIMA model, achieving 141.1 vs. 179.3 for the SARIMA model.\nOverall, it seems like the accuracy metrics might favor the seasonal naive model, while the prediction forecasts look more accurate for the SARIMA model.\nCross Validation: Let’s do a seasonal cross-validation with 1 and 10-step-ahead forecasts.\n\n\nCode\n    # I add a 100 day buffer to my period of 365 days to have enough data\n    test &lt;- 100 \n    trainnum &lt;- length(aaData) - test\n    rmse1 &lt;- vector(mode = 'numeric', length = 100)\n    rmse2 &lt;- vector(mode = 'numeric', length = 100)\n    rmse361 &lt;- vector(mode = 'numeric', length = 100)\n    rmse362 &lt;- vector(mode = 'numeric', length = 100)\n\n    for(i in 1:100) {\n\n        \n        xtrain &lt;- aaData[c(1:(trainnum + i - 1))]\n        xtest &lt;-  aaData[c((trainnum + i +1):(trainnum+i+10))]\n        \n    \n        ######## model ###########\n        fit2 &lt;- arima(xtrain, order = c(1,1,1), seasonal = list(order = c(0,1,0)))\n        fcast2 &lt;- predict(fit2, n.ahead = 10)$pred\n        \n        # Errors\n\n        rmse2[i]  &lt;-sqrt((fcast2[1]-xtest[1])^2)\n        rmse362[i]  &lt;- mean( sqrt((fcast2 -xtest)^2) )\n        \n    }\n    \n# create index\nindex &lt;- c(1:100)\nggplot() +\n    geom_line(aes(x = index, y = rmse2, color = '1 Step Ahead') ) + \n    geom_line(aes(x = index, y = rmse362, color = '10 Steps Ahead')) + \n    labs(title = \"1 vs. 10 Steps Ahead Forecasts\", color = \"Forecast\", \n    x = \"Days\", y = \"Predicted Number of Weather Events\" ) +  theme(text = element_text(size=14))\n\n\n\n\n\nThe chart above shows the RMSE for the cross validated forecasts with windows 1 and 10. I had to use 10 for my seasonal window as my computer was unable to handle the 365 window, and could not produce results. But my data also had short-term seasonality so I relied upon that here. The red line represents the 10-step ahead forecast average RMSE and the blue line represents the 1-step ahead forecast RMSE. Overall you can see that both forecasts perform better and worse around the same time, except that the short-term forecast has a lagged reaction to the same periods where the long-term forecast performed poorly. In some cases, however, the 1-step RMSE does exceed the 10-step RMSE, suggesting poor short term performance."
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html",
    "href": "ARIMAX SARIMAX VAR.html",
    "title": "ARIMAX SARIMAX VAR",
    "section": "",
    "text": "We have the following independent variables:\n\nInterest Rate Expectation Changes - 3 Months\nInterest Rate Expectation Changes - 6 Months\nInterest Rate Expectation Changes - 1 Year\nExtreme Weather Events - Daily Event Number\nExtreme Weather Events - Daily Property Damage\nExtreme Weather Events - Daily Casualties\nExtreme Weather Events - Hurricanes\nExpected Volatility (VIX) - Value\nExpected Volatility (VIX) - Daily Change\nWork Stoppages - Daily Striking Worker Total\nWork Stoppages - Daily New Strike Beggining\nWork Stoppages - Daily New Workers Striking\n\n\n\nFirst we need to create all 12 predictors, then we can combine them to estimate the models as needed.\n\n\nCode\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(lubridate)\nlibrary(reticulate)\n\n\nLet’s quickly retrieve the daily stock price ranges for the indices:\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\nNow, let’s gather the VIX data, since it is also treated like a stock price, and should be available from the same package\n\n\nCode\nvixIn &lt;- quantmod::getSymbols(\"^VIX\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\nvixIn$dailyChange &lt;- vixIn$VIX.Close - lag(vixIn$VIX.Close)\n# 8 is vixIn$VIX.Close\n# 9 is vixIn$dailyChange\n\n#vixIn &lt;- vixIn %&gt;% \n#    mutate(date = ymd(index(vixIn)))\n#head(vixIn)\n\n\nWe have bond yields stored in a CSV, but let’s calculate daily changes:\n\n\nCode\nyieldCurve &lt;- read.csv('data/treasuries.csv')\n\nyieldCurve$mo3delta &lt;- yieldCurve$X3.Mo - lag(yieldCurve$X3.Mo)\nyieldCurve$mo6delta &lt;- yieldCurve$X6.Mo - lag(yieldCurve$X6.Mo)\nyieldCurve$mo12delta &lt;- yieldCurve$X1.Yr - lag(yieldCurve$X1.Yr) \n\nyieldCurve$Date &lt;- mdy(yieldCurve$Date)\n\n\nWorth noting: Both the treasury data and the VIX data only start on January 4th.\nNext up, let’s prepare the weather event data:\n\n\nCode\nweather_data &lt;- read.csv('data/storms_clean.csv')\n\n\nweather_data$month &lt;- weather_data$BEGIN_YEARMONTH %% 100\n\nweather_data &lt;- weather_data %&gt;%\n    mutate(realdate = make_date(YEAR, month, BEGIN_DAY)) %&gt;%\n    mutate(DAMAGE_PROPERTY =  str_replace(DAMAGE_PROPERTY, \"K\", \"\") )  %&gt;%\n    mutate(DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)) %&gt;%\n    mutate(DAMAGE_PROPERTY = replace_na(DAMAGE_PROPERTY, 0))\n\n\n# Daily Event Number\ndaily_events &lt;- weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(events = length(EPISODE_ID))\n\n# Daily Property Damage\ndaily_property &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(pdam = sum(DAMAGE_PROPERTY))\n\n# Daily Daily Casualties\ndaily_casualties &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(casualties = sum(INJURIES_DIRECT) + sum(INJURIES_INDIRECT) + sum(DEATHS_DIRECT) + sum(DEATHS_INDIRECT))\n\n# Daily Hurricanes\ndaily_hurricanes &lt;- weather_data %&gt;%\n    filter(EVENT_TYPE == \"Hurricane\") %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(hurricaneWarnings = length(EPISODE_ID))\n\n\n# Daily joined data\n\nweather_merged &lt;- full_join(daily_events, daily_property, by = \"realdate\")\nweather_merged &lt;- full_join(weather_merged, daily_casualties, by = 'realdate')\nweather_merged &lt;- full_join(weather_merged, daily_hurricanes, by = 'realdate')\n\n#head(weather_merged)\n\n\nFinally, lets get the striking worker data ready:\n\n\nCode\nstrike_data &lt;- read.csv('data/strikes.csv')\n\n# clean date format\nstrike_data$start = mdy(strike_data$Work.stoppage.beginning.date)\nstrike_data$end = mdy(strike_data$Work.stoppage.ending.date)\nstrike_data$workers = as.numeric(str_replace(strike_data$Number.of.workers.2., \",\", \"\" ))\n\n\n#head(strike_data)\n\ntarget_dates &lt;- ymd(index(spyIn))\n\ndaily_workers &lt;- vector(mode = \"numeric\", length = length(target_dates))\n\nfor(i in seq_along(target_dates)) {\n    tempDat &lt;- strike_data %&gt;%\n        filter(start &lt;= target_dates[i]) %&gt;%\n        filter(end &gt;= target_dates[i])\n\n    daily_workers[i] = sum(tempDat$workers)\n}\n\nworkerDF &lt;- data.frame('workers' = daily_workers, 'date' = target_dates)\n#plot(daily_workers, type = 'l')\n\n\nNow, we can combined all of these datasets into one dataframe, joining on the date columns\n\n\nCode\n# Convert TS objects to df, and fix the date column\nvixDF &lt;- data.frame(vixIn)\nvixDF$date &lt;- ymd(index(vixIn))\nspyDF &lt;- data.frame(spyIn)\nspyDF$date &lt;- ymd(index(spyIn))\nqqqDF &lt;- data.frame(qqqIn)\nqqqDF$date &lt;- ymd(index(qqqIn))\niwmDF &lt;- data.frame(iwmIn)\niwmDF$date &lt;- ymd(index(iwmIn))\n\n# Join symbols together\ntickers &lt;- left_join(spyDF, vixDF, by = 'date')\ntickers &lt;- left_join(tickers, qqqDF, by = 'date')\ntickers &lt;- left_join(tickers, iwmDF, by = 'date')\n\n# Join weather data\ncombinedData &lt;- left_join(tickers, weather_merged, by = c(\"date\" = \"realdate\"))\n\n# Join Bond Yields\ncombinedData &lt;- left_join(combinedData, yieldCurve, by = c(\"date\" = \"Date\"))\n\n# Join Labor Data\ncombinedData &lt;- left_join(combinedData, workerDF, by = 'date')\n\n#head(combinedData)\n\n\n\n\n\nWe will combine these 12 predictors into 5 models, for SPY, QQQ, and IWM intraday volatility:\n\n\nThis model is selected based on the literature review, which suggested that weather events and investor expecations could affect stock prices. This is the “kitchen sink” model, where I am throwing in variables from all data sources. However, looking at the variables individually, such as daily property damage vs. SPY daily price range, we don’t nessecarily see clear correlation (see plot below which resembles white noise). But I am interested to see how these variables are related when taking many different contextual factors into account in the same model.\n\n\nCode\nggplot(combinedData, aes(x = log(spyRange), y = log(pdam)) ) + geom_point() + labs(title = \"3 Month Interest Rate Changes vs. VIX Change \", x = \"Log SPY Daily Spread\", y = \"Log Property Damage From Storms\")\n\n\n\n\n\n\n\n\nThis model is based on a belief that there is an interrelationship between VIX prices and bond yields. This is because both would increase and decrease based on investor expectations for macroeconomic performance in upcoming months. If investors feel the economy will perform poorly, then this might predict bond yields lowering, as well as increased volatility which would be reflected by increases in the VIX. We also see a weak linear correlation in these daily values, as pictured in the plot below.\n\n\nCode\nggplot(combinedData, aes(x = mo3delta, y = dailyChange ) ) + geom_point() + labs(title = \"3 Month Interest Rate Changes vs. VIX Change \", x = \"Change in 3-Month Interest Rates\", y = \"Change in VIX Price\")\n\n\n\n\n\n\n\n\nThis model is all about exogenous shocks. New strikes beggining and hurricane warnings are infrequent but extreme events, which have been grouped together with short-term interest rates (3 month window) to try and capture extreme-but-short-termm influences on volatility.\n\n\n\nThe intent in models 4 and 5 is to follow the same logic as model 1, but to test each of the other two central indices in question. Model 4 will have QQ as the target and model 5 will have IWM as the target.This will allow us to determine if the effects of these exogenous varibables differ based on the model in question. These exogenous variables were selected based off of the literature review, but now we will look at whether large-cap tech companies (QQQ) or small enterprises (IWM) are more likely to be affected by this kind of volatility."
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html#planning-models",
    "href": "ARIMAX SARIMAX VAR.html#planning-models",
    "title": "ARIMAX SARIMAX VAR",
    "section": "",
    "text": "We have the following independent variables:\n\nInterest Rate Expectation Changes - 3 Months\nInterest Rate Expectation Changes - 6 Months\nInterest Rate Expectation Changes - 1 Year\nExtreme Weather Events - Daily Event Number\nExtreme Weather Events - Daily Property Damage\nExtreme Weather Events - Daily Casualties\nExtreme Weather Events - Hurricanes\nExpected Volatility (VIX) - Value\nExpected Volatility (VIX) - Daily Change\nWork Stoppages - Daily Striking Worker Total\nWork Stoppages - Daily New Strike Beggining\nWork Stoppages - Daily New Workers Striking\n\n\n\nFirst we need to create all 12 predictors, then we can combine them to estimate the models as needed.\n\n\nCode\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(lubridate)\nlibrary(reticulate)\n\n\nLet’s quickly retrieve the daily stock price ranges for the indices:\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\nNow, let’s gather the VIX data, since it is also treated like a stock price, and should be available from the same package\n\n\nCode\nvixIn &lt;- quantmod::getSymbols(\"^VIX\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\nvixIn$dailyChange &lt;- vixIn$VIX.Close - lag(vixIn$VIX.Close)\n# 8 is vixIn$VIX.Close\n# 9 is vixIn$dailyChange\n\n#vixIn &lt;- vixIn %&gt;% \n#    mutate(date = ymd(index(vixIn)))\n#head(vixIn)\n\n\nWe have bond yields stored in a CSV, but let’s calculate daily changes:\n\n\nCode\nyieldCurve &lt;- read.csv('data/treasuries.csv')\n\nyieldCurve$mo3delta &lt;- yieldCurve$X3.Mo - lag(yieldCurve$X3.Mo)\nyieldCurve$mo6delta &lt;- yieldCurve$X6.Mo - lag(yieldCurve$X6.Mo)\nyieldCurve$mo12delta &lt;- yieldCurve$X1.Yr - lag(yieldCurve$X1.Yr) \n\nyieldCurve$Date &lt;- mdy(yieldCurve$Date)\n\n\nWorth noting: Both the treasury data and the VIX data only start on January 4th.\nNext up, let’s prepare the weather event data:\n\n\nCode\nweather_data &lt;- read.csv('data/storms_clean.csv')\n\n\nweather_data$month &lt;- weather_data$BEGIN_YEARMONTH %% 100\n\nweather_data &lt;- weather_data %&gt;%\n    mutate(realdate = make_date(YEAR, month, BEGIN_DAY)) %&gt;%\n    mutate(DAMAGE_PROPERTY =  str_replace(DAMAGE_PROPERTY, \"K\", \"\") )  %&gt;%\n    mutate(DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)) %&gt;%\n    mutate(DAMAGE_PROPERTY = replace_na(DAMAGE_PROPERTY, 0))\n\n\n# Daily Event Number\ndaily_events &lt;- weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(events = length(EPISODE_ID))\n\n# Daily Property Damage\ndaily_property &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(pdam = sum(DAMAGE_PROPERTY))\n\n# Daily Daily Casualties\ndaily_casualties &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(casualties = sum(INJURIES_DIRECT) + sum(INJURIES_INDIRECT) + sum(DEATHS_DIRECT) + sum(DEATHS_INDIRECT))\n\n# Daily Hurricanes\ndaily_hurricanes &lt;- weather_data %&gt;%\n    filter(EVENT_TYPE == \"Hurricane\") %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(hurricaneWarnings = length(EPISODE_ID))\n\n\n# Daily joined data\n\nweather_merged &lt;- full_join(daily_events, daily_property, by = \"realdate\")\nweather_merged &lt;- full_join(weather_merged, daily_casualties, by = 'realdate')\nweather_merged &lt;- full_join(weather_merged, daily_hurricanes, by = 'realdate')\n\n#head(weather_merged)\n\n\nFinally, lets get the striking worker data ready:\n\n\nCode\nstrike_data &lt;- read.csv('data/strikes.csv')\n\n# clean date format\nstrike_data$start = mdy(strike_data$Work.stoppage.beginning.date)\nstrike_data$end = mdy(strike_data$Work.stoppage.ending.date)\nstrike_data$workers = as.numeric(str_replace(strike_data$Number.of.workers.2., \",\", \"\" ))\n\n\n#head(strike_data)\n\ntarget_dates &lt;- ymd(index(spyIn))\n\ndaily_workers &lt;- vector(mode = \"numeric\", length = length(target_dates))\n\nfor(i in seq_along(target_dates)) {\n    tempDat &lt;- strike_data %&gt;%\n        filter(start &lt;= target_dates[i]) %&gt;%\n        filter(end &gt;= target_dates[i])\n\n    daily_workers[i] = sum(tempDat$workers)\n}\n\nworkerDF &lt;- data.frame('workers' = daily_workers, 'date' = target_dates)\n#plot(daily_workers, type = 'l')\n\n\nNow, we can combined all of these datasets into one dataframe, joining on the date columns\n\n\nCode\n# Convert TS objects to df, and fix the date column\nvixDF &lt;- data.frame(vixIn)\nvixDF$date &lt;- ymd(index(vixIn))\nspyDF &lt;- data.frame(spyIn)\nspyDF$date &lt;- ymd(index(spyIn))\nqqqDF &lt;- data.frame(qqqIn)\nqqqDF$date &lt;- ymd(index(qqqIn))\niwmDF &lt;- data.frame(iwmIn)\niwmDF$date &lt;- ymd(index(iwmIn))\n\n# Join symbols together\ntickers &lt;- left_join(spyDF, vixDF, by = 'date')\ntickers &lt;- left_join(tickers, qqqDF, by = 'date')\ntickers &lt;- left_join(tickers, iwmDF, by = 'date')\n\n# Join weather data\ncombinedData &lt;- left_join(tickers, weather_merged, by = c(\"date\" = \"realdate\"))\n\n# Join Bond Yields\ncombinedData &lt;- left_join(combinedData, yieldCurve, by = c(\"date\" = \"Date\"))\n\n# Join Labor Data\ncombinedData &lt;- left_join(combinedData, workerDF, by = 'date')\n\n#head(combinedData)\n\n\n\n\n\nWe will combine these 12 predictors into 5 models, for SPY, QQQ, and IWM intraday volatility:\n\n\nThis model is selected based on the literature review, which suggested that weather events and investor expecations could affect stock prices. This is the “kitchen sink” model, where I am throwing in variables from all data sources. However, looking at the variables individually, such as daily property damage vs. SPY daily price range, we don’t nessecarily see clear correlation (see plot below which resembles white noise). But I am interested to see how these variables are related when taking many different contextual factors into account in the same model.\n\n\nCode\nggplot(combinedData, aes(x = log(spyRange), y = log(pdam)) ) + geom_point() + labs(title = \"3 Month Interest Rate Changes vs. VIX Change \", x = \"Log SPY Daily Spread\", y = \"Log Property Damage From Storms\")\n\n\n\n\n\n\n\n\nThis model is based on a belief that there is an interrelationship between VIX prices and bond yields. This is because both would increase and decrease based on investor expectations for macroeconomic performance in upcoming months. If investors feel the economy will perform poorly, then this might predict bond yields lowering, as well as increased volatility which would be reflected by increases in the VIX. We also see a weak linear correlation in these daily values, as pictured in the plot below.\n\n\nCode\nggplot(combinedData, aes(x = mo3delta, y = dailyChange ) ) + geom_point() + labs(title = \"3 Month Interest Rate Changes vs. VIX Change \", x = \"Change in 3-Month Interest Rates\", y = \"Change in VIX Price\")\n\n\n\n\n\n\n\n\nThis model is all about exogenous shocks. New strikes beggining and hurricane warnings are infrequent but extreme events, which have been grouped together with short-term interest rates (3 month window) to try and capture extreme-but-short-termm influences on volatility.\n\n\n\nThe intent in models 4 and 5 is to follow the same logic as model 1, but to test each of the other two central indices in question. Model 4 will have QQ as the target and model 5 will have IWM as the target.This will allow us to determine if the effects of these exogenous varibables differ based on the model in question. These exogenous variables were selected based off of the literature review, but now we will look at whether large-cap tech companies (QQQ) or small enterprises (IWM) are more likely to be affected by this kind of volatility."
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html#model-selection",
    "href": "ARIMAX SARIMAX VAR.html#model-selection",
    "title": "ARIMAX SARIMAX VAR",
    "section": "Model Selection",
    "text": "Model Selection\nIn this section, I will begin by identifying the candidate model structures for each of the 5 overarching models outlined above. I will identify candidate models through auto.arima for ARIMAX models, plus hand-selected values. For VAR models, I will identify 2 candidates for each overall model with the autoVAR function.\n\nModel Selection for ARIMAX Models\n\nModel 1: SPY ~ Interest Rate 1-Year + Daily Weather Property Damage + Daily VIX Change + Daily Striking Worker Total\n\n\nCode\nxMatrix = combinedData[,c('mo12delta', 'pdam', 'dailyChange', 'workers')]\nxMatrix[is.na(xMatrix)] &lt;- 0\nxMatrix = scale(xMatrix)\nxMatrix = as.matrix(xMatrix)\n\n#xMatrix\n\nmod1candidate1 = auto.arima(scale(combinedData$spyRange), xreg = xMatrix, trace = FALSE)\ncheckresiduals(mod1candidate1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(4,1,1) errors\nQ* = 3.619, df = 5, p-value = 0.6055\n\nModel df: 5.   Total lags used: 10\n\n\nAuto ARIMA picks out the model (4,1,1) for the standardized data. The residual diagnostic plots look good, with the residuals normally distributed.\n\n\nCode\nxMatrix = as.data.frame(xMatrix)\n\n# Lets examine the residuals directly to identify \nmod1candidate2 &lt;- lm(scale(combinedData$spyRange) ~ mo12delta + pdam + dailyChange + workers, data = xMatrix )\n#summary(mod1candidate2)\n\nresid1 &lt;- mod1candidate2$residuals\npacf(resid1,main = \"PACF of Model Residuals\")\n\n\n\n\n\nCode\nacf(resid1,main = \"ACF of Model Residuals\")\n\n\n\n\n\nBased on the PACF and ACF of the residuals from the regression, it seems we should definitely difference the series, as we have many significant lag terms in the ACF. On the PACF, we can see 4 terms clearly signfificant. Based on these charts, I might try the model (4,1,0). I will try up through (4,2,2) and look for the lowest aic.\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \ntemp=c()\nd=1\nD=1\ns=12\n \ni=1\ntemp= data.frame()\nls=matrix(rep(NA,9*378),nrow=378)\n \n \nfor (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\n#SARIMA.c(p1=1,p2=5,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=resid1) %&gt;% filter(!is.na(p))\n\nmod1candidate2 &lt;- arima(resid1, order = c(1,0,1), seasonal = list(order = c(0,1,0)))\ncheckresiduals(mod1candidate2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1)\nQ* = 13.208, df = 8, p-value = 0.1049\n\nModel df: 2.   Total lags used: 10\n\n\nThe function to evaluate various p,d,q values returns SARIMA(1,0,1)(0,1,0)[12] with the lowest AIC and BIC. The residuals of this second model show clear correlation around lags 2 and 4, which was not present in the 4,1,0 model that auto arima suggested. So overall, I would say the diagnostics look worse for the second model than the first.\n\n\nModel 3: (ARIMAX) IWM ~ Casualties + Hurricanes + Interest Rate 3-Months\n\n\nCode\nxMatrix3 = combinedData[,c('mo3delta', 'casualties', 'hurricaneWarnings')]\nxMatrix3[is.na(xMatrix3)] &lt;- 0\nxMatrix3 = scale(xMatrix3)\nxMatrix3 = as.matrix(xMatrix3)\n\n#xMatrix\n\nmod3candidate1 = auto.arima(scale(combinedData$iwmRange), xreg = xMatrix3, trace = FALSE)\ncheckresiduals(mod3candidate1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,1,3) errors\nQ* = 5.873, df = 6, p-value = 0.4376\n\nModel df: 4.   Total lags used: 10\n\n\nAuto.arima identifies (1,1,3) as the best model. The residuals show a low level of correlation in the lags, which is encouraging, and overall the residuals are mostly normally distributed although they are somewhat skewed to the right. Now, let’s see what we manually select, also considering a SARIMAX model.\nPrepare residuals:\n\n\nCode\nxMatrix3 = as.data.frame(xMatrix3)\n\n# Lets examine the residuals directly to identify \nmod3candidate2 &lt;- lm(scale(combinedData$iwmRange) ~ casualties + mo3delta + hurricaneWarnings, data = xMatrix3 )\n#summary(mod3candidate2)\n\nresid3 &lt;- mod3candidate2$residuals\npacf(resid3, main = \"PACF of Model Residuals\")\n\n\n\n\n\nCode\nacf(resid3, main = \"ACF of Model Residuals\")\n\n\n\n\n\nThe ACF and PACF plots of the residuals from linear regression are mixed, but there is clear correlation through value 5 in the PACF plot. The ACF plot has many significant terms, suggesting the series should be differenced. Now, I’ll loop through all the options to see if there is a suitable SARIMA model for the residuals:\n\n\nCode\n#output=SARIMA.c(p1=1,p2=3,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=resid3)\n#output %&gt;% filter(!is.na(p))\n\n\nThe best model identified by a small margin is SARIMA(1,0,1)(1,1,0). Let’s check the diagnostics\n\n\nCode\nresidualsMod3Can2 &lt;- arima(resid3, order = c(1,0,1), seasonal = list(order = c(1,1,0)))\ncheckresiduals(residualsMod3Can2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1)\nQ* = 17.291, df = 8, p-value = 0.02722\n\nModel df: 2.   Total lags used: 10\n\n\nThe residuals for this model arent encouraging, as the Ljung Box test returns a p value of 0.03. THe residuals also do not look perfectly normally distributed.\n\n\nModel 4: (ARIMAX) QQQ ~ All Interest Rates + All Extreme Weather Events + Daily VIX Change + Daily Striking Workers\nRunning an auto arima:\n\n\nCode\nxMatrix4 = combinedData[,c('mo3delta', 'mo6delta', 'mo12delta', 'events', 'dailyChange', 'workers')]\nxMatrix4[is.na(xMatrix4)] &lt;- 0\nxMatrix4 = scale(xMatrix4)\nxMatrix4 = as.matrix(xMatrix4)\n\n#xMatrix\n\nmod4candidate1 = auto.arima(scale(combinedData$qqqRange), xreg = xMatrix4, trace = FALSE)\ncheckresiduals(mod4candidate1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,1,1) errors\nQ* = 2.722, df = 8, p-value = 0.9506\n\nModel df: 2.   Total lags used: 10\n\n\nAuto Arima returns (1,1,1). The diagnostics look acceptable, although there is clustered volatility in the residual plot. The Ljung-Box test returns p = 0.95, suggesting there is not autocorrelation in the residuals. However, the residual lag plot has high correlation around lag 20, and the correlation of the residuals slightly increases as the lags get greater.\nNow let’s select a candidate manually, including from SARIMA models. First we calculate and review the residuals from the linear regression:\n\n\nCode\nxMatrix4 = as.data.frame(xMatrix4)\n\n# Lets examine the residuals directly to identify \nmod4candidate2 &lt;- lm(scale(combinedData$qqqRange) ~ mo3delta + mo6delta + mo12delta + events + dailyChange, data = xMatrix4 )\n#summary(mod1candidate2)\n\nresid4 &lt;- mod4candidate2$residuals\npacf(resid4, main = \"PACF of Model Residuals\")\n\n\n\n\n\nCode\nacf(resid4, main = \"ACF of Model Residuals\")\n\n\n\n\n\nThe ACF plot has many significant lags (&gt;10) which suggests we may need to difference the residuals. The PACF plot has high significance through lag 5. Let’s run a function to check all of the values up through p=2 and q=5.\n\n\nCode\n#mod4candidate2fit =SARIMA.c(p1=1,p2=2,q1=1,q2=5,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=resid4)\n#mod4candidate2fit %&gt;% filter(!is.na(p))\n\n\nThe best AIC and BIC scores returned by the function are for the model SARIMA(1,0,1)(1,1,0). Let’s look at the diagnostic plots to see how well this model captures the data:\n\n\nCode\nmod4candidate2fit = arima(resid4, order = c(1,0,1), seasonal = list(order = c(1,1,0)))\ncheckresiduals(mod4candidate2fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1)\nQ* = 4.6414, df = 8, p-value = 0.7951\n\nModel df: 2.   Total lags used: 10\n\n\nThe residuals for this model look similar to the auto.arima model, so it will be interesting to compare them with cross validation. Otherwise, it is notable that the residuals display clustered volatility, while the lag plot shows significant correlations at some values, although the Ljung-Box test returns 0.795 so we can conclude there is no autocorrelation in the residuals.\n\n\nModel 5: (ARIMAX) IWM ~ All Interest Rates + All Extreme Weather Events + Daily VIX Change + Daily Striking Workers\nRunning an auto arima:\n\n\nCode\nxMatrix5 = combinedData[,c('mo3delta', 'mo6delta', 'mo12delta', 'events', 'dailyChange', 'workers')]\nxMatrix5[is.na(xMatrix4)] &lt;- 0\nxMatrix5 = scale(xMatrix4)\nxMatrix5 = as.matrix(xMatrix4)\n\n#xMatrix\n\nmod5candidate1 = auto.arima(scale(combinedData$iwmRange), xreg = xMatrix5, trace = FALSE)\ncheckresiduals(mod5candidate1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,1,3) errors\nQ* = 5.3537, df = 6, p-value = 0.4993\n\nModel df: 4.   Total lags used: 10\n\n\nAuto Arima returns (1,1,3). The diagnostics look acceptable, although there is clustered volatility in the residual plot. The Ljung-Box test returns p = 0.499, suggesting there is not autocorrelation in the residuals. However, the residual lag plot has high correlation around lag 14 and 26.\nNow let’s select a candidate manually, including from SARIMA models. First we calculate and review the residuals from the linear regression:\n\n\nCode\nxMatrix5 = as.data.frame(xMatrix5)\n\n# Lets examine the residuals directly to identify \nmod5candidate2 &lt;- lm(scale(combinedData$iwmRange) ~ mo3delta + mo6delta + mo12delta + events + dailyChange, data = xMatrix5 )\n#summary(mod5candidate2)\n\nresid5 &lt;- mod5candidate2$residuals\npacf(resid5,main = \"PACF of Model Residuals\")\n\n\n\n\n\nCode\nacf(resid5,main = \"ACF of Model Residuals\")\n\n\n\n\n\nThe ACF plot has many significant lags (&gt;10) which suggests we may need to difference the residuals. The PACF plot has high significance through lag 5. Let’s run a function to check all of the values up through p=2 and q=5.\n\n\nCode\n#mod5candidate2fit =SARIMA.c(p1=1,p2=2,q1=1,q2=5,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=resid5)\n#mod5candidate2fit %&gt;% filter(!is.na(p))\n\n\nThe best AIC and BIC scores returned by the function are for the model SARIMA(1,0,1)(1,1,0). The same as for QQQ with the same exogenous predictors. Let’s look at the diagnostic plots to see how well this model captures the data:\n\n\nCode\nmod5candidate2fit = arima(resid5, order = c(1,0,1), seasonal = list(order = c(1,1,0)))\ncheckresiduals(mod5candidate2fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1)\nQ* = 18.89, df = 8, p-value = 0.01546\n\nModel df: 2.   Total lags used: 10\n\n\nThe residuals for this model look somewhat unideal, with significant lags at 2, 6, 12, 14, and 26. Otherwise, it is notable that the residuals display clustered volatility, while the lag plot shows significant correlations at some values. The Ljung Box test returns 0.015 suggesting the residuals are autocorrelated, which is an issue.\n\n\n\nModel Selection for VAR Models\n\nModel 2: (VAR) SPY ~ Interest Rate 3-Months + Daily VIX Value\nStep 1, let’s fit VAR with p=1 just to see the relationship between our 3 variables (SPY intraday range, 3-month interest rate changes, and the real daily VIX values).\n\n\nCode\nxMatrix2 = combinedData[,c('VIX.Adjusted', 'mo3delta', 'spyRange') ]\nxMatrix2[is.na(xMatrix2)] &lt;- 0\nxMatrix2 = scale(xMatrix2)\nxMatrix2 = as.matrix(xMatrix2)\n\n#xMatrix\n\nsummary(vars::VAR(xMatrix2, p = 1, type = 'both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: VIX.Adjusted, mo3delta, spyRange \nDeterministic variables: both \nSample size: 689 \nLog Likelihood: -1891.17 \nRoots of the characteristic polynomial:\n0.9396 0.05755 0.05755\nCall:\nvars::VAR(y = xMatrix2, p = 1, type = \"both\")\n\n\nEstimation results for equation VIX.Adjusted: \n============================================= \nVIX.Adjusted = VIX.Adjusted.l1 + mo3delta.l1 + spyRange.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nVIX.Adjusted.l1  9.330e-01  1.915e-02  48.710   &lt;2e-16 ***\nmo3delta.l1     -2.685e-02  1.301e-02  -2.064   0.0394 *  \nspyRange.l1      4.189e-03  1.888e-02   0.222   0.8245    \nconst            1.716e-02  2.680e-02   0.641   0.5221    \ntrend           -5.707e-05  6.794e-05  -0.840   0.4013    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3372 on 684 degrees of freedom\nMultiple R-Squared: 0.8869, Adjusted R-squared: 0.8862 \nF-statistic:  1341 on 4 and 684 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation mo3delta: \n========================================= \nmo3delta = VIX.Adjusted.l1 + mo3delta.l1 + spyRange.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)   \nVIX.Adjusted.l1 -0.0429432  0.0558806  -0.768  0.44247   \nmo3delta.l1     -0.1137505  0.0379419  -2.998  0.00282 **\nspyRange.l1     -0.1245585  0.0550961  -2.261  0.02409 * \nconst            0.1330199  0.0781812   1.701  0.08932 . \ntrend           -0.0003844  0.0001982  -1.939  0.05287 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9838 on 684 degrees of freedom\nMultiple R-Squared: 0.03914,    Adjusted R-squared: 0.03352 \nF-statistic: 6.965 on 4 and 684 DF,  p-value: 1.69e-05 \n\n\nEstimation results for equation spyRange: \n========================================= \nspyRange = VIX.Adjusted.l1 + mo3delta.l1 + spyRange.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nVIX.Adjusted.l1  0.6616598  0.0403536  16.397  &lt; 2e-16 ***\nmo3delta.l1      0.0662097  0.0273994   2.416   0.0159 *  \nspyRange.l1      0.0661674  0.0397871   1.663   0.0968 .  \nconst           -0.3143373  0.0564577  -5.568 3.71e-08 ***\ntrend            0.0008977  0.0001431   6.271 6.34e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7104 on 684 degrees of freedom\nMultiple R-Squared: 0.496,  Adjusted R-squared: 0.4931 \nF-statistic: 168.3 on 4 and 684 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             VIX.Adjusted  mo3delta spyRange\nVIX.Adjusted      0.11371 -0.019172 0.076451\nmo3delta         -0.01917  0.967841 0.006324\nspyRange          0.07645  0.006324 0.504715\n\nCorrelation matrix of residuals:\n             VIX.Adjusted  mo3delta spyRange\nVIX.Adjusted      1.00000 -0.057792 0.319129\nmo3delta         -0.05779  1.000000 0.009049\nspyRange          0.31913  0.009049 1.000000\n\n\nThe initial VAR fit is encouraging, as the 3 variables are all significant. SPY range has a p value of 0.09, which is slightly above the 0.05 threshold that would be ideal, but still suggests it helps explain the variance in the other variables in the model. The overall R squared and adjusted R squared are also encouraging, at 0.5, which is exceptionally high for a model concerning stock prices.\nNow lets use VAR select to identify some preferrable p values.\n\n\nCode\nvars::VARselect(xMatrix2, lag.max = 10, type = 'both')\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      1      1      5 \n\n$criteria\n                 1           2           3          4           5           6\nAIC(n) -2.95967636 -2.96412325 -2.96215454 -2.9562924 -3.00144103 -2.99099019\nHQ(n)  -2.92106481 -2.90234476 -2.87720911 -2.8481801 -2.87016173 -2.83654396\nSC(n)  -2.85992432 -2.80451997 -2.74270003 -2.6769867 -2.66228407 -2.59198201\nFPE(n)  0.05183573  0.05160586  0.05170783  0.0520123  0.04971693  0.05024022\n                 7           8           9          10\nAIC(n) -2.98502423 -2.96721214 -2.95685167 -2.95666268\nHQ(n)  -2.80741107 -2.76643204 -2.73290464 -2.70954871\nSC(n)  -2.52616482 -2.44850150 -2.37828980 -2.31824958\nFPE(n)  0.05054219  0.05145229  0.05199039  0.05200299\n\n\nVAR select returns either p =5 or p = 1 as the best fits, with AIC and FPE selecting p=5, and HQ and SC selecting p =1. We will use cross validation to compare these options."
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html#model-evaluation",
    "href": "ARIMAX SARIMAX VAR.html#model-evaluation",
    "title": "ARIMAX SARIMAX VAR",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nIn this section, I will use cross validation to select the best candidate model for each of the 5 overaching model designs. First, lets define a cross validation function\n\n\nCode\n#######\n\ncrossVal &lt;- function(arima1order, arima2order, sarima2order, data) {\n\n    # window is always 1\n    test &lt;- 30\n    trainnum &lt;- length(data) - test\n    rmse1 &lt;- vector(mode = 'numeric', length = 30)\n    rmse2 &lt;- vector(mode = 'numeric', length = 30)\n\n    for(i in 1:30) {\n        #print(trainnum + ((i-1) * 4))\n        #print(trainnum + (i*4))\n        #print(trainnum + ((i-1) * 4) +1)\n        \n        xtrain &lt;- data[c(1:(trainnum + i - 1))]\n        xtest &lt;-  data[c((trainnum+1):(trainnum+i+1))]\n        \n        \n        \n        ######## first Model ############\n        fit &lt;- arima(xtrain, order = arima1order)\n        fcast &lt;- predict(fit, n.ahead = 1)$pred[1]\n        \n        \n        ######## second model ###########\n        fit2 &lt;- arima(xtrain, order = arima2order, seasonal = sarima2order)\n        fcast2 &lt;- predict(fit2, n.ahead = 1)$pred[1]\n        \n        # Errors\n\n        rmse1[i]  &lt;-sqrt((fcast-xtest[1])^2)\n        rmse2[i]  &lt;-sqrt((fcast2-xtest[1])^2)\n        \n    }\n    \n    outputs = data.frame(\"rmse1\" = rmse1, 'rmse2' = rmse2)\n    return(outputs)\n\n}\n\n\n\nModel 1 (ARIMAX)\nLets test the function out on the first model, comparing ARIMA(4,1,1) vs. SARIMA(1,0,1)(0,1,0) on the residuals of the linear regression.\n\n\nCode\nmodel1comparison = crossVal(c(4,1,1), c(1,0,1), list(order = c(0,1,0)), resid1)\n\nmean(model1comparison$rmse1)\n\n\n[1] 0.2690243\n\n\nCode\nmean(model1comparison$rmse2)\n\n\n[1] 0.2696219\n\n\nBetween the two models, model 1, selected by auto.arima, beats out the sarima model with a mean RMSE of 0.2690 vs. 0.2696. However, the models do perform similarly to eachother. Let’s look at a graph to make the difference more clear:\n\n\nCode\nindex1 = c(1:nrow(model1comparison))\nggplot(data = model1comparison, aes(x = index1, y = rmse1), color = 'blue') + geom_line() + geom_line(aes(x = index1, y = rmse2), color = 'red') + labs(title = 'Comparing RMSE of ARIMAX(4,1,1) Black and SARIMAX(1,0,1)(0,1,0) Red')\n\n\n\n\n\nAs we can see in the cross-validation chart, the two models perform similarly, and tend to have higher and lower errors at the same time, with both of the models performing poorly near the middle of the cross validation data sets. But overall, the ARIMA (ARIMAX) model performs the best at predicting the residuals. As such, my chosen model for Model 1 is ARIMAX(4,1,1).\n\nPredictions\nNow, let’s make predictions:\n\n\nCode\n#mod1candidate2 &lt;- lm(scale(combinedData$spyRange) ~ mo12delta + pdam + dailyChange + workers, data = xMatrix )\n\n\nmod1pdam &lt;- forecast(auto.arima(combinedData$pdam), 10)\nmod1delta12 &lt;- forecast(auto.arima(combinedData$mo12delta), 10)\nmod1dailyChange &lt;- forecast(auto.arima(combinedData$dailyChange), 10)\nmod1workers &lt;- forecast(auto.arima(combinedData$workers), 10)\n\npredictors1 &lt;- data.frame(cbind(pdam = mod1pdam$mean, mo12delta = mod1delta12$mean, dailyChange = mod1dailyChange$mean, workers = mod1workers$mean))\n\nfit = arima(combinedData$spyRange, order = c(4,1,1), xreg = xMatrix)\n\n#summary(fit)\n#forecast(fit)\nmod1 = predict(fit, newxreg = predictors1)\n\nautoplot(mod1$pred, main = \"ARIMA(4,1,1) Predictions, SPY Daily Range\")\n\n\n\n\n\nHere we can see the models predictions for the SPY daily upcoming range, using auto.arima generated models to predict the external regressors. Overall, the model predicts a precipitous drop in the daily range in SPY prices in the upcoming 10 days.\n\n\n\nModel 2 VAR\nLet’s forecast our VAR model, which used 3-month interest rate changes and daily VIX values to predict SPY’s intraday range. For this model we wanted to compare the p values of 1 and 5 to find the best model.\nLets, run our CV function:\n\n\nCode\ndata = xMatrix2\n\n    # window is always 1\n    test &lt;- 30\n    trainnum &lt;- nrow(data) - test\n    rmse1 &lt;- matrix(NA, 30,3)\n    rmse1 &lt;- data.frame(rmse1)\n    rmse2 &lt;- matrix(NA, 30,3)\n    rmse2 &lt;- data.frame(rmse2)\n\n    for(i in 1:29) {\n\n        xtrain &lt;- data[c(1:(trainnum + i - 1)),]\n        xtest &lt;-  data[c((trainnum+i):(trainnum+i+1)),]\n        \n        \n        \n        ######## first Model ############\n        fit &lt;- vars::VAR(xtrain, p = 1, type = 'both')\n        \n        fcast &lt;- predict(fit, n.ahead = 1)$fcst\n        \n        ff&lt;-data.frame(fcast$VIX.Adjusted[,1],fcast$mo3delta[,1],fcast$spyRange[,1])\n        \n        ######## second model ###########\n        fit2 &lt;- vars::VAR(xtrain,p =5, type = 'both')\n        fcast2 &lt;- predict(fit2, n.ahead = 1)$fcst\n        \n        ff2&lt;-data.frame(fcast2$VIX.Adjusted[,1],fcast2$mo3delta[,1],fcast2$spyRange[,1])\n\n        # Errors\n\n        rmse1[i,]  = sqrt((ff-xtest)^2)\n        rmse2[i,]  = sqrt((ff2-xtest)^2)\n        \n    }\n    \n    \n#print(rmse1)\n\n\nnames(rmse1) =c(\"VIXPrice\", \"3mo\",\"SPYDailyRange\")\nnames(rmse2) =c(\"VIXPrice\", \"3mo\",\"SPYDailyRange\")\n\ncolMeans(rmse1, na.rm = TRUE)\n\n\n     VIXPrice           3mo SPYDailyRange \n    0.1477893     1.1090823     0.6958689 \n\n\nCode\ncolMeans(rmse2, na.rm = TRUE)\n\n\n     VIXPrice           3mo SPYDailyRange \n    0.1387773     1.1899464     0.6822915 \n\n\nAfter cross validating the 2 VAR models across 30 1-day intervals, we obtain the following average RMSE for the different variables: VIX Price: p=1 -&gt; 0.148, p=5 -&gt; 0.139. 3 Month Interest Rate Changes: p=1 -&gt; 1.109, p=5 -&gt; 1.190. SPY Daily Range: p=1 -&gt; 0.696, p=5 -&gt; 0.682.\nOverall, the p=1 VAR model was better at predicting the interest rate changes variable, and the p=5 VAR model performed better on the VIX Price and SPY daily range variables.\nLet’s plot their performance:\n\n\nCode\nindex2 = c(1:nrow(rmse1))\n\nggplot() + \n  geom_line(data = rmse1, aes(x = index2, y = VIXPrice),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = index2, y = VIXPrice),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Vix Prices, Blue = (p=1), Red = (p=5)\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\"))) +  theme(text = element_text(size=14))\n\n\n\n\n\nCode\nggplot() + \n  geom_line(data = rmse1, aes(x = index2, y = SPYDailyRange),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = index2, y = SPYDailyRange),color = \"red\") +\n  labs(\n    title = \"CV RMSE for SPY Daily Range, Blue = (p=1), Red = (p=5)\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\"))) +  theme(text = element_text(size=14))\n\n\n\n\n\nOverall, the charts show what we confirmed with the average values: That the mean performance of the p=5 model was better on average.\nNow let’s predict:\n\n\nCode\nfinalmod2 = vars::VAR(xMatrix2, p = 5, type = 'both')\n        \nmod2forecast &lt;- predict(finalmod2, n.ahead = 10)$fcst\n\nindexmod2 = c(1:10)\nggplot() +\n    geom_line(aes(x = indexmod2, y = mod2forecast$spyRange[1:10],color = \"SPY Daily Range\")) +\n    geom_line(aes(x = indexmod2, y = mod2forecast$mo3delta[1:10],color = \"3 Month Change in Interest Rates\")) +\n    geom_line(aes(x = indexmod2, y = mod2forecast$VIX.Adjusted[1:10],color = \"VIX Price\")) + labs(y = \"Scaled Value\", x = \"Days Predicted into the Future\", color = \"Series Predicted\", title = \"VAR Forecast of VIX, SPY Range, and 3mo Interest Rates\") +  theme(text = element_text(size=14))\n\n\n\n\n\nHere we can see the scale forecasts for the 3 key variables, which are created with the predict function for the VAR model with p=5. Overall, the model predicts the SPY range to raise slightly for the next 10 days, and the 3 month interest rates to change sharply.\n\n\nModel 3 (ARIMAX) IWM ~ Casualties + Hurricanes + Interest Rate 3-Months\nLet’s compare the model returned by auto.arima, ARIMA(1,1,3), vs the model I found by hand, SARIMA(1,0,1)(1,1,0), on the residuals of the linear regression for model 3.\n\n\nCode\nmodel3comparison = crossVal(c(1,1,3), c(1,0,1), list(order = c(1,1,0)), resid3)\n\nmean(model3comparison$rmse1)\n\n\n[1] 0.5537179\n\n\nCode\nmean(model3comparison$rmse2)\n\n\n[1] 0.6006214\n\n\nOnce again, the simple ARIMA model beats out the SARIMA architecture in terms of average RMSE. The average for ARIMA(1,1,3) was 0.554, while for SARIMA(1,0,1)(1,1,0) it was 0.601. Let’s look at a plot of the rmse values to see how the models fared:\n\n\nCode\nindex3 = c(1:nrow(model3comparison))\nggplot(data = model3comparison, aes(x = index3, y = rmse1), color = 'blue') + geom_line() + geom_line(aes(x = index3, y = rmse2), color = 'red') + labs(title = 'Comparing RMSE of ARIMAX(1,1,3) Black and SARIMAX(1,0,1)(1,1,0) Red') +  theme(text = element_text(size=14))\n\n\n\n\n\nJust like for model 1, the two approaches performed similarly. If one model had a high RMSE for a particular value, the other model was likely to perform poorly as well. Overall however, we can see that for a given value the black line (ARIMA) tended to perform better. In the end, the ARIMAX models seems to fit the data well. As such, my chosen model for Model 3 is: ARIMAX(1,1,3)/.\n\nPredictions\nNow, let’s make predictions:\n\n\nCode\n#xMatrix3 = combinedData[,c('mo3delta', 'casualties', 'hurricaneWarnings')]\n\nmod3cas &lt;- forecast(auto.arima(combinedData$casualties), 10)\nmod3delta3 &lt;- forecast(auto.arima(combinedData$mo3delta), 10)\nmod3hurricane &lt;- forecast(auto.arima(combinedData$hurricaneWarnings), 10)\n\npredictors3 &lt;- data.frame(cbind(casualties = mod3cas$mean, mo3delta = mod3delta3$mean, hurricaneWarnings = mod3hurricane$mean))\n\nfit3 = arima(combinedData$iwmRange, order = c(1,1,3), xreg = xMatrix3)\n\n#summary(fit)\n#forecast(fit)\nmod3 = predict(fit3, newxreg = predictors3)\n\nautoplot(mod3$pred, main = \"ARIMA(1,1,3) Predictions for IWM Range\")\n\n\n\n\n\nHere we can see the models predictions for the IWM daily upcoming range, using auto.arima generated models to predict the external regressors. Overall, the model predicts a steep incline in IWM intraday ranges in the upcoming 10 days, especially in the first 5 before leveling off.\n\n\n\nModel 4 ARIMAX\nLet’s compare the model returned by auto.arima, ARIMA(1,1,1), vs the model I found by hand, SARIMA(1,0,1)(1,1,0), on the residuals of the linear regression for model 4.\n\n\nCode\nmodel4comparison = crossVal(c(1,1,1), c(1,0,1), list(order = c(1,1,0)), resid4)\n\nmean(model4comparison$rmse1)\n\n\n[1] 0.1693671\n\n\nCode\nmean(model4comparison$rmse2)\n\n\n[1] 0.1693451\n\n\nUnlike the previous model, the SARIMA architecture beat out the simple ARIMA model, with RMSEs of 0.16934 and 0.16937 respectively. Let’s plot both RMSE values to see how this discrepancy emerged:\n\n\nCode\nindex4 = c(1:nrow(model4comparison))\nggplot(data = model4comparison, aes(x = index4, y = rmse1), color = 'blue') + geom_line() + geom_line(aes(x = index4, y = rmse2), color = 'red') + labs(title = 'Comparing RMSE of ARIMAX(1,1,1) Black and SARIMAX(1,0,1)(1,1,0) Red') +  theme(text = element_text(size=14))\n\n\n\n\n\nVisible in the chart above, the two models performed almost identically on the cross validation data data. Overall however, we can see that for a given value the black line (ARIMA) and the red line (SARIMAX) tended to perform almost equivalently. However, due to its slight performance edge, my chosen model for Model 4 is: SARIMAX(1,0,1)(1,1,0).\n\nPredictions\nNow, let’s make predictions:\n\n\nCode\nxMatrix4 = combinedData[,c('events', 'mo12delta', 'mo6delta', 'mo3delta', 'dailyChange', 'workers')]\n\nmod4events &lt;- forecast(auto.arima(combinedData$events), 10)\nmod4delta12 &lt;- forecast(auto.arima(combinedData$mo12delta), 10)\nmod4delta6 &lt;- forecast(auto.arima(combinedData$mo6delta), 10)\nmod4delta3 &lt;- forecast(auto.arima(combinedData$mo3delta), 10)\nmod4dailyChange &lt;- forecast(auto.arima(combinedData$dailyChange), 10)\nmod4workers &lt;- forecast(auto.arima(combinedData$workers), 10)\n\n\npredictors4 &lt;- data.frame(cbind(events = mod4events$mean, mo12delta = mod4delta12$mean, mo6delta = mod4delta6$mean, mo3delta = mod4delta3$mean, dailyChange = mod4dailyChange$mean, workers = mod4workers$mean))\n\n\nfit4 = arima(combinedData$qqqRange, order = c(1,0,1), seasonal = list(order = c(1,1,0)), xreg = xMatrix4)\n\n#summary(fit)\n#forecast(fit)\nmod4 = predict(fit4, newxreg = predictors4)\n\nautoplot(mod4$pred, main = \"Predictions, Next 10 Timesteps, Model 4\")\n\n\n\n\n\nHere we can see the models predictions for the QQQ’s daily upcoming range, using auto.arima generated models to predict the external regressors. Overall, the model predicts a steep incline in QQQ intraday ranges in the upcoming several days, but then decreases again for the remained of the prediction window.\n\n\n\nModel 5 ARIMAX\nLet’s compare the model returned by auto.arima, ARIMA(1,1,3), vs the model I found by hand, SARIMA(1,0,1)(1,1,0), on the residuals of the linear regression for model 4.\n\n\nCode\nmodel5comparison = crossVal(c(1,1,3), c(1,0,1), list(order = c(1,1,0)), resid5)\n\nmean(model5comparison$rmse1)\n\n\n[1] 0.6360297\n\n\nCode\nmean(model5comparison$rmse2)\n\n\n[1] 0.6746319\n\n\nUnlike the previous model, the simple ARIMA model performs much better than the complicated SARIMA model, with an RMSE of 0.636 that is far lower than the SARIMA’s 0.675.\n\n\nCode\nindex5 = c(1:nrow(model5comparison))\nggplot(data = model5comparison, aes(x = index5, y = rmse1), color = 'blue') + geom_line() + geom_line(aes(x = index5, y = rmse2), color = 'red') + labs(title = 'Comparing RMSE of ARIMAX(1,1,3) Black and SARIMAX(1,0,1)(1,1,0) Red') +  theme(text = element_text(size=14))\n\n\n\n\n\nVisible in the chart above, the ARIMA model (black line) performs much better than the SARIMA at most data points. Unlike previous comparisons, for model 5 the two approaches appear to diverge greatly, trading off in which has a more accurate prediction at each point. Overall however, we can see that for a given value the black line is more likely to be correct, and so I would choose the ARIMA(1,1,3) model for the IWM data.\n\nPredictions\nNow, let’s make predictions:\n\n\nCode\nxMatrix5 = combinedData[,c('events', 'mo12delta', 'mo6delta', 'mo3delta', 'dailyChange', 'workers')]\n\nmod5events &lt;- forecast(auto.arima(combinedData$events), 10)\nmod5delta12 &lt;- forecast(auto.arima(combinedData$mo12delta), 10)\nmod5delta6 &lt;- forecast(auto.arima(combinedData$mo6delta), 10)\nmod5delta3 &lt;- forecast(auto.arima(combinedData$mo3delta), 10)\nmod5dailyChange &lt;- forecast(auto.arima(combinedData$dailyChange), 10)\nmod5workers &lt;- forecast(auto.arima(combinedData$workers), 10)\n\n\npredictors5 &lt;- data.frame(cbind(events = mod5events$mean, mo12delta = mod5delta12$mean, mo6delta = mod5delta6$mean, mo3delta = mod5delta3$mean, dailyChange = mod5dailyChange$mean, workers = mod5workers$mean))\n\n\nfit5 = arima(combinedData$iwmRange, order = c(1,1,3), xreg = xMatrix5)\n\n#summary(fit)\n#forecast(fit)\nmod5 = predict(fit5, newxreg = predictors5)\n\nautoplot(mod5$pred, main = \"Predictions, Next 10 Timesteps, Model 5\")\n\n\n\n\n\nHere we can see the models predictions for the QQQ’s daily upcoming range, using auto.arima generated models to predict the external regressors. Overall, the model predicts a steep incline in QQQ intraday ranges in the upcoming several days, but then decreases again for the remained of the prediction window.\nI will dicsuss our conclusions from these 5 models further in the conclusions page."
  },
  {
    "objectID": "Conclusions.html",
    "href": "Conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Results in Brief\n\n\nThe project attempted to model intraday volatility, measured as the range between the highest and lowest daily price recorded as a percentage of the opening price. In particular, it looked at this metric for popular exchange traded funds tracking the S&P 500, NASDAQ 100, and Russel 2000. To accomplish this, it introduced exogenous variables from four sources: extreme weather events, interest rate expectations, investor confidence, and labor stoppages.\nUltimately, the three ETFs proved to have similar dynamics both as univariate time series and also as elements in multivariate models. The external predictors, however, seemed substantially distinguished from eachother and almost all contained some relevant data in predicting the three outcome variables. When exploring the data, it became clear that the amount of variation in the indices intraday range was not constant, and at different points in time had been much higher or lower than average. Just as with the overall trend, however, the three indices shared this trait in common.\nIn an attempt to model these variables and make predictions, the project began with ARIMA and SARIMA models. ARMA models proved immediately effective, with all of the main series having a variant of an ARMA model performing well. Somewhat surprisingly, however, the financial instruments did not have major seasonal components. This rendered SARIMA models largely ineffective in describind their behavior.\nThe next step was to incorporate the exogenous datasets in order to explain more of the variation in the indices’ intraday ranges. These exogenous predictors were combined in a number of ways, through both VAR and ARIMAX models. On this front, the role of the exogenous predictors was encouraging. All four datasets had relevant predictive power. In particular: the daily change in the VIX (investor sentiment), hurricane events (weather data), daily striking workers (labor stoppages), and three month interest rate changes (bond yield data). While these models did not explain all of the variation in the data by any means, they demonstrated that these external factors impact market volatility.\nFinancial time series modeling with ARCH and GARCH models made intuitive sense for the data. The intraday ranges exhibited similar behavior to daily stock market returns, which are often the focus of ARCH and GARCH models. All of the major indices were well suited to GARCH models in some form, which suggests that volatility can be modeled similar to traditional financial metrics.\nFinally, several deep learning methods, including RNN, GRU, and LSTM models, were applied to the dataset. Here, the LSTM models performed the best, and outperformed all traditional statistical models except for the VAR approaches. This demonstrated that univariate volatility is best understood with deep learning, although multivariate analysis is still best performed statistically.\nOverall, the work in this project has indicated that many external factors play a role in shaping market volatility, and the results contained herein suggest future work is merited in identifying other exogenous predictors that are shaping the markets which impact all of our lives."
  },
  {
    "objectID": "Conclusions.html#synthesizing-results-from-across-the-project",
    "href": "Conclusions.html#synthesizing-results-from-across-the-project",
    "title": "Conclusions",
    "section": "",
    "text": "Results in Brief\n\n\nThe project attempted to model intraday volatility, measured as the range between the highest and lowest daily price recorded as a percentage of the opening price. In particular, it looked at this metric for popular exchange traded funds tracking the S&P 500, NASDAQ 100, and Russel 2000. To accomplish this, it introduced exogenous variables from four sources: extreme weather events, interest rate expectations, investor confidence, and labor stoppages.\nUltimately, the three ETFs proved to have similar dynamics both as univariate time series and also as elements in multivariate models. The external predictors, however, seemed substantially distinguished from eachother and almost all contained some relevant data in predicting the three outcome variables. When exploring the data, it became clear that the amount of variation in the indices intraday range was not constant, and at different points in time had been much higher or lower than average. Just as with the overall trend, however, the three indices shared this trait in common.\nIn an attempt to model these variables and make predictions, the project began with ARIMA and SARIMA models. ARMA models proved immediately effective, with all of the main series having a variant of an ARMA model performing well. Somewhat surprisingly, however, the financial instruments did not have major seasonal components. This rendered SARIMA models largely ineffective in describind their behavior.\nThe next step was to incorporate the exogenous datasets in order to explain more of the variation in the indices’ intraday ranges. These exogenous predictors were combined in a number of ways, through both VAR and ARIMAX models. On this front, the role of the exogenous predictors was encouraging. All four datasets had relevant predictive power. In particular: the daily change in the VIX (investor sentiment), hurricane events (weather data), daily striking workers (labor stoppages), and three month interest rate changes (bond yield data). While these models did not explain all of the variation in the data by any means, they demonstrated that these external factors impact market volatility.\nFinancial time series modeling with ARCH and GARCH models made intuitive sense for the data. The intraday ranges exhibited similar behavior to daily stock market returns, which are often the focus of ARCH and GARCH models. All of the major indices were well suited to GARCH models in some form, which suggests that volatility can be modeled similar to traditional financial metrics.\nFinally, several deep learning methods, including RNN, GRU, and LSTM models, were applied to the dataset. Here, the LSTM models performed the best, and outperformed all traditional statistical models except for the VAR approaches. This demonstrated that univariate volatility is best understood with deep learning, although multivariate analysis is still best performed statistically.\nOverall, the work in this project has indicated that many external factors play a role in shaping market volatility, and the results contained herein suggest future work is merited in identifying other exogenous predictors that are shaping the markets which impact all of our lives."
  },
  {
    "objectID": "Conclusions.html#answering-research-questions",
    "href": "Conclusions.html#answering-research-questions",
    "title": "Conclusions",
    "section": "Answering Research Questions",
    "text": "Answering Research Questions\n\n\n\n\n\n\n\nQuestion\nAnswer\n\n\n\n\n1. Is the intraday volatility of major stock indices truly random, or does it contain patterns that can be modelled?\nWhile no model can explain 100%, or even a majority of the volatility in these major indices, many different models can explain some part. This is evidenced by the many models in the project that achieved significant results.\n\n\n2. Do the major US stock indices differ greatly in the structure of their volatility over time?\nAccording to this project, stock indices have not had major shifts in how their volatility is structured over time, although they do vary in the short term, with periods of increased or decreased volatility.\n\n\n3. How do markets react in response to chaining expectations about macroeconomic conditions?\nIt appears that changes in the pricing of bonds, especially short term bonds for the next three months, are reflected in volatility in major indices. This means markets react with increased volaitlity to changing macroeconomic conditions.\n\n\n4. How do markets incorporate extreme weather event warnings into their pricing of major indices, and how does this affect volatility?\nIt appears weather warnings do not have an effect, however hurricane events in particular may trigger increased volatility, although whether this is through their physical impacts remains an open question.\n\n\n5. How do markets react in response to major strikes and labor stoppages, and does this differ based on the size of the strike or the types of companies in question?\nWhile the number of strikes was less important, I did find evidence that markets reacted to large numbers of workers being on strike in the economy.\n\n\n6. How does investor confidence impact markets?\nInvestor confidence (particularly the VIX index) has a clear, if small, effect on market volatility.\n\n\n7. Do the major US stock indices differ in their response to external stimuli, and are some more sensitive to external volitility?\nBroadly speaking, I found the major indices to be very similar in their reaction to external stimuli and internal dynamics.\n\n\n8. Are the patterns exhibited by intraday volatility in major indices best-suited to traditional statistical models or deep learning? If deep learning models fit better, then what does that say about patterns in volatility?\nIt varies, deep learning models do perform competitively, but traditional models ultimately had the best results in this study.\n\n\n9. Have the relationships between the intraday ranges in the prices of securities and external factors changed over time?\nThis project did not find any evidence of a time-variant effect between the exogenous factors and securities, but the question was ultimately not addressed in the most detail possible. Further work would be nessecary to resolve this conclusively.\n\n\n10. How well can we explain variations in intraday volatility, when synthesizing as much external data and models as possible?\nOverall, we could not explain even 10% of the variation in intraday volatility with models that accounted for many variables, although the results continued to improve as mode predictors were added to the models."
  },
  {
    "objectID": "Data Visualization.html",
    "href": "Data Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The central question of this research project is about stock market volatility. As such the following visualizations will explore this subject in different areas. To start with, let’s define the outcome variables in question:"
  },
  {
    "objectID": "Data Visualization.html#introduction",
    "href": "Data Visualization.html#introduction",
    "title": "Data Visualization",
    "section": "",
    "text": "The central question of this research project is about stock market volatility. As such the following visualizations will explore this subject in different areas. To start with, let’s define the outcome variables in question:"
  },
  {
    "objectID": "Data Visualization.html#financial-data",
    "href": "Data Visualization.html#financial-data",
    "title": "Data Visualization",
    "section": "Financial Data",
    "text": "Financial Data\nFor each security: S&P 500, QQQ Pro-Shares ETF, and Russell 200 index, we are looking at the true range (intraday difference between highest and lowest price/previous closing value). The window of our investigation is January 1st 2021 - September 30th 2023.\nLet’s start by simply seeing the absolute prices of these securities over time:\n\n\n\n\n\nIn these charts, we can see that the overall trend for all three securities has been somewhat of a decline since the start of the time window. We can also see that QQQ and IWM have moved further in their trends than SPY, which makes sense as that is the most stable index of the three.\nNow, let us look at the intraday range measure of volatility, which is expressed as a percent of the opening price:\n\n\nNULL\n\n\n\n\n\nIn this chart, we can see that the three indices tended to move together, with the midpoint of the time window having a higher average range in daily prices than the beginning or end for all of the securities. However, we can also see again that volatility is not even between them, with SPY and IWM having higher daily ranges than QQQ on average (at least according to the naked eye)."
  },
  {
    "objectID": "Data Visualization.html#year-bond-yields",
    "href": "Data Visualization.html#year-bond-yields",
    "title": "Data Visualization",
    "section": "3-Year Bond Yields",
    "text": "3-Year Bond Yields\nNow, let’s review the trends in the federal funds rate:\n\n\n\n\n\n\nBy reviewing the data, we can see that the 1, 3, and 6 month rates on bonds tend to move together. However, the shorter length bonds, namely the 1 month rate, have the greatest volatility. We can also see that these variables will be tricky to include in our analsis, as they remained near zero for the first half of the dataset, before jumping up quickly in mid 2022."
  },
  {
    "objectID": "Data Visualization.html#labor-data",
    "href": "Data Visualization.html#labor-data",
    "title": "Data Visualization",
    "section": "Labor Data",
    "text": "Labor Data\nLet’s examine the strikes that we have contained in our labor dataset. First let’s look at the number of strikes by year:\n\n\n\n\n\nWe can see that the number of strikes varied year-to-year, although 2023 was not complete in the dataset. But what if we looked at the overlap in strikes, and the number of workers striking concurrently:\n\n\n\n\n\n\nWe can see that the number of striking concurrent workers has varied overtime, but it should be an interesting variable to study as there are definite peaks and troughs in the dataset. A few particular strikes had a big impact but lasted only a few days, such as the teachers strikes in Portland and Los Angeles, which each lasted less than a week."
  },
  {
    "objectID": "Data Visualization.html#weather-data",
    "href": "Data Visualization.html#weather-data",
    "title": "Data Visualization",
    "section": "Weather Data",
    "text": "Weather Data\nFinally, let’s see what our weather data for this period looks like:\n\n\n\n\n\n\n\n\nLooking at the first weather data chart, we can see that the total number of weather events appears to have some weak seasonality (which makes sense since weather is seasonal). The lowest months are September through November, and the highest are December through August (winter + hurricane season).\nIn the second chart, we can see that total property damage and casualties move together. Unlike the total number of storms, the data has more heteroskedacticity and less seasonality, suggesting these numbers are more volatile and linked to individual, severe storms. However, the two trends are associated, with periods of elevated property damage having higher casualties and vice versa."
  },
  {
    "objectID": "Exploratory Data Analysis.html",
    "href": "Exploratory Data Analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Let’s start with reading in the intraday range data. First with the S&P 500.\n\n\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(ggplot2)\n\n\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\n#decomposedSPY &lt;- decompose(spyIn$spyRange)\n\n##decompedSPY = HoltWinters(spyIn$spyRange,beta = FALSE,gamma = FALSE)\n#plot(decompedSPY)\n\nautoplot(spyIn$spyRange)\n\n\n\n\n\nCode\nacf(spyIn$spyRange)\n\n\n\n\n\nCode\npacf(spyIn$spyRange)\n\n\n\n\n\nUpon initial review of the SPY intraday range data, it appears that there is some trend, but no seasonality. The decomposition function would not work on the data as it could not recognize periodicity. Let’s review the lag plots of the data for SPY, QQQ, and IWM:\n\n\nCode\nspyLagPlot &lt;- gglagplot(spyIn$spyRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, SPY Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n\nqqqLagPlot &lt;- gglagplot(qqqIn$qqqRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, QQQ Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n  \n\niwmLagPlot &lt;- gglagplot(iwmIn$iwmRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, IWM Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n\n\nspyLagPlot\n\n\n\n\n\nCode\nqqqLagPlot\n\n\n\n\n\nCode\niwmLagPlot\n\n\n\n\n\nWhile these lag plots do not show a clear trend, they do show clear heteroskedacticity. We can see that the ranges are always positive, so this already means the distribution is one-sided. However, the positive ends of the spectrum (a higher percent range for the day) are much further spread out than those values closer to zero. If the data didn’t always have to be positive, I could see the plots looking like Gaussian white noise (i.e. imagine the lag plots visualize, but with each one rotated to all 4 quadrants instead of +X/+Y and you would have a near-Gaussian distribution).\nAs a next step, to study these relationships with the lag plots further, we can difference the data.\n\n\nCode\ndiff1 &lt;- diff(spyIn$spyRange)\ndiffqqq &lt;- diff(qqqIn$qqqRange)\ndiffiwm &lt;- diff(iwmIn$iwmRange)\n\n\nggtsdisplay(diff1, main = \"Differenced SPY Intraday Daily Range\")\n\n\n\n\n\nCode\nggtsdisplay(diffqqq, main = \"Differenced QQQ Intraday Daily Range\")\n\n\n\n\n\nCode\nggtsdisplay(diffiwm, main = \"Differenced IWM Intraday Daily Range\")\n\n\n\n\n\nAfter differencing, the data appears to have lost its trend. Moverover, the first lag in the ACF plot has very strong correlation, suggesting we may have over-differenced. However, there is still visible heteroskedacticity in the differences between intraday ranges. These patterns are essentially the same between the three charts.\n\n\nCode\nacf(diff1,    na.action = na.exclude, main = \"\")\ntitle(\"First Order Differenced ACF for SPY Daily Range\")\n\n\n\n\n\nCode\npacf(diff1,    na.action = na.exclude, main = \"\")\ntitle(\"First Order Differenced PACF for SPY Daily Range\")\n\n\n\n\n\nAfter differencing, we see 2 lags being significant in the ACF plot. In the PACF plot, we see about 4 lags being significant. This is a marked departure from the original plots, where the ACF showed clear non-stationarity and the PACF had many significant lags.\n\n\nCode\nlibrary(tseries)\n\ndiffnoNA &lt;- diff1$spyRange[!is.na(diff1$spyRange)]\nadf.test(diffnoNA)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNA\nDickey-Fuller = -12.455, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ndiffnoNAqqq &lt;- diffqqq$qqqRange[!is.na(diffqqq$qqqRange)]\n\nadf.test(diffnoNAqqq)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNAqqq\nDickey-Fuller = -12.75, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ndiffnoNAiwm &lt;- diffiwm$iwmRange[!is.na(diffiwm$iwmRange)]\n\nadf.test(diffnoNAiwm)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNAiwm\nDickey-Fuller = -12.624, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWith dickey fuller test result of 0.01 for all three series, we can reject the null hypothesis and conclude that all of the series are stationary.\n\n\nDigging once again into the SPY data:\n\n\nCode\nspyDiff1 &lt;- as.ts(diffnoNA)\n\nspyAvg3Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/3, 3))\nspyAvg5Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/5, 5))\nspyAvg30Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/30, 30))\nspyAvg50Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/50, 50))\n\nspyAvg100Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/100, 100))\n\n\nautoplot(spyAvg3Diff1, main = \"3 Day Lag of SPY Daily Range\")\n\n\n\n\n\nCode\nautoplot(spyAvg5Diff1, main = \"5 Day Lag of SPY Daily Range\")\n\n\n\n\n\nCode\nautoplot(spyAvg30Diff1, main = \"30 Day Lag of SPY Daily Range\")\n\n\n\n\n\nCode\nautoplot(spyAvg50Diff1, main = \"50 Day Lag of SPY Daily Range\")\n\n\n\n\n\nCode\nautoplot(spyAvg100Diff1, main = \"100 Day Lag of SPY Daily Range\")\n\n\n\n\n\nOverall the effect of the moving average at higher numbers is to reveal trend and seasonality in the data. At lower smoothing levels (3 and 5 days), the time series looked almost unchanged, and still highly variant. Only at a 30 day average window did the seasonality start to appear, with clear periods in the data and a repeating pattern. Notably, even with a repeating pattern the data still showed obvious heteroskedasticity, with periods of increased varianced. The 50 and 100 day moving average windows began to obfuscate the periods, while keeping the heteroskedacticity, although they did have smaller variations and were nearer to 0 on average. In adition, the 50 and 100 days had discernable trends in the data. We see similar results for QQQ and IWM, which I have abridged here."
  },
  {
    "objectID": "Exploratory Data Analysis.html#time-series-analysis",
    "href": "Exploratory Data Analysis.html#time-series-analysis",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Let’s start with reading in the intraday range data. First with the S&P 500.\n\n\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(ggplot2)\n\n\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\n#decomposedSPY &lt;- decompose(spyIn$spyRange)\n\n##decompedSPY = HoltWinters(spyIn$spyRange,beta = FALSE,gamma = FALSE)\n#plot(decompedSPY)\n\nautoplot(spyIn$spyRange)\n\n\n\n\n\nCode\nacf(spyIn$spyRange)\n\n\n\n\n\nCode\npacf(spyIn$spyRange)\n\n\n\n\n\nUpon initial review of the SPY intraday range data, it appears that there is some trend, but no seasonality. The decomposition function would not work on the data as it could not recognize periodicity. Let’s review the lag plots of the data for SPY, QQQ, and IWM:\n\n\nCode\nspyLagPlot &lt;- gglagplot(spyIn$spyRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, SPY Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n\nqqqLagPlot &lt;- gglagplot(qqqIn$qqqRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, QQQ Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n  \n\niwmLagPlot &lt;- gglagplot(iwmIn$iwmRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, IWM Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n\n\nspyLagPlot\n\n\n\n\n\nCode\nqqqLagPlot\n\n\n\n\n\nCode\niwmLagPlot\n\n\n\n\n\nWhile these lag plots do not show a clear trend, they do show clear heteroskedacticity. We can see that the ranges are always positive, so this already means the distribution is one-sided. However, the positive ends of the spectrum (a higher percent range for the day) are much further spread out than those values closer to zero. If the data didn’t always have to be positive, I could see the plots looking like Gaussian white noise (i.e. imagine the lag plots visualize, but with each one rotated to all 4 quadrants instead of +X/+Y and you would have a near-Gaussian distribution).\nAs a next step, to study these relationships with the lag plots further, we can difference the data.\n\n\nCode\ndiff1 &lt;- diff(spyIn$spyRange)\ndiffqqq &lt;- diff(qqqIn$qqqRange)\ndiffiwm &lt;- diff(iwmIn$iwmRange)\n\n\nggtsdisplay(diff1, main = \"Differenced SPY Intraday Daily Range\")\n\n\n\n\n\nCode\nggtsdisplay(diffqqq, main = \"Differenced QQQ Intraday Daily Range\")\n\n\n\n\n\nCode\nggtsdisplay(diffiwm, main = \"Differenced IWM Intraday Daily Range\")\n\n\n\n\n\nAfter differencing, the data appears to have lost its trend. Moverover, the first lag in the ACF plot has very strong correlation, suggesting we may have over-differenced. However, there is still visible heteroskedacticity in the differences between intraday ranges. These patterns are essentially the same between the three charts.\n\n\nCode\nacf(diff1,    na.action = na.exclude, main = \"\")\ntitle(\"First Order Differenced ACF for SPY Daily Range\")\n\n\n\n\n\nCode\npacf(diff1,    na.action = na.exclude, main = \"\")\ntitle(\"First Order Differenced PACF for SPY Daily Range\")\n\n\n\n\n\nAfter differencing, we see 2 lags being significant in the ACF plot. In the PACF plot, we see about 4 lags being significant. This is a marked departure from the original plots, where the ACF showed clear non-stationarity and the PACF had many significant lags.\n\n\nCode\nlibrary(tseries)\n\ndiffnoNA &lt;- diff1$spyRange[!is.na(diff1$spyRange)]\nadf.test(diffnoNA)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNA\nDickey-Fuller = -12.455, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ndiffnoNAqqq &lt;- diffqqq$qqqRange[!is.na(diffqqq$qqqRange)]\n\nadf.test(diffnoNAqqq)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNAqqq\nDickey-Fuller = -12.75, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ndiffnoNAiwm &lt;- diffiwm$iwmRange[!is.na(diffiwm$iwmRange)]\n\nadf.test(diffnoNAiwm)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNAiwm\nDickey-Fuller = -12.624, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWith dickey fuller test result of 0.01 for all three series, we can reject the null hypothesis and conclude that all of the series are stationary.\n\n\nDigging once again into the SPY data:\n\n\nCode\nspyDiff1 &lt;- as.ts(diffnoNA)\n\nspyAvg3Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/3, 3))\nspyAvg5Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/5, 5))\nspyAvg30Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/30, 30))\nspyAvg50Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/50, 50))\n\nspyAvg100Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/100, 100))\n\n\nautoplot(spyAvg3Diff1, main = \"3 Day Lag of SPY Daily Range\")\n\n\n\n\n\nCode\nautoplot(spyAvg5Diff1, main = \"5 Day Lag of SPY Daily Range\")\n\n\n\n\n\nCode\nautoplot(spyAvg30Diff1, main = \"30 Day Lag of SPY Daily Range\")\n\n\n\n\n\nCode\nautoplot(spyAvg50Diff1, main = \"50 Day Lag of SPY Daily Range\")\n\n\n\n\n\nCode\nautoplot(spyAvg100Diff1, main = \"100 Day Lag of SPY Daily Range\")\n\n\n\n\n\nOverall the effect of the moving average at higher numbers is to reveal trend and seasonality in the data. At lower smoothing levels (3 and 5 days), the time series looked almost unchanged, and still highly variant. Only at a 30 day average window did the seasonality start to appear, with clear periods in the data and a repeating pattern. Notably, even with a repeating pattern the data still showed obvious heteroskedasticity, with periods of increased varianced. The 50 and 100 day moving average windows began to obfuscate the periods, while keeping the heteroskedacticity, although they did have smaller variations and were nearer to 0 on average. In adition, the 50 and 100 days had discernable trends in the data. We see similar results for QQQ and IWM, which I have abridged here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hiking in the Cascades circa. 2020\n\n\n\nHi Everyone, I am Corwin Dark and I am a first year master’s student in the Data Science and Analytics Program here at Georgetown. I grew up in Seattle, Washington before coming to DC for university. I attended American University for my undergraduate degree, which was in international relations. After graduating during COVID, I worked for a small consulting firm on data-related topics for international development programs. After a few years of working, I realized I had a strong curiosity to learn more about cutting edge data science methods, and so I have come back to school. Within data science, I am really interested in how algorithms and techniques can be combined to perform better than they would alone, which is why I really enjoyed creating this project for class and drawing on so many different techniques. I am super excited to be in the Georgetown program with you all, and I hope you enjoy my portfolio! Start reading with the introduction here."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "About Me",
    "section": "",
    "text": "Hiking in the Cascades circa. 2020\n\n\n\nHi Everyone, I am Corwin Dark and I am a first year master’s student in the Data Science and Analytics Program here at Georgetown. I grew up in Seattle, Washington before coming to DC for university. I attended American University for my undergraduate degree, which was in international relations. After graduating during COVID, I worked for a small consulting firm on data-related topics for international development programs. After a few years of working, I realized I had a strong curiosity to learn more about cutting edge data science methods, and so I have come back to school. Within data science, I am really interested in how algorithms and techniques can be combined to perform better than they would alone, which is why I really enjoyed creating this project for class and drawing on so many different techniques. I am super excited to be in the Georgetown program with you all, and I hope you enjoy my portfolio! Start reading with the introduction here."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "About Me",
    "section": "Interests",
    "text": "Interests\n\nDeep learning and neural networks\nEnsembling and meta learning\nFinancial data science"
  }
]