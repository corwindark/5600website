[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hiking in the Cascades circa. 2020\n\n\nHi Everyone, I am Corwin Dark and I am a first year master’s student in the Data Science and Analytics Program here at Georgetown. I grew up in Seattle, Washington before coming to DC for university. I attended American University for my undergraduate degree, which was in international relations. After graduating during COVID, I worked for a small consulting firm on data-related topics for international development programs. After a few years of working, I realized I had a strong curiosity to learn more about cutting edge data science methods, and so I have come back to school. Within data science, I am really interested in how algorithms and techniques can be combined to perform better than they would alone, which is why I really enjoyed creating this project for class and drawing on so many different techniques. I am super excited to be in the Georgetown program with you all, and I hope you enjoy my portfolio! Start reading with the introduction here."
  },
  {
    "objectID": "Exploratory Data Analysis.html",
    "href": "Exploratory Data Analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Let’s start with reading in the intraday range data. First with the S&P 500.\n\n\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(ggplot2)\n\n\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\n#decomposedSPY &lt;- decompose(spyIn$spyRange)\n\n##decompedSPY = HoltWinters(spyIn$spyRange,beta = FALSE,gamma = FALSE)\n#plot(decompedSPY)\n\nautoplot(spyIn$spyRange)\n\n\n\n\n\nCode\nacf(spyIn$spyRange)\n\n\n\n\n\nCode\npacf(spyIn$spyRange)\n\n\n\n\n\nUpon initial review of the SPY intraday range data, it appears that there is some trend, but no seasonality. The decomposition function would not work on the data as it could not recognize periodicity. Let’s review the lag plots of the data for SPY, QQQ, and IWM:\n\n\nCode\nspyLagPlot &lt;- gglagplot(spyIn$spyRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, SPY Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n\nqqqLagPlot &lt;- gglagplot(qqqIn$qqqRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, QQQ Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n  \n\niwmLagPlot &lt;- gglagplot(iwmIn$iwmRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, IWM Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n\n\nspyLagPlot\n\n\n\n\n\nCode\nqqqLagPlot\n\n\n\n\n\nCode\niwmLagPlot\n\n\n\n\n\nWhile these lag plots do not show a clear trend, they do show clear heteroskedacticity. We can see that the ranges are always positive, so this already means the distribution is one-sided. However, the positive ends of the spectrum (a higher percent range for the day) are much further spread out than those values closer to zero. If the data didn’t always have to be positive, I could see the plots looking like Gaussian white noise (i.e. imagine the lag plots visualize, but with each one rotated to all 4 quadrants instead of +X/+Y and you would have a near-Gaussian distribution).\nAs a next step, to study these relationships with the lag plots further, we can difference the data.\n\n\nCode\ndiff1 &lt;- diff(spyIn$spyRange)\ndiffqqq &lt;- diff(qqqIn$qqqRange)\ndiffiwm &lt;- diff(iwmIn$iwmRange)\n\n\nggtsdisplay(diff1, main = \"Differenced SPY Intraday Daily Range\")\n\n\n\n\n\nCode\nggtsdisplay(diffqqq, main = \"Differenced QQQ Intraday Daily Range\")\n\n\n\n\n\nCode\nggtsdisplay(diffiwm, main = \"Differenced IWM Intraday Daily Range\")\n\n\n\n\n\nAfter differencing, the data appears to have lost its trend. Moverover, the first lag in the ACF plot has very strong correlation, suggesting we may have over-differenced. However, there is still visible heteroskedacticity in the differences between intraday ranges. These patterns are essentially the same between the three charts.\n\n\nCode\nacf(diff1,    na.action = na.exclude)\n\n\n\n\n\nCode\npacf(diff1,    na.action = na.exclude)\n\n\n\n\n\nAfter differencing, we see 2 lags being significant in the ACF plot. In the PACF plot, we see about 4 lags being significant. This is a marked departure from the original plots, where the ACF showed clear non-stationarity and the PACF had many significant lags.\n\n\nCode\nlibrary(tseries)\n\ndiffnoNA &lt;- diff1$spyRange[!is.na(diff1$spyRange)]\nadf.test(diffnoNA)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNA\nDickey-Fuller = -12.455, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\n#\n\n\nWith dickey fuller test result of 0.01, we can reject the null hypothesis and conclude the series is stationary.\n\n\n\n\nCode\nspyDiff1 &lt;- as.ts(diffnoNA)\n\nspyAvg3Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/3, 3))\nspyAvg5Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/5, 5))\nspyAvg30Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/30, 30))\nspyAvg50Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/50, 50))\n\nspyAvg100Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/100, 100))\n\n\nautoplot(spyAvg3Diff1)\n\n\n\n\n\nCode\nautoplot(spyAvg5Diff1)\n\n\n\n\n\nCode\nautoplot(spyAvg30Diff1)\n\n\n\n\n\nCode\nautoplot(spyAvg50Diff1)\n\n\n\n\n\nCode\nautoplot(spyAvg100Diff1)\n\n\n\n\n\nOverall the effect of the moving average at higher numbers is to reveal seasonality in the data. At lower smoothing levels (3 and 5 days), the time series looked almost unchanged, and still highly variant. Only at a 30 day average window did the seasonality start to appear, with clear periods in the data and a repeating pattern. Notably, even with a repeating pattern the data still showed obvious heteroskedasticity, with periods of increased varianced. The 50 and 100 day moving average windows began to obfuscate the periods, while keeping the heteroskedacticity, although they did have smaller variations and were nearer to 0 on average. In adition, the 50 and 100 days had discernable trends in the data."
  },
  {
    "objectID": "Deep Learning for TS.html",
    "href": "Deep Learning for TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nimport subprocess\n\nimport sys\nimport os \n\ndef install(name):\n    subprocess.call([sys.executable, '-m', 'pip', 'install', name])\n\n#install('yfinance')\n\n\nWARNING:tensorflow:From C:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead."
  },
  {
    "objectID": "Data Sources.html",
    "href": "Data Sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Financial data on the prices of different stocks are readily available in both R and Python through various packages, and amongst these the Yahoo Finance package is a particularly popular option. To approximate the daily range in value of major indices, I chose to look at popular low-cost ETFs which attempt to track the value of stocks contained in these indices. For the S&P 500, this is SPY, run by State Street, for the Russel 2000 this is iShare’s IWM, and for the NASDAQ 100 it is the ever-popular Invesco QQQ fund. The data I haave gathered for these tickers includes the open and close of stock prices for each day, as well as the highest and lowest price recorded for the day. These indices are widely used and will be a relevant outcome variable for the project to study. The main output value that will be calculated is the high-low range of the day.\n\n\n\nSPY Historical Data\nIWM Historical Data\nQQQ Historical Data"
  },
  {
    "objectID": "ARMA ARIMA SARIMA Models.html",
    "href": "ARMA ARIMA SARIMA Models.html",
    "title": "ARMA ARIMA SARIMA MODELS",
    "section": "",
    "text": "Loading packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(lubridate)\n\n\nBringing the data into this tab as well:\n\n\nShow Code\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\ndiff1SPY &lt;- diff(spyIn$spyRange)\ndiff1QQQ &lt;- diff(qqqIn$qqqRange)\ndiff1IWM &lt;- diff(iwmIn$iwmRange)\n\n\n\nStationarity of the time series\n\nBased on previous results, and the fact that I am using “pseudo-differenced data” in that I am taking the percentage range in prices, in addition to a single differencing, means that the time series are stationary\n\nBuilding ARIMA Model\n\nSince I did some of this work with SPY data on the EDA tab, I will focus on QQQ range data here.\n\n\nCode\n#print(diff1QQQ)\nacf(diff1QQQ, na.action = na.exclude)\n\n\n\n\n\nCode\npacf(diff1QQQ, na.action = na.exclude)\n\n\n\n\n\nBased on these charts the order I would pick for QQQ is: ARIMA(2,1,0)\n\n\nCode\nmodelQQQ1 &lt;- arima(diff1QQQ, order = c(2,1,0))\nsummary(modelQQQ1)\n\n\n\nCall:\narima(x = diff1QQQ, order = c(2, 1, 0))\n\nCoefficients:\n          ar1      ar2\n      -1.0035  -0.4832\ns.e.   0.0334   0.0334\n\nsigma^2 estimated as 0.0001131:  log likelihood = 2149.08,  aic = -4292.16\n\nTraining set error measures:\n                       ME       RMSE         MAE      MPE     MAPE      MASE\nTraining set 4.630039e-05 0.01062913 0.007805518 87.46782 258.6253 0.6398223\n                   ACF1\nTraining set -0.1944037\n\n\n\nEquation is x = -1.0034x(t-1) - 0.4832x(t-2) + error\nModel Diagnostic:\n\n\n\nCode\nstats::tsdiag(modelQQQ1)\n\n\n\n\n\nThe Ljung Box statistics look cood, although the ACF of the residuals does have 1 significant term.\nI originally tried a (4,1,2) model, however the ljung box statistics were highly correlated, and I suspected overfitting. After reducing the parametrization greatly, the new model performed mnuch better.\n\n\n\n\n\nCode\nautoQQQ &lt;- auto.arima(diff1QQQ)\nsummary(autoQQQ)\n\n\nSeries: diff1QQQ \nARIMA(1,0,1) with zero mean \n\nCoefficients:\n          ar1      ma1\n      -0.0008  -0.8122\ns.e.   0.0524   0.0361\n\nsigma^2 = 5.483e-05:  log likelihood = 2402.77\nAIC=-4799.55   AICc=-4799.51   BIC=-4785.94\n\nTraining set error measures:\n                        ME       RMSE         MAE      MPE     MAPE     MASE\nTraining set -6.694728e-05 0.00739421 0.005455453 283.3672 549.0245 0.446905\n                    ACF1\nTraining set 0.003755607\n\n\nThe auto.arima method chose an ARIMA(1,0,1) model. However, this model did not perform as well in terms of AIC, with the Auto arima model having a score of -4799 while my model had a score of -4292.\n\nForecasting with my model\n\n\n\nCode\nplot(forecast(modelQQQ1, 10), xlim = c(650,750))   \n\n\n\n\n\nForecasting with auto arima model\n\n\nCode\nplot(forecast(autoQQQ, 10), xlim = c(650,750))\n\n\n\n\n\nOverall, my model has a slightly more dynamic prediction than the auto arima function, which quicly levels out to 0. However, my model also has a much wider uncertainty band.\n\nCompare ARIMA model with benchmarks\n\n\n\nCode\nnaiveModelQQQ &lt;- naive(diff1QQQ, h=1)\nsnaiveModelQQQ &lt;- snaive(diff1QQQ, h=1)\n\nsummary(naiveModelQQQ)\n\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = diff1QQQ, h = 1) \n\nResidual sd: 0.0165 \n\nError measures:\n                       ME     RMSE        MAE      MPE     MAPE MASE       ACF1\nTraining set 2.387746e-05 0.016488 0.01219951 309.6279 564.6879    1 -0.6746326\n\nForecasts:\n    Point Forecast       Lo 80      Hi 80       Lo 95      Hi 95\n691   -0.003168135 -0.02429836 0.01796209 -0.03548403 0.02914776\n\n\nCode\nsummary(snaiveModelQQQ)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = diff1QQQ, h = 1) \n\nResidual sd: 0.0165 \n\nError measures:\n                       ME     RMSE        MAE      MPE     MAPE MASE       ACF1\nTraining set 2.387746e-05 0.016488 0.01219951 309.6279 564.6879    1 -0.6746326\n\nForecasts:\n    Point Forecast       Lo 80      Hi 80       Lo 95      Hi 95\n691   -0.003168135 -0.02429836 0.01796209 -0.03548403 0.02914776\n\n\nCode\nsummary(modelQQQ1)\n\n\n\nCall:\narima(x = diff1QQQ, order = c(2, 1, 0))\n\nCoefficients:\n          ar1      ar2\n      -1.0035  -0.4832\ns.e.   0.0334   0.0334\n\nsigma^2 estimated as 0.0001131:  log likelihood = 2149.08,  aic = -4292.16\n\nTraining set error measures:\n                       ME       RMSE         MAE      MPE     MAPE      MASE\nTraining set 4.630039e-05 0.01062913 0.007805518 87.46782 258.6253 0.6398223\n                   ACF1\nTraining set -0.1944037\n\n\nI fit a naive and seasonal naive model. On RMSE my model had the best performance, with 0.011, while the naive and snaive models had 0.017 rmse each (since there was no seasonal period I realized they were the same model). On MAE my arima model had 0.008 while the seasonal naive models had 0.0122.\nLet’s compare forecasts:\n\n\nCode\nplot(forecast(modelQQQ1, 10), xlim = c(650,750)) \n\n\n\n\n\n\n\nCode\nplot(forecast(naiveModelQQQ, 10), xlim = c(650,750))   \n\n\n\n\n\nHere, the naive method can only forecast 1 observation into the future, since the seasonal period is one. Which is an advantage to my model, but realistically means the naive model should be evaluated with cross validation.\n\nSARIMA\n\nLet’s look for a seasonal affect in the ACF plots, using the weather events data. First we prepare the data:\n\n\nCode\nweather_data &lt;- read.csv('data/storms_clean.csv')\n\n\nweather_data$month &lt;- weather_data$BEGIN_YEARMONTH %% 100\n\nweather_data &lt;- weather_data %&gt;%\n    mutate(realdate = make_date(YEAR, month, BEGIN_DAY)) %&gt;%\n    mutate(DAMAGE_PROPERTY =  str_replace(DAMAGE_PROPERTY, \"K\", \"\") )  %&gt;%\n    mutate(DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)) %&gt;%\n    mutate(DAMAGE_PROPERTY = replace_na(DAMAGE_PROPERTY, 0))\n\n\nWarning: There was 1 warning in `mutate()`.\ni In argument: `DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nCode\n# Daily Event Number\ndaily_events &lt;- weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(events = length(EPISODE_ID))\n\n# Daily Property Damage\ndaily_property &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(pdam = sum(DAMAGE_PROPERTY))\n\n# Daily Daily Casualties\ndaily_casualties &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(casualties = sum(INJURIES_DIRECT) + sum(INJURIES_INDIRECT) + sum(DEATHS_DIRECT) + sum(DEATHS_INDIRECT))\n\n# Daily Hurricanes\ndaily_hurricanes &lt;- weather_data %&gt;%\n    filter(EVENT_TYPE == \"Hurricane\") %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(hurricaneWarnings = length(EPISODE_ID))\n\n\n# Daily joined data\n\nweather_merged &lt;- full_join(daily_events, daily_property, by = \"realdate\")\nweather_merged &lt;- full_join(weather_merged, daily_casualties, by = 'realdate')\nweather_merged &lt;- full_join(weather_merged, daily_hurricanes, by = 'realdate')\n\nhead(weather_merged)\n\n\n# A tibble: 6 x 5\n  realdate   events  pdam casualties hurricaneWarnings\n  &lt;date&gt;      &lt;int&gt; &lt;dbl&gt;      &lt;int&gt;             &lt;int&gt;\n1 2021-01-01    643  918           1                NA\n2 2021-01-02     75  189.          0                NA\n3 2021-01-03     80   63           3                NA\n4 2021-01-04     44   20           0                NA\n5 2021-01-05     18    0           2                NA\n6 2021-01-06     27  850           2                NA\n\n\nNow, lets look at the acf plot:\n\n\nCode\nacf(weather_merged$events, lag.max = 365)\n\n\n\n\n\nWith a 365 lag plot (as we are looking at weather data), we can see that for about 1/4 of the 365 lags, there is some positive correlation in the residuals (the same season), then for 1/2 the lags after that there is negative correlation (the opposite seasons), and then a return to significant positive correlation about 3/4 of the way through the data. This appears to show a clear seasonal effect of about 365. So let’s seasonally difference the data.\n\n\nCode\nseasonDiff &lt;- weather_merged$events %&gt;% diff(lag = 365)\nacf(seasonDiff, lag.max = 365)\n\n\n\n\n\nCode\npacf(seasonDiff)\n\n\n\n\n\nAfter seasonal differencing, this plot looks much much better, without noticeable season-to-season correlations in the lags, although there is still some short-term correlation. And some repeating period which appears to be almost weekly in the residuals.\nBased on the ACF and PACF plots, I would consider p of 1, d of 0, and q of 3. Then for P and Q I might consider 0, D would be 1 since we seasonally differenced. But let’s run some code to see the AIC of different values:\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \ntemp=c()\nd=1\nD=1\ns=12\n \ni=1\ntemp= data.frame()\nls=matrix(rep(NA,9*378),nrow=378)\n \nfor (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\nSARIMA.c(p1=1,p2=5,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=weather_merged$events) %&gt;% filter(!is.na(p))\n\n\n   p d q P D Q      AIC      BIC     AICc\n1  0 0 0 0 1 0 12204.30 12213.93 12204.32\n2  0 1 0 0 1 0 12419.43 12424.24 12419.43\n3  0 0 0 0 1 1 12204.30 12213.93 12204.32\n4  0 1 0 0 1 1 12419.43 12424.24 12419.43\n5  0 0 0 1 1 0 12204.30 12213.93 12204.32\n6  0 1 0 1 1 0 12419.43 12424.24 12419.43\n7  0 0 0 1 1 1 12204.30 12213.93 12204.32\n8  0 1 0 1 1 1 12419.43 12424.24 12419.43\n9  0 0 0 2 1 0 12204.30 12213.93 12204.32\n10 0 1 0 2 1 0 12419.43 12424.24 12419.43\n11 0 0 0 2 1 1 12204.30 12213.93 12204.32\n12 0 0 1 0 1 0 12102.10 12116.55 12102.13\n13 0 1 1 0 1 0 12130.44 12140.07 12130.46\n14 0 0 1 0 1 1 12102.10 12116.55 12102.13\n15 0 1 1 0 1 1 12130.44 12140.07 12130.46\n16 0 0 1 1 1 0 12102.10 12116.55 12102.13\n17 0 1 1 1 1 0 12130.44 12140.07 12130.46\n18 0 0 1 1 1 1 12102.10 12116.55 12102.13\n19 0 0 1 2 1 0 12102.10 12116.55 12102.13\n20 0 0 2 0 1 0 12090.24 12109.50 12090.28\n21 0 1 2 0 1 0 12071.61 12086.05 12071.64\n22 0 0 2 0 1 1 12090.24 12109.50 12090.28\n23 0 0 2 1 1 0 12090.24 12109.50 12090.28\n24 0 0 3 0 1 0 12090.34 12114.42 12090.41\n25 1 0 0 0 1 0 12084.41 12098.85 12084.43\n26 1 1 0 0 1 0 12303.82 12313.44 12303.83\n27 1 0 0 0 1 1 12084.41 12098.85 12084.43\n28 1 1 0 0 1 1 12303.82 12313.44 12303.83\n29 1 0 0 1 1 0 12084.41 12098.85 12084.43\n30 1 1 0 1 1 0 12303.82 12313.44 12303.83\n31 1 0 0 1 1 1 12084.41 12098.85 12084.43\n32 1 0 0 2 1 0 12084.41 12098.85 12084.43\n33 1 0 1 0 1 0 12084.67 12103.93 12084.72\n34 1 1 1 0 1 0 12067.11 12081.55 12067.14\n35 1 0 1 0 1 1 12084.67 12103.93 12084.72\n36 1 0 1 1 1 0 12084.67 12103.93 12084.72\n37 1 0 2 0 1 0 12077.36 12101.43 12077.43\n38 2 0 0 0 1 0 12085.17 12104.42 12085.21\n39 2 1 0 0 1 0 12240.65 12255.09 12240.67\n40 2 0 0 0 1 1 12085.17 12104.42 12085.21\n41 2 0 0 1 1 0 12085.17 12104.42 12085.21\n42 2 0 1 0 1 0 12074.21 12098.29 12074.28\n43 3 0 0 0 1 0 12085.39 12109.46 12085.45\n\n\nCode\nSARIMA.c(p1=1,p2=5,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=seasonDiff) %&gt;% filter(!is.na(p))\n\n\n   p d q P D Q      AIC      BIC     AICc\n1  0 0 0 0 1 0 7590.941 7599.546 7590.963\n2  0 1 0 0 1 0 7681.255 7685.556 7681.263\n3  0 0 0 0 1 1 7590.941 7599.546 7590.963\n4  0 1 0 0 1 1 7681.255 7685.556 7681.263\n5  0 0 0 1 1 0 7590.941 7599.546 7590.963\n6  0 1 0 1 1 0 7681.255 7685.556 7681.263\n7  0 0 0 1 1 1 7590.941 7599.546 7590.963\n8  0 1 0 1 1 1 7681.255 7685.556 7681.263\n9  0 0 0 2 1 0 7590.941 7599.546 7590.963\n10 0 1 0 2 1 0 7681.255 7685.556 7681.263\n11 0 0 0 2 1 1 7590.941 7599.546 7590.963\n12 0 0 1 0 1 0 7508.844 7521.752 7508.888\n13 0 1 1 0 1 0 7574.405 7583.007 7574.427\n14 0 0 1 0 1 1 7508.844 7521.752 7508.888\n15 0 1 1 0 1 1 7574.405 7583.007 7574.427\n16 0 0 1 1 1 0 7508.844 7521.752 7508.888\n17 0 1 1 1 1 0 7574.405 7583.007 7574.427\n18 0 0 1 1 1 1 7508.844 7521.752 7508.888\n19 0 0 1 2 1 0 7508.844 7521.752 7508.888\n20 0 0 2 0 1 0 7502.242 7519.453 7502.316\n21 0 1 2 0 1 0 7501.777 7514.680 7501.822\n22 0 0 2 0 1 1 7502.242 7519.453 7502.316\n23 0 0 2 1 1 0 7502.242 7519.453 7502.316\n24 0 0 3 0 1 0 7504.242 7525.755 7504.353\n25 1 0 0 0 1 0 7502.164 7515.072 7502.208\n26 1 1 0 0 1 0 7641.447 7650.048 7641.469\n27 1 0 0 0 1 1 7502.164 7515.072 7502.208\n28 1 1 0 0 1 1 7641.447 7650.048 7641.469\n29 1 0 0 1 1 0 7502.164 7515.072 7502.208\n30 1 1 0 1 1 0 7641.447 7650.048 7641.469\n31 1 0 0 1 1 1 7502.164 7515.072 7502.208\n32 1 0 0 2 1 0 7502.164 7515.072 7502.208\n33 1 0 1 0 1 0 7502.957 7520.168 7503.031\n34 1 1 1 0 1 0 7494.739 7507.642 7494.784\n35 1 0 1 0 1 1 7502.957 7520.168 7503.031\n36 1 0 1 1 1 0 7502.957 7520.168 7503.031\n37 1 0 2 0 1 0 7504.242 7525.755 7504.353\n38 2 0 0 0 1 0 7502.828 7520.038 7502.902\n39 2 1 0 0 1 0 7610.893 7623.795 7610.937\n40 2 0 0 0 1 1 7502.828 7520.038 7502.902\n41 2 0 0 1 1 0 7502.828 7520.038 7502.902\n42 2 0 1 0 1 0 7504.793 7526.306 7504.904\n43 3 0 0 0 1 0 7504.726 7526.239 7504.838\n\n\nBased on the results of the function, the minimum AIC and BIC are for the model: (0,1,2)(0,1,0). If I run the SARIMA function on the 365 differenced data, then it returns (1,1,1,)(0,1,0) as the model with the lowest AIC. So I will compare these two models for the series, using diagnostics.\n\n\nCode\nmod1 &lt;- Arima(weather_merged$events,order=c(0,1,2),seasonal=c(0,1,0))\nmod2 &lt;- Arima(weather_merged$events,order=c(1,1,1),seasonal=c(0,1,0))\n\ncheckresiduals(mod1 )\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,2)\nQ* = 8.0235, df = 8, p-value = 0.4312\n\nModel df: 2.   Total lags used: 10\n\n\nThe residuals for model 1 (0,1,2)(0,1,0) show some clustering of volatility. In addition, they appear to be skewed to the right, as the right tail of the residual distribution is fatter than the left tail and has more outlying values. Finally, the ACF plot of the residuals looks good, with little visible correlation and no values crossing the significance line. In addition, the Ljung-Box test returns p=0.43, suggesting we can reject the idea that there is autocorrelation in the residuals.\n\n\nCode\ncheckresiduals(mod2 )\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,1)\nQ* = 2.7198, df = 8, p-value = 0.9507\n\nModel df: 2.   Total lags used: 10\n\n\nThe residual diagnostics for model 2 (1,1,1)(0,1,0) are similar to model 1, except that they have even less correlatoin visible in the residual plots. The residual’s distribution is still skewed to the right, with a fatter positive tail. The ACF plot, however, has even less correlation visible, with only two values even coming marginally close to the significance line. There is still some heteroskedacticity in the plot over time, however, suggesting clustering of volatility.\nNow, let’s use an Auto.Arima function to determine the correct model:\n\n\nCode\naaData &lt;- ts(weather_merged$events)\nmod3 &lt;- auto.arima(aaData, seasonal = TRUE, trace = TRUE)\n\n\n\n Fitting models using approximations to speed things up...\n\n ARIMA(2,0,2) with non-zero mean : 12069.53\n ARIMA(0,0,0) with non-zero mean : 12204.32\n ARIMA(1,0,0) with non-zero mean : 12079.82\n ARIMA(0,0,1) with non-zero mean : 12102.76\n ARIMA(0,0,0) with zero mean     : 12785.96\n ARIMA(1,0,2) with non-zero mean : 12082.52\n ARIMA(2,0,1) with non-zero mean : 12069.52\n ARIMA(1,0,1) with non-zero mean : 12081.01\n ARIMA(2,0,0) with non-zero mean : 12079.36\n ARIMA(3,0,1) with non-zero mean : 12069.61\n ARIMA(3,0,0) with non-zero mean : 12080.39\n ARIMA(3,0,2) with non-zero mean : 12072.31\n ARIMA(2,0,1) with zero mean     : Inf\n\n Now re-fitting the best model(s) without approximations...\n\n ARIMA(2,0,1) with non-zero mean : 12074.28\n\n Best model: ARIMA(2,0,1) with non-zero mean \n\n\nRegardless of the frequency fed into the model, Auto.arima only wants to fit a (2,0,1) ARIMA model. I tried 365, 90, 60, 40, 14, and 7 day frequencies, and in each case the seasonal term was not chosen for the data. I think the reason that it doesn’t recognize the seasonality is because it doesn’t worok with 365, which should be the best frequency for the data.\nForecast with a confidence band: Model 1 (0,1,2)(0,1,0)\n\n\nCode\nplot(forecast(mod2), xlim = c(850,925))\n\n\n\n\n\nModel 2: (1,1,1)(0,1,0)\n\n\nCode\nplot(forecast(mod1), xlim = c(850,925))\n\n\n\n\n\nI think the forecasts are interesting, because model 2 has a more dynamic forecast, with a decrease over several days before leveling out its prediction. Model 1, meanwhile, predicts that the series will hardly change after its first prediction. While the series does not have a lot of trend going into the prediction interval, weather events are dynamic and I would tend to believe the model which includes more variation as opposed to constant numbers of events. Hence, I would select the (1,1,1)(0,1,0) SARIMA model.\nBenchmark Comparison\nWe will use two benchmark methods: A seasonal naive forecast and a mean forecast.\n\n\nCode\nbase1 &lt;- snaive(aaData, 10)\nbase2 &lt;- meanf(aaData, 10)\n\nplot(base1, xlim = c(850,925))\n\n\n\n\n\nHere we can see the plot for the seasonal naive model’s forecasts (10 days out), which show a predicted value close to the last observed value in the series. It has a high degree of uncertainty as shown by the prediction interval, which is quite wide.\n\n\nCode\nplot(base2, xlim = c(850,925) ) \n\n\n\n\n\nThe forecast for the meanf model (above) departs further from the previously observed values, as the mean of the series is substantially below recently observed values. However, the model has a smaller prediction interval than the seasonal naive forecast, which is a slight advantage.\nNow let’s look at the accuracy of the three forcasts:\n\n\nCode\naccuracy(snaive(aaData))\n\n\n                    ME     RMSE      MAE       MPE    MAPE MASE       ACF1\nTraining set -0.232967 222.2549 134.6615 -92.97186 141.104    1 -0.3466138\n\n\nCode\naccuracy(meanf(aaData))\n\n\n                       ME     RMSE      MAE       MPE     MAPE     MASE\nTraining set 4.269057e-15 195.8141 137.4244 -231.3227 259.5254 1.020517\n                  ACF1\nTraining set 0.3527022\n\n\nCode\naccuracy(mod2)\n\n\n                   ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 1.865123 182.4393 120.2761 -152.6725 179.3078 0.8931731\n                    ACF1\nTraining set 0.006055532\n\n\nThe accuracy statistics were a mixed result between the seasonal naive forecast and the SARIMA(1,1,1)(0,1,0) model. The mean forecast did not perform better on any metric than the other two models, and could be discarded. On Root Mean Squared Error, the SARIMA model beat the Seasonal Naive model with a value of 182.4 vs. 222.3, respectively. On Mean Absolute Error, the SARIMA model also performed better, with 120.3 compared to the Seasonal Naive model’s 134.7. On the Mean Absolute Percentage Error, however, the Seasonal Naive model performed better than the SARIMA model, achieving 141.1 vs. 179.3 for the SARIMA model.\nOverall, it seems like the accuracy metrics might favor the seasonal naive model, while the prediction forecasts look more accurate for the SARIMA model.\nCross Validation: Let’s do a seasonal cross-validation with 1 and 10-step-ahead forecasts.\n\n\nCode\n    # I add a 100 day buffer to my period of 365 days to have enough data\n    test &lt;- 100 \n    trainnum &lt;- length(aaData) - test\n    rmse1 &lt;- vector(mode = 'numeric', length = 100)\n    rmse2 &lt;- vector(mode = 'numeric', length = 100)\n    rmse361 &lt;- vector(mode = 'numeric', length = 100)\n    rmse362 &lt;- vector(mode = 'numeric', length = 100)\n\n    for(i in 1:100) {\n\n        \n        xtrain &lt;- aaData[c(1:(trainnum + i - 1))]\n        xtest &lt;-  aaData[c((trainnum + i +1):(trainnum+i+10))]\n        \n    \n        ######## model ###########\n        fit2 &lt;- arima(xtrain, order = c(1,1,1), seasonal = list(order = c(0,1,0)))\n        fcast2 &lt;- predict(fit2, n.ahead = 10)$pred\n        \n        # Errors\n\n        rmse2[i]  &lt;-sqrt((fcast2[1]-xtest[1])^2)\n        rmse362[i]  &lt;- mean( sqrt((fcast2 -xtest)^2) )\n        \n    }\n    \n# create index\nindex &lt;- c(1:100)\nggplot() +\n    geom_line(aes(x = index, y = rmse2), color = 'blue' ) + \n    geom_line(aes(x = index, y = rmse362), color = 'red')\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\nWarning: Removed 10 rows containing missing values (`geom_line()`).\n\n\n\n\n\nThe chart above shows the RMSE for the cross validated forecasts with windows 1 and 10. I had to use 10 for my seasonal window as my computer was unable to handle the 365 window, and could not produce results. But my data also had short-term seasonality so I relied upon that here. The red line represents the 10-step ahead forecast average RMSE and the blue line represents the 1-step ahead forecast RMSE. Overall you can see that both forecasts perform better and worse around the same time, except that the short-term forecast has a lagged reaction to the same periods where the long-term forecast performed poorly. In some cases, however, the 1-step RMSE does exceed the 10-step RMSE, suggesting poor short term performance."
  },
  {
    "objectID": "about me.html",
    "href": "about me.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html",
    "href": "ARIMAX SARIMAX VAR.html",
    "title": "ARIMAX SARIMAX VAR",
    "section": "",
    "text": "We have the following independent variables: 1. Interest Rate Expectation Changes - 3 Months 2. Interest Rate Expectation Changes - 6 Months 3. Interest Rate Expectation Changes - 1 Year 4. Extreme Weather Events - Daily Event Number 5. Extreme Weather Events - Daily Property Damage 6. Extreme Weather Events - Daily Casualties 7. Extreme Weather Events - Hurricanes 8. Expected Volatility (VIX) - Value 9. Expected Volatility (VIX) - Daily Change 10. Work Stoppages - Daily Striking Worker Total 11. Work Stoppages - Daily New Strike Beggining 12. Work Stoppages - Daily New Workers Striking\n\n\nFirst we need to create all 12 predictors, then we can combine them to estimate the models as needed.\n\n\nCode\nlibrary(knitr)\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.4.4     v purrr   1.0.1\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(quantmod)\n\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nLoading required package: TTR\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nCode\nlibrary(forecast)\nlibrary(tseries)\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\nlibrary(reticulate)\n\n\nLet’s quickly retrieve the daily stock price ranges for the indices:\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\nNow, let’s gather the VIX data, since it is also treated like a stock price, and should be available from the same package\n\n\nCode\nvixIn &lt;- quantmod::getSymbols(\"^VIX\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nWarning: ^VIX contains missing values. Some functions will not work if objects\ncontain missing values in the middle of the series. Consider using na.omit(),\nna.approx(), na.fill(), etc to remove or replace them.\n\n\nCode\nvixIn$dailyChange &lt;- vixIn$VIX.Close - lag(vixIn$VIX.Close)\n# 8 is vixIn$VIX.Close\n# 9 is vixIn$dailyChange\n\n#vixIn &lt;- vixIn %&gt;% \n#    mutate(date = ymd(index(vixIn)))\nhead(vixIn)\n\n\n           VIX.Open VIX.High VIX.Low VIX.Close VIX.Volume VIX.Adjusted\n2021-01-04    23.04    29.19   22.56     26.97          0        26.97\n2021-01-05    26.94    28.60   24.80     25.34          0        25.34\n2021-01-06    25.48    26.77   22.14     25.07          0        25.07\n2021-01-07    23.67    23.91   22.25     22.37          0        22.37\n2021-01-08    22.43    23.34   21.42     21.56          0        21.56\n2021-01-11    23.31    24.81   23.23     24.08          0        24.08\n           dailyChange\n2021-01-04          NA\n2021-01-05  -1.6299992\n2021-01-06  -0.2700005\n2021-01-07  -2.6999989\n2021-01-08  -0.8100014\n2021-01-11   2.5200005\n\n\nWe have bond yields stored in a CSV, but let’s calculate daily changes:\n\n\nCode\nyieldCurve &lt;- read.csv('data/treasuries.csv')\n\nyieldCurve$mo3delta &lt;- yieldCurve$X3.Mo - lag(yieldCurve$X3.Mo)\nyieldCurve$mo6delta &lt;- yieldCurve$X6.Mo - lag(yieldCurve$X6.Mo)\nyieldCurve$mo12delta &lt;- yieldCurve$X1.Yr - lag(yieldCurve$X1.Yr) \n\nyieldCurve$Date &lt;- mdy(yieldCurve$Date)\n\n\nWorth noting: Both the treasury data and the VIX data only start on January 4th.\nNext up, let’s prepare the weather event data:\n\n\nCode\nweather_data &lt;- read.csv('data/storms_clean.csv')\n\n\nweather_data$month &lt;- weather_data$BEGIN_YEARMONTH %% 100\n\nweather_data &lt;- weather_data %&gt;%\n    mutate(realdate = make_date(YEAR, month, BEGIN_DAY)) %&gt;%\n    mutate(DAMAGE_PROPERTY =  str_replace(DAMAGE_PROPERTY, \"K\", \"\") )  %&gt;%\n    mutate(DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)) %&gt;%\n    mutate(DAMAGE_PROPERTY = replace_na(DAMAGE_PROPERTY, 0))\n\n\nWarning: There was 1 warning in `mutate()`.\ni In argument: `DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nCode\n# Daily Event Number\ndaily_events &lt;- weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(events = length(EPISODE_ID))\n\n# Daily Property Damage\ndaily_property &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(pdam = sum(DAMAGE_PROPERTY))\n\n# Daily Daily Casualties\ndaily_casualties &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(casualties = sum(INJURIES_DIRECT) + sum(INJURIES_INDIRECT) + sum(DEATHS_DIRECT) + sum(DEATHS_INDIRECT))\n\n# Daily Hurricanes\ndaily_hurricanes &lt;- weather_data %&gt;%\n    filter(EVENT_TYPE == \"Hurricane\") %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(hurricaneWarnings = length(EPISODE_ID))\n\n\n# Daily joined data\n\nweather_merged &lt;- full_join(daily_events, daily_property, by = \"realdate\")\nweather_merged &lt;- full_join(weather_merged, daily_casualties, by = 'realdate')\nweather_merged &lt;- full_join(weather_merged, daily_hurricanes, by = 'realdate')\n\nhead(weather_merged)\n\n\n# A tibble: 6 x 5\n  realdate   events  pdam casualties hurricaneWarnings\n  &lt;date&gt;      &lt;int&gt; &lt;dbl&gt;      &lt;int&gt;             &lt;int&gt;\n1 2021-01-01    643  918           1                NA\n2 2021-01-02     75  189.          0                NA\n3 2021-01-03     80   63           3                NA\n4 2021-01-04     44   20           0                NA\n5 2021-01-05     18    0           2                NA\n6 2021-01-06     27  850           2                NA\n\n\nFinally, lets get the striking worker data ready:\n\n\nCode\nstrike_data &lt;- read.csv('data/strikes.csv')\n\n# clean date format\nstrike_data$start = mdy(strike_data$Work.stoppage.beginning.date)\nstrike_data$end = mdy(strike_data$Work.stoppage.ending.date)\nstrike_data$workers = as.numeric(str_replace(strike_data$Number.of.workers.2., \",\", \"\" ))\n\n\nhead(strike_data)\n\n\n            ï..Organizations.involved Industry.code.1.\n1          Hunts Point Produce Market           445200\n2                 Columbia University            61131\n3 Allegheny Technologies Incorporated           332710\n4              Warrior Met Coal, Inc.           212112\n5                 New York University            61131\n6                         Cook County           622110\n  Work.stoppage.beginning.date Work.stoppage.ending.date Number.of.workers.2.\n1                    1/17/2021                 1/23/2021                1,400\n2                    3/15/2021                  1/7/2022                3,000\n3                    3/30/2021                 7/13/2021                1,300\n4                     4/1/2021                 3/31/2022                1,100\n5                    4/26/2021                 5/15/2021                2,200\n6                    6/25/2021                 7/12/2021                2,000\n  Days.idle..cumulative.for.this.work.stoppage.3.      start        end workers\n1                                           5,600 2021-01-17 2021-01-23    1400\n2                                         177,000 2021-03-15 2022-01-07    3000\n3                                          94,900 2021-03-30 2021-07-13    1300\n4                                         273,900 2021-04-01 2022-03-31    1100\n5                                          33,000 2021-04-26 2021-05-15    2200\n6                                          22,000 2021-06-25 2021-07-12    2000\n\n\nCode\ntarget_dates &lt;- ymd(index(spyIn))\n\ndaily_workers &lt;- vector(mode = \"numeric\", length = length(target_dates))\n\nfor(i in seq_along(target_dates)) {\n    tempDat &lt;- strike_data %&gt;%\n        filter(start &lt;= target_dates[i]) %&gt;%\n        filter(end &gt;= target_dates[i])\n\n    daily_workers[i] = sum(tempDat$workers)\n}\n\nworkerDF &lt;- data.frame('workers' = daily_workers, 'date' = target_dates)\nplot(daily_workers, type = 'l')\n\n\n\n\n\nNow, we can combined all of these datasets into one dataframe, joining on the date columns\n\n\nCode\n# Convert TS objects to df, and fix the date column\nvixDF &lt;- data.frame(vixIn)\nvixDF$date &lt;- ymd(index(vixIn))\nspyDF &lt;- data.frame(spyIn)\nspyDF$date &lt;- ymd(index(spyIn))\nqqqDF &lt;- data.frame(qqqIn)\nqqqDF$date &lt;- ymd(index(qqqIn))\niwmDF &lt;- data.frame(iwmIn)\niwmDF$date &lt;- ymd(index(iwmIn))\n\n# Join symbols together\ntickers &lt;- left_join(spyDF, vixDF, by = 'date')\ntickers &lt;- left_join(tickers, qqqDF, by = 'date')\ntickers &lt;- left_join(tickers, iwmDF, by = 'date')\n\n# Join weather data\ncombinedData &lt;- left_join(tickers, weather_merged, by = c(\"date\" = \"realdate\"))\n\n# Join Bond Yields\ncombinedData &lt;- left_join(combinedData, yieldCurve, by = c(\"date\" = \"Date\"))\n\n# Join Labor Data\ncombinedData &lt;- left_join(combinedData, workerDF, by = 'date')\n\nhead(combinedData)\n\n\n  SPY.Open SPY.High SPY.Low SPY.Close SPY.Volume SPY.Adjusted    spyRange\n1   375.31   375.45  364.82    368.79  110210800     354.1974 0.028323266\n2   368.10   372.50  368.05    371.33   66426200     356.6369 0.012089139\n3   369.71   376.98  369.12    373.55  107997700     358.7691 0.021259950\n4   376.10   379.90  375.91    379.10   68766800     364.0994 0.010608854\n5   380.59   381.49  377.10    381.26   71677200     366.1740 0.011534681\n6   377.85   380.58  377.72    378.69   51034700     363.7057 0.007569102\n        date VIX.Open VIX.High VIX.Low VIX.Close VIX.Volume VIX.Adjusted\n1 2021-01-04    23.04    29.19   22.56     26.97          0        26.97\n2 2021-01-05    26.94    28.60   24.80     25.34          0        25.34\n3 2021-01-06    25.48    26.77   22.14     25.07          0        25.07\n4 2021-01-07    23.67    23.91   22.25     22.37          0        22.37\n5 2021-01-08    22.43    23.34   21.42     21.56          0        21.56\n6 2021-01-11    23.31    24.81   23.23     24.08          0        24.08\n  dailyChange QQQ.Open QQQ.High QQQ.Low QQQ.Close QQQ.Volume QQQ.Adjusted\n1          NA   315.11   315.29  305.18    309.31   45305900     304.2443\n2  -1.6299992   308.29   312.14  308.29    311.86   29323400     306.7526\n3  -0.2700005   307.00   311.88  305.98    307.54   52809600     302.5033\n4  -2.6999989   310.28   315.84  310.25    314.98   30394800     309.8215\n5  -0.8100014   317.34   319.39  315.08    319.03   33955800     313.8052\n6   2.5200005   315.98   317.19  313.75    314.42   32746400     309.2707\n    qqqRange IWM.Open IWM.High IWM.Low IWM.Close IWM.Volume IWM.Adjusted\n1 0.03208409   197.54   197.89  190.94    193.50   33664200     186.8583\n2 0.01248826   193.09   197.62  193.07    196.49   27442900     189.7457\n3 0.01921822   199.48   206.78  199.16    204.53   52952200     197.5097\n4 0.01801597   205.71   208.52  205.70    208.17   24031400     201.0247\n5 0.01358174   209.32   209.77  204.66    207.72   29017000     200.5902\n6 0.01088677   205.09   208.12  204.83    207.54   20945100     200.4164\n    iwmRange events pdam casualties hurricaneWarnings X1.Mo X2.Mo X3.Mo X4.Mo\n1 0.03518273     44   20          0                NA  0.09  0.09  0.09    NA\n2 0.02356408     18    0          2                NA  0.08  0.09  0.09    NA\n3 0.03819929     27  850          2                NA  0.09  0.09  0.09    NA\n4 0.01370865     74    0          0                NA  0.09  0.09  0.09    NA\n5 0.02441239     29   20          2                NA  0.08  0.08  0.08    NA\n6 0.01604171     68    0          0                NA  0.09  0.08  0.08    NA\n  X6.Mo X1.Yr X2.Yr X3.Yr X5.Yr X7.Yr X10.Yr X20.Yr X30.Yr mo3delta mo6delta\n1  0.09  0.10  0.11  0.16  0.36  0.64   0.93   1.46   1.66     0.00     0.00\n2  0.09  0.10  0.13  0.17  0.38  0.66   0.96   1.49   1.70     0.00     0.00\n3  0.09  0.11  0.14  0.20  0.43  0.74   1.04   1.60   1.81     0.00     0.00\n4  0.09  0.11  0.14  0.22  0.46  0.78   1.08   1.64   1.85     0.01     0.00\n5  0.09  0.10  0.14  0.24  0.49  0.81   1.13   1.67   1.87     0.00    -0.01\n6  0.10  0.10  0.14  0.22  0.50  0.84   1.15   1.68   1.88    -0.01     0.01\n  mo12delta workers\n1      0.00       0\n2     -0.01       0\n3      0.00       0\n4      0.01       0\n5      0.00       0\n6     -0.01       0\n\n\n\n\n\nWe will combine these 12 predictors into 5 models, for SPY, QQQ, and IWM intraday volatility:\n\n\nThis model is selected based on the literature review, which suggested that weather events and investor expecations could affect stock prices. This is the “kitchen sink” model, where I am throwing in variables from all data sources. However, looking at the variables individually, such as daily property damage vs. SPY daily price range, we don’t nessecarily see clear correlation (see plot below which resembles white noise). But I am interested to see how these variables are related when taking many different contextual factors into account in the same model.\n\n\nCode\nggplot(combinedData, aes(x = log(spyRange), y = log(pdam)) ) + geom_point() + labs(title = \"3 Month Interest Rate Changes vs. VIX Change \", x = \"Log SPY Daily Spread\", y = \"Log Property Damage From Storms\")\n\n\nWarning: Removed 63 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nThis model is based on a belief that there is an interrelationship between VIX prices and bond yields. This is because both would increase and decrease based on investor expectations for macroeconomic performance in upcoming months. If investors feel the economy will perform poorly, then this might predict bond yields lowering, as well as increased volatility which would be reflected by increases in the VIX. We also see a weak linear correlation in these daily values, as pictured in the plot below.\n\n\nCode\nggplot(combinedData, aes(x = mo3delta, y = dailyChange ) ) + geom_point() + labs(title = \"3 Month Interest Rate Changes vs. VIX Change \", x = \"Change in 3-Month Interest Rates\", y = \"Change in VIX Price\")\n\n\nWarning: Removed 30 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nThis model is all about exogenous shocks. New strikes beggining and hurricane warnings are infrequent but extreme events, which have been grouped together with short-term interest rates (3 month window) to try and capture extreme-but-short-termm influences on volatility.\n\n\n\n\n\n\nModels 4 and 5 follow the same logic as model 1, being selected based off of the literature review, but looking at QQQ as oppposed to SPY to see whether large-cap tech companies are more likely to be affected by this kind of volatility."
  },
  {
    "objectID": "Data Visualization.html",
    "href": "Data Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The central question of this research project is about stock market volatility. As such the following visualizations will explore this subject in different areas. To start with, let’s define the outcome variables in question:"
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.4.4     v purrr   1.0.1\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nLoading required package: TTR\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n\n\n\nThis chart shows the extreme volatility of Moderna’s stock price compared to its peers. The stock has gone from the cheapest of the 5 per share, to the most expensive by a factor of 2, back down to the 2nd most expensive, in the 3-year window visible. Meanwhile, the other 4 stocks have followed a roughly slow and steady upward trend, with Abbivie and Amgen having the greatest progression during the time period.\n\n\n\n\n\n\nHere, we see a steep downward trend in BTC price before Winter 2022, however BTC price has recovered from the low and roughly stabalized after that inflection point.\n\n\n\n\n\n\nThe candlestick chart shows how volatile bitcoins price can be in each day of the window studied. The candlesticks show wide intraday ranges in price.\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n      STATION                      NAME       DATE DAPR MDPR PRCP SNOW SNWD\n1 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-06   NA   NA    0    0   NA\n2 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-07   NA   NA    0    0   NA\n3 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-08   NA   NA    0    0   NA\n4 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-09   NA   NA    0    0   NA\n5 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-10   NA   NA    0    0   NA\n6 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-11   NA   NA    0    0   NA\n  TMAX TMIN TOBS WESD\n1   NA   NA   NA   NA\n2   NA   NA   NA   NA\n3   NA   NA   NA   NA\n4   NA   NA   NA   NA\n5   NA   NA   NA   NA\n6   NA   NA   NA   NA\n\n\nWarning: Removed 69 rows containing missing values (`position_stack()`).\n\n\n\n\n\n\nthis chart shows the total precipitation recorded at 7 stations by month of the year. We can see the USC00182325 has a substantially higher total precipitation amount in August and September than all the other months. It is dubious, however, that the same station records no precipitation from October to December, and it leads me to wonder about whether the data is faulty. Secondly, we can see August has the highest total amount of precipitation recorded in the dataset.\n\n\n\n\n\n\nHere, we can see the CPI index has increased steadily since the 1940s, with a slower period of increase between 1960 and 1970, and a higher period of increase between 1970 and 1980, followed by a marked jump after 2020."
  },
  {
    "objectID": "Financial Time Series Models (ARCH GARCH).html",
    "href": "Financial Time Series Models (ARCH GARCH).html",
    "title": "Financial Time Series Models (ARCH GARCH)",
    "section": "",
    "text": "Introduction\n\nOn this page, I will look to see whether ARCH and GARCH models may be a good fit for the change in intraday range of popular stock market indices. While these models would usually be fitted on the returns of financial intstruments, the day over day change in intraday percent range also exhibits volatility clustering, and intraday range is the exact subject of my project. This means that modelling these time series successfully would be complementary with the other models I have tried to fit, and I am eager to investigate whether these methods will work well at answering my research questions. I will look to fit 3 models: 1 each for the daily range of SPY, QQQ, and IWM.\n\nStationarity and Volatility\n\nLoad packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(quantmod)\n\n\nRead in data\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\nPlotting the daily ranges:\nSPY\n\n\nCode\nplot(spyIn$spyRange)\n\n\n\n\n\nThe SPY data certainly appears to have clustered volatility, as there are large swings in close proximity to eachother. But it also has noticeable trend, with the intraday ranges being higher during the middle of the time series before dropping down again towards the end. To combat this, let’s look at the differenced series:\n\n\nCode\nplot(spyIn$spyRange %&gt;% diff())\n\n\n\n\n\nNow, we see a plot that looks very similar to the daily returns of a stock and promising for ARCH/GARCH modeling, which means it is stationary but has clustered volatility. To adjust for this, we can fit an ARIMA model on the SPY ranges, and then fit a GARCH model on the residuals, which will come from differenced data that is now stationary.\nRepeating the process for QQQ:\n\n\nCode\nplot(qqqIn$qqqRange)\n\n\n\n\n\nOnce again we see clear non-stationarity in the data, so let’s look at the differenced values:\n\n\nCode\nplot(qqqIn$qqqRange %&gt;% diff())\n\n\n\n\n\nSimilar to the SPY data, the time series is now stationary, although there is heteroskedasticity. So we might expect to model QQQ and SPY similarly, with an ARIMA + ARCH/GARCH model.\nLet’s check IWM:\n\n\nCode\nplot(iwmIn$iwmRange)\n\n\n\n\n\nIWM’s intraday range looks slightly more stationary than the other 2 time series, and also has more extreme changes in volatility over time. However, we can still see somewhat of a trend, such that a moving average would be obviously nonstationary, so once again we difference the series.\n\n\nCode\nplot(iwmIn$iwmRange %&gt;% diff())\n\n\n\n\n\nWhile the variance looked elevated in the non-differenced plot, it is actually less hesteroskedastic in the differenced plot. This suggests IWM may not have as much autocorrelation in the variance as the other two indices. An ARIMA model alone might suffice here.\n\nLook at past arima model for the data (fit here and copy to that tab later)\n\nNow, let’s gather ARIMA models for the intraday range. First we declare the helper function\n\n\nCode\narimaResults &lt;- function(data) {\n    \n    i=1\n    temp= data.frame()\n    ls=matrix(rep(NA,6*100),nrow=100) # roughly nrow = 3x4x2\n\n    for (p in 1:6)# p=1,2,3 : 3\n    {\n        for(q in 1:6)# q=1,2,3,4 :4\n        {\n            for(d in 1:2) {\n                \n                if(p-1+d - 1+q-1&lt;=8) #usual threshold\n                {\n                    \n                    model&lt;- Arima(data,order=c(p-1,d-1,q-1),include.drift=FALSE) \n                    ls[i,]= c(p-1,d-1,q-1,model$aic,model$bic,model$aicc)\n                    i=i+1\n                    #print(i)\n                    \n                }\n\n            }\n                \n        }\n    }\n\n    temp= as.data.frame(ls)\n    names(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n    return(temp[which.min(temp$AIC),])\n    #return(temp)\n}\n\n\nNow, let’s find good-fitting models starting with SPY:\n\n\nCode\narimaResults(spyIn$spyRange)\n\n\n   p d q       AIC       BIC      AICc\n43 3 0 3 -5139.956 -5103.662 -5139.744\n\n\nThe best model returned by the function is ARMA(3,0,3). It is surprising that the models which differenced the data didn’t perform better, but I manually reviewed those options and they all had worse AIC and BIC scores.\nFor QQQ:\n\n\nCode\narimaResults(qqqIn$qqqRange)\n\n\n   p d q      AIC       BIC      AICc\n15 1 0 1 -4814.81 -4796.663 -4814.751\n\n\nFor QQQ, the model with the lowest AIC score was ARMA(1,0,1). Once again I expected a differenced model to perform better, but I will continue with the residuals from this model.\nFor IWM:\n\n\nCode\narimaResults(iwmIn$iwmRange)\n\n\n   p d q       AIC       BIC      AICc\n56 4 0 4 -4857.373 -4812.006 -4857.049\n\n\nFitting the pattern of the best models for the financial instruments being of the form ARMA(N, 0, N), the model with the lowest AIC for IWM was ARMA(4,0,4).\n\nReviewing Residuals to Identify ARCH/GARCH Models\n\nNow, lets look at the residuals of all of these models, to identify clustering and see if we need to fit ARCH/GARCH models on the residuals.\nDeclaring the models\n\n\nCode\nspyARMA &lt;- arima(spyIn$spyRange, order = c(3,0,3))\nqqqARMA &lt;- arima(qqqIn$qqqRange, order = c(1,0,1))\niwmARMA &lt;- arima(iwmIn$iwmRange, order = c(4,0,4))\n\n\nSPY range residuals:\n\n\nCode\ncheckresiduals(spyARMA)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(3,0,3) with non-zero mean\nQ* = 5.2511, df = 4, p-value = 0.2625\n\nModel df: 6.   Total lags used: 10\n\n\nThe residuals show definite clustering around timestamps 260 and 450, which suggests a ARCH/GARCH model will be a good fit. Now lets look at ACF and PACF plots of the residuals and squared residuals:\n\n\nCode\nacf(spyARMA$residuals)\n\n\n\n\n\nCode\npacf(spyARMA$residuals)\n\n\n\n\n\nCode\nacf(spyARMA$residuals^2)\n\n\n\n\n\nCode\npacf(spyARMA$residuals^2)\n\n\n\n\n\nCode\n#install.packages('FinTS')\nlibrary(FinTS)\nArchTest(spyIn$spyRange)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  spyIn$spyRange\nChi-squared = 204.01, df = 12, p-value &lt; 2.2e-16\n\n\nCode\n#ArchTest(spyARMA$residuals)\n\n\nBased on the ACF and PACF plots, it seems like an ARCH model of (1) might be the best fit for SPY residual data. An ARCH test of the original range data confirms that there is an ARCH effect in the dataset.\nQQQ Residuals:\n\n\nCode\nacf(qqqARMA$residuals)\n\n\n\n\n\nCode\npacf(qqqARMA$residuals)\n\n\n\n\n\nCode\nacf(qqqARMA$residuals^2)\n\n\n\n\n\nCode\npacf(qqqARMA$residuals^2)\n\n\n\n\n\nCode\n#install.packages('FinTS')\nlibrary(FinTS)\nArchTest(qqqIn$qqqRange)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  qqqIn$qqqRange\nChi-squared = 173.7, df = 12, p-value &lt; 2.2e-16\n\n\nCode\nArchTest(qqqARMA$residuals)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  qqqARMA$residuals\nChi-squared = 17.618, df = 12, p-value = 0.1278\n\n\nThe QQQ residuals suggest GARCH values up to (1,3). The squared residuals suggest more similar to a ARCH(1) model. ARCH tests of the original range data confirm an ARCH effect.\nIWM Residuals:\n\n\nCode\nacf(iwmARMA$residuals)\n\n\n\n\n\nCode\npacf(iwmARMA$residuals)\n\n\n\n\n\nCode\nacf(iwmARMA$residuals^2)\n\n\n\n\n\nCode\npacf(iwmARMA$residuals^2)\n\n\n\n\n\nCode\n#install.packages('FinTS')\nlibrary(FinTS)\nArchTest(iwmIn$iwmRange)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  iwmIn$iwmRange\nChi-squared = 174.39, df = 12, p-value &lt; 2.2e-16\n\n\nCode\nArchTest(iwmARMA$residuals)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  iwmARMA$residuals\nChi-squared = 28.872, df = 12, p-value = 0.004116\n\n\nThe IWM residuals look similar to the spy residuals, but suggest an ARCH (1) model. The squared residuals suggest GARCH(1,1).\n\nFinding and evaluating ARCH and GARCH Fits\n\nLet’s start by declaring a GARCH helper function that checks possible values:\n\n\nCode\nlibrary(tseries)\ngarchResults &lt;- function(data) {\n    model &lt;- list() ## set counter\n    cc &lt;- 1\n    for (p in 1:7) {\n        for (q in 1:7) {\n            if(p + q &lt; 8) {\n                model[[cc]] &lt;- garch(data,order=c(q-1,p),trace=F)\n                cc &lt;- cc + 1\n            }\n        }\n    } \n\n    ## get AIC values for model evaluation\n    GARCH_AIC &lt;- sapply(model, AIC) ## model with lowest AIC is the best\n    #which(GARCH_AIC == min(GARCH_AIC))\n    ## [1] 24\n    model[[which(GARCH_AIC == min(GARCH_AIC))]]\n}\n\n\n\nSPY GARCH Model Fitting, Diagnostics, and Equation\n\nLet’s check our ARCH(1) model hypothesis for SPY:\n\n\nCode\nlibrary(vars)\nlibrary(fGarch)\n\nspyMod &lt;- garchResults(spyARMA$residuals)\nsummary(spyMod)\n\n\n\nCall:\ngarch(x = data, order = c(q - 1, p), trace = F)\n\nModel:\nGARCH(4,1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9251 -0.6944 -0.1773  0.4743  5.7716 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 1.295e-06   5.529e-07    2.342   0.0192 *  \na1 2.677e-01   4.417e-02    6.060 1.36e-09 ***\nb1 2.309e-01   9.399e-02    2.457   0.0140 *  \nb2 3.670e-09   1.336e-01    0.000   1.0000    \nb3 8.880e-02   1.380e-01    0.643   0.5199    \nb4 4.160e-01   9.804e-02    4.244 2.20e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 692.91, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.46517, df = 1, p-value = 0.4952\n\n\nCode\n#checkresiduals(spyARMA)\ncheckresiduals(spyMod)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 2.669, df = 10, p-value = 0.9882\n\nModel df: 0.   Total lags used: 10\n\n\nThe automatic function returns a surprise: The GARCH model with the lowest AIC is acutally GARCH(4,1). The diagnostics for this model are also promising, the 4th and 1st lag variance are highly significant (p &lt; 0.05), as is the 1st residual error. The Box-Ljung test is decidedly above the threshold at which we can reject the idea that there is no autocorrelation in the residuals (p = 0.495) which is another encouraging sign for the model fit. This suggests that the model has explained most of the signal in the data, with little correlation left in the residuals. It also shows that the results have improved from the ARMA(3,0,3) model alone, which had a Ljung-Box test result of 0.26, which suggests less correlation is left in the residuals after fitting a GARCH(4,1) model.\n\n\nCode\ngarchFit(formula = ~arma(3,3) + garch(4,1), data = spyIn$spyRange, trace = FALSE)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(3, 3) + garch(4, 1), data = spyIn$spyRange, \n    trace = FALSE) \n\nMean and Variance Equation:\n data ~ arma(3, 3) + garch(4, 1)\n&lt;environment: 0x00000000387fa8e0&gt;\n [data = spyIn$spyRange]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1          ar2          ar3          ma1          ma2  \n 2.4917e-05   9.7804e-01   5.9471e-01  -5.7502e-01  -6.6483e-01  -5.8857e-01  \n        ma3        omega       alpha1       alpha2       alpha3       alpha4  \n 2.9376e-01   3.2859e-07   5.3270e-02   1.0000e-08   1.0000e-08   1.0000e-08  \n      beta1  \n 9.3784e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      2.492e-05   3.759e-05    0.663 0.507405    \nar1     9.780e-01   1.474e-01    6.634 3.28e-11 ***\nar2     5.947e-01   1.858e-01    3.200 0.001374 ** \nar3    -5.750e-01   8.150e-02   -7.055 1.73e-12 ***\nma1    -6.648e-01   1.629e-01   -4.082 4.46e-05 ***\nma2    -5.886e-01   1.573e-01   -3.742 0.000182 ***\nma3     2.938e-01   8.403e-02    3.496 0.000473 ***\nomega   3.286e-07   2.829e-07    1.161 0.245454    \nalpha1  5.327e-02   4.160e-02    1.281 0.200369    \nalpha2  1.000e-08   4.400e-02    0.000 1.000000    \nalpha3  1.000e-08   1.060e-01    0.000 1.000000    \nalpha4  1.000e-08   8.994e-02    0.000 1.000000    \nbeta1   9.378e-01   2.959e-02   31.699  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2619.748    normalized:  3.796736 \n\nDescription:\n Fri Dec 01 22:44:15 2023 by user: corwi \n\n\nThe final evaluation of all the parameters together, however, suggests that the GARCH model should be (0,1). SO I will write the final correct formula as:\nFinal formula is: Rt = 0.000025 + 0.978 * R(t-1) + 0.595 * R(t-2) - 0.575 * R(t-3) - 0.665 * W(t-1) - 0.589 * W(t-2) + 0.294 * W(t-3) + At\nAt = Sigma(t) * Epsilon(t)\nSigma(t) ^ 2 = 0.00000033 + 0.94 * Sigma(t-1)^2\n\nQQQ GARCH Model Fitting, Diagnostics, and Equation\n\n\n\nCode\nqqqMod &lt;- garchResults(qqqARMA$residuals)\nsummary(qqqMod)\n\n\n\nCall:\ngarch(x = data, order = c(q - 1, p), trace = F)\n\nModel:\nGARCH(1,1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9569 -0.6967 -0.1981  0.4453  5.4864 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 3.515e-07   1.633e-07    2.152   0.0314 *  \na1 3.527e-02   5.331e-03    6.615 3.72e-11 ***\nb1 9.582e-01   6.218e-03  154.113  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 591.42, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.070318, df = 1, p-value = 0.7909\n\n\nCode\n#qqqMod\n\n#checkresiduals(qqqARMA)\n#checkresiduals(qqqMod)\n\n\nThe GARCH model with the lowest AIC for the QQQ residuals is GARCH(1,1), which is within the range of the (1,3) we suspected, but still a different value than we anticipated. All of the terms in this model a significant at the p&lt;0.05 level, both the residual error lag term and the lag variance term. In addition, the Ljung-Box test score improves from 0.1825 in the ARMA(1,0,1) model alone, to 0.51 in the GARCH model. This suggests the model has lowered the autocorrelation in the residuals.\n\n\nCode\ngarchFit(formula = ~arma(1,1) + garch(1,1), data = qqqIn$qqqRange, trace = FALSE)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(1, 1) + garch(1, 1), data = qqqIn$qqqRange, \n    trace = FALSE) \n\nMean and Variance Equation:\n data ~ arma(1, 1) + garch(1, 1)\n&lt;environment: 0x000000003ae13dd0&gt;\n [data = qqqIn$qqqRange]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1          ma1        omega       alpha1        beta1  \n 6.1100e-04   9.6203e-01  -7.4073e-01   3.5130e-07   3.5178e-02   9.5819e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      6.110e-04   2.413e-04    2.532   0.0113 *  \nar1     9.620e-01   1.488e-02   64.636  &lt; 2e-16 ***\nma1    -7.407e-01   3.685e-02  -20.099  &lt; 2e-16 ***\nomega   3.513e-07   2.017e-07    1.741   0.0816 .  \nalpha1  3.518e-02   7.556e-03    4.656 3.23e-06 ***\nbeta1   9.582e-01   7.993e-03  119.873  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2441.621    normalized:  3.538581 \n\nDescription:\n Fri Dec 01 22:44:15 2023 by user: corwi \n\n\nFinal formula is: Rt = 0.00061 + 0.962 * R(t-1) - 0.741 * W(t-1) + At\nAt = Sigma(t) * Epsilon(t)\nSigma(t) ^ 2 = 0.00000035 + .034 * a(t-1)^2 + 0.96 * Sigma(t-1)^2\n\nIWM GARCH Model Fitting, Diagnostics, and Equation\n\n\n\nCode\niwmMod &lt;- garchResults(iwmARMA$residuals)\nsummary(iwmMod)\n\n\n\nCall:\ngarch(x = data, order = c(q - 1, p), trace = F)\n\nModel:\nGARCH(1,1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7824 -0.7249 -0.1962  0.4915  5.1795 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 2.153e-06   8.987e-07    2.395 0.016602 *  \na1 6.089e-02   1.693e-02    3.597 0.000322 ***\nb1 8.969e-01   2.903e-02   30.900  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 338.27, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 8.6335e-05, df = 1, p-value = 0.9926\n\n\nCode\n# checkresiduals(iwmARMA)\n\ncheckresiduals(iwmMod)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 4.2445, df = 10, p-value = 0.9356\n\nModel df: 0.   Total lags used: 10\n\n\nFor IWM, the squared residuals showed a GARCH(1,1) fit, and this is also what the function returned as the model with the lowest AIC value. All of the terms were significant at the p &lt; 0.01 threshold, and the Ljung-Box test returns 0.498, improving from 0.088 in the ARMA(4,0,4) model alone. This suggests the GARCH fit has done a very good job at removing autocorrelation from the resdiuals.\n\n\nCode\ngarchFit(formula = ~arma(4,2) + garch(1,1), data = iwmIn$iwmRange, trace = FALSE)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(4, 2) + garch(1, 1), data = iwmIn$iwmRange, \n    trace = FALSE) \n\nMean and Variance Equation:\n data ~ arma(4, 2) + garch(1, 1)\n&lt;environment: 0x000000002da012f0&gt;\n [data = iwmIn$iwmRange]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu          ar1          ar2          ar3          ar4          ma1  \n 3.4855e-04   6.6410e-01   5.6820e-01  -1.7672e-01  -7.5180e-02  -3.9551e-01  \n        ma2        omega       alpha1        beta1  \n-4.7133e-01   1.7350e-06   5.8615e-02   9.0767e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      3.485e-04   3.509e-04    0.993  0.32058    \nar1     6.641e-01   1.613e-01    4.118 3.83e-05 ***\nar2     5.682e-01   2.229e-01    2.549  0.01080 *  \nar3    -1.767e-01   5.433e-02   -3.253  0.00114 ** \nar4    -7.518e-02   6.747e-02   -1.114  0.26516    \nma1    -3.955e-01   1.592e-01   -2.484  0.01299 *  \nma2    -4.713e-01   1.791e-01   -2.631  0.00851 ** \nomega   1.735e-06   1.000e-06    1.734  0.08287 .  \nalpha1  5.861e-02   1.936e-02    3.027  0.00247 ** \nbeta1   9.077e-01   3.244e-02   27.979  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2457.18    normalized:  3.56113 \n\nDescription:\n Fri Dec 01 22:44:16 2023 by user: corwi \n\n\nWhen pulling all of the variables together, the final model diagnostic suggests that the moving average terms 3 and 4 are not significant. As such, I will write the equation for the final selected model of ARMA(4,2) + GARCH(1,1):\nFinal formula is: Rt = 0.00035 + 0.664 * R(t-1) + 0.568 * R(t-2) - 0.177 * R(t-3) - 0.075 * R(t-4) - 0.396 * W(t-1) - 0.471 * W(t-2) + At\nAt = Sigma(t) * Epsilon(t)\nSigma(t) ^ 2 = 0.00000017 + .0586 * a(t-1)^2 + 0.908 * Sigma(t-1)^2"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In this project I will study the daily volatility of major US stock indices over time, specifically the Standard and Poors 500, the Russel 2000, and the NASDAQ 100. By volatility, I specifically mean the intraday range in prices that these indices take on the open market. I will both review these intraday ranges as univariate time series, and also attempt to shed light on variables that impact volatility by incorporating external factors. I will look at warnings of extreme weather events, investor confidence metrics, future bond rate expectations, and work stoppage data on strikes and striking workers. Using the statistical models we have learned in class, I hope to investigate the relationships between these factors and stock market volatility."
  },
  {
    "objectID": "Introduction.html#topic-explanation",
    "href": "Introduction.html#topic-explanation",
    "title": "Introduction",
    "section": "",
    "text": "In this project I will study the daily volatility of major US stock indices over time, specifically the Standard and Poors 500, the Russel 2000, and the NASDAQ 100. By volatility, I specifically mean the intraday range in prices that these indices take on the open market. I will both review these intraday ranges as univariate time series, and also attempt to shed light on variables that impact volatility by incorporating external factors. I will look at warnings of extreme weather events, investor confidence metrics, future bond rate expectations, and work stoppage data on strikes and striking workers. Using the statistical models we have learned in class, I hope to investigate the relationships between these factors and stock market volatility."
  },
  {
    "objectID": "Introduction.html#the-big-picture",
    "href": "Introduction.html#the-big-picture",
    "title": "Introduction",
    "section": "The Big Picture:",
    "text": "The Big Picture:\nFinancial markets are directly important to the well-being of everyday Americans. While the stock market is often seen as exclusive to a small circle of elite investors, in reality millions of Americans have exposure to the markets through retirement funds, which are made up of many different kinds of investments. If we can become better at predicting the course of financial markets, particularly in relation to external forces like climate change, it could promise to reduce volatility and inefficiency in the market. In turn, this would help everyday people build up their savings to enjoy a better quality of life, retire sooner, and have more financial security.\nFurther, understanding the movements of financial markets is important in controlling sentiment amongst investors. In cases like the onset of the COVID-19 pandemic, stock market crashes could snowball into businesses cutting more jobs than needed, which could directly impact peoples’ livelihoods. By looking at extreme events such as major strikes and severe weather warnings, I hope to elucidate the effect of these changes.\n\n\n\nBig Picture"
  },
  {
    "objectID": "Introduction.html#analytical-angles",
    "href": "Introduction.html#analytical-angles",
    "title": "Introduction",
    "section": "Analytical Angles:",
    "text": "Analytical Angles:\nThe price of stock market indices are considered to be noisy and near-impossible to predict. Related values such as intraday range are also expected to be difficult to predict with any accuracy (given that any such prediction could yield profit and should be closed under the efficient market hypothesis).\nFirst, looking at intraday volatility as univariate time series. Conceptually, this means studying the securities in a vacumn, and assuming no external information will determine their future while. While this approach might seem naive, it actually mirrors how some financial institutions might manage risk, where you look to a stocks past volatility to determine future volatility.\nSecond, understanding intraday volatility as a series of values which are reflective of individuals actions and preferences. In this case, the individuals in question are investors, and we will look at investor sentiment and see how it might affect prices in financial markets.\nThird, viewing intraday volatility in the context of external stimuli. From this analytical perspective, the stock market is a closed system, which is disrupted by external forces from time-to-time that cause unexpected behaviour. Studying the impact of extreme weather events and work stoppages relate to this point.\nFourth, understanding intraday volatility as an output variable of larger expectations in the financial system. By looking at changes in future bond prices (i.e. predictions for future bond rates), we can understand how indices react to shifting financial conditions overall."
  },
  {
    "objectID": "Introduction.html#literature-review",
    "href": "Introduction.html#literature-review",
    "title": "Introduction",
    "section": "Literature Review:",
    "text": "Literature Review:\nMany of the factors this project seeks to investigate have previously been reviewed by academics. For instance, the consequence of investor sentiment on the stock market has been widely studied, in terms of how it impacts stock market crises (Zouaoui, Nouyrigat, and Beer 2011), trading volume (So and Lei 2015), and especially returns (Smales 2017). Further, authors have previously found an impact of climate events, such as hurricanes (Liu, Ferreira, and Karali 2021) on stock prices of particular companies. The fact that these issues have been studied previously is encouraging, because in each case the authors chose a specific group of time and company limited stock market data. This project might extend their results to other stock market datasets, and might hope to incorporate these relationships into new statistical models."
  },
  {
    "objectID": "Introduction.html#guiding-questions",
    "href": "Introduction.html#guiding-questions",
    "title": "Introduction",
    "section": "Guiding Questions:",
    "text": "Guiding Questions:\n\nIs the intraday volatility of major stock indices truly random, or does it contain patterns that can be modelled?\nDo the major US stock indices differ greatly in the structure of their volatility over time?\nHow do markets react in response to chaining expectations about macroeconomic conditions?\nHow do markets incorporate extreme weather event warnings into their pricing of major indices, and how does this affect volatility?\nHow do markets react in response to major strikes and labor stoppages, and does this differ based on the size of the strike or the types of companies in question?\nHow does investor confidence impact markets?\nDo the major US stock indices differ in their response to external stimuli, and are some more sensitive to external volitility?\nAre the patterns exhibited by intraday volatility in major indices best-suited to traditional statistical models or deep learning? If deep learning models fit better, then what does that say about patterns in volatility?\nHave the relationships between the intraday ranges in the prices of securities and external factors changed over time?\nHow well can we explain variations in intraday volatility, when synthesizing as much external data and models as possible?"
  },
  {
    "objectID": "Data Sources.html#sp-500-nasdaq-100-and-russel-2000-daily-data-yahoo-finance",
    "href": "Data Sources.html#sp-500-nasdaq-100-and-russel-2000-daily-data-yahoo-finance",
    "title": "Data Sources",
    "section": "",
    "text": "Source for SPY\nSource for IWM\nSource for QQQ\nFinancial data on the prices of different stocks are readily available in both R and Python through various packages, and amongst these the Yahoo Finance package is a particularly popular option. To approximate the daily range in value of major indices, I chose to look at popular low-cost ETFs which attempt to track the value of stocks contained in these indices. For the S&P 500, this is SPY, run by State Street, for the Russel 2000 this is iShare’s IWM, and for the NASDAQ 100 it is the ever-popular Invesco QQQ fund. The data I haave gathered for these tickers includes the open and close of stock prices for each day, as well as the highest and lowest price recorded for the day. These indices are widely used and will be a relevant outcome variable for the project to study. The main output value thaht will be calculated is the high-low range of the day."
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html#planning-models",
    "href": "ARIMAX SARIMAX VAR.html#planning-models",
    "title": "ARIMAX SARIMAX VAR",
    "section": "",
    "text": "We have the following independent variables: 1. Interest Rate Expectation Changes - 3 Months 2. Interest Rate Expectation Changes - 6 Months 3. Interest Rate Expectation Changes - 1 Year 4. Extreme Weather Events - Daily Event Number 5. Extreme Weather Events - Daily Property Damage 6. Extreme Weather Events - Daily Casualties 7. Extreme Weather Events - Hurricanes 8. Expected Volatility (VIX) - Value 9. Expected Volatility (VIX) - Daily Change 10. Work Stoppages - Daily Striking Worker Total 11. Work Stoppages - Daily New Strike Beggining 12. Work Stoppages - Daily New Workers Striking\n\n\nFirst we need to create all 12 predictors, then we can combine them to estimate the models as needed.\n\n\nCode\nlibrary(knitr)\nlibrary(tidyverse)\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.4.4     v purrr   1.0.1\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(quantmod)\n\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nLoading required package: TTR\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nCode\nlibrary(forecast)\nlibrary(tseries)\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\nlibrary(reticulate)\n\n\nLet’s quickly retrieve the daily stock price ranges for the indices:\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\nNow, let’s gather the VIX data, since it is also treated like a stock price, and should be available from the same package\n\n\nCode\nvixIn &lt;- quantmod::getSymbols(\"^VIX\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nWarning: ^VIX contains missing values. Some functions will not work if objects\ncontain missing values in the middle of the series. Consider using na.omit(),\nna.approx(), na.fill(), etc to remove or replace them.\n\n\nCode\nvixIn$dailyChange &lt;- vixIn$VIX.Close - lag(vixIn$VIX.Close)\n# 8 is vixIn$VIX.Close\n# 9 is vixIn$dailyChange\n\n#vixIn &lt;- vixIn %&gt;% \n#    mutate(date = ymd(index(vixIn)))\nhead(vixIn)\n\n\n           VIX.Open VIX.High VIX.Low VIX.Close VIX.Volume VIX.Adjusted\n2021-01-04    23.04    29.19   22.56     26.97          0        26.97\n2021-01-05    26.94    28.60   24.80     25.34          0        25.34\n2021-01-06    25.48    26.77   22.14     25.07          0        25.07\n2021-01-07    23.67    23.91   22.25     22.37          0        22.37\n2021-01-08    22.43    23.34   21.42     21.56          0        21.56\n2021-01-11    23.31    24.81   23.23     24.08          0        24.08\n           dailyChange\n2021-01-04          NA\n2021-01-05  -1.6299992\n2021-01-06  -0.2700005\n2021-01-07  -2.6999989\n2021-01-08  -0.8100014\n2021-01-11   2.5200005\n\n\nWe have bond yields stored in a CSV, but let’s calculate daily changes:\n\n\nCode\nyieldCurve &lt;- read.csv('data/treasuries.csv')\n\nyieldCurve$mo3delta &lt;- yieldCurve$X3.Mo - lag(yieldCurve$X3.Mo)\nyieldCurve$mo6delta &lt;- yieldCurve$X6.Mo - lag(yieldCurve$X6.Mo)\nyieldCurve$mo12delta &lt;- yieldCurve$X1.Yr - lag(yieldCurve$X1.Yr) \n\nyieldCurve$Date &lt;- mdy(yieldCurve$Date)\n\n\nWorth noting: Both the treasury data and the VIX data only start on January 4th.\nNext up, let’s prepare the weather event data:\n\n\nCode\nweather_data &lt;- read.csv('data/storms_clean.csv')\n\n\nweather_data$month &lt;- weather_data$BEGIN_YEARMONTH %% 100\n\nweather_data &lt;- weather_data %&gt;%\n    mutate(realdate = make_date(YEAR, month, BEGIN_DAY)) %&gt;%\n    mutate(DAMAGE_PROPERTY =  str_replace(DAMAGE_PROPERTY, \"K\", \"\") )  %&gt;%\n    mutate(DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)) %&gt;%\n    mutate(DAMAGE_PROPERTY = replace_na(DAMAGE_PROPERTY, 0))\n\n\nWarning: There was 1 warning in `mutate()`.\ni In argument: `DAMAGE_PROPERTY = as.numeric(DAMAGE_PROPERTY)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nCode\n# Daily Event Number\ndaily_events &lt;- weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(events = length(EPISODE_ID))\n\n# Daily Property Damage\ndaily_property &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(pdam = sum(DAMAGE_PROPERTY))\n\n# Daily Daily Casualties\ndaily_casualties &lt;-  weather_data %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(casualties = sum(INJURIES_DIRECT) + sum(INJURIES_INDIRECT) + sum(DEATHS_DIRECT) + sum(DEATHS_INDIRECT))\n\n# Daily Hurricanes\ndaily_hurricanes &lt;- weather_data %&gt;%\n    filter(EVENT_TYPE == \"Hurricane\") %&gt;%\n    group_by(realdate) %&gt;%\n    summarize(hurricaneWarnings = length(EPISODE_ID))\n\n\n# Daily joined data\n\nweather_merged &lt;- full_join(daily_events, daily_property, by = \"realdate\")\nweather_merged &lt;- full_join(weather_merged, daily_casualties, by = 'realdate')\nweather_merged &lt;- full_join(weather_merged, daily_hurricanes, by = 'realdate')\n\nhead(weather_merged)\n\n\n# A tibble: 6 x 5\n  realdate   events  pdam casualties hurricaneWarnings\n  &lt;date&gt;      &lt;int&gt; &lt;dbl&gt;      &lt;int&gt;             &lt;int&gt;\n1 2021-01-01    643  918           1                NA\n2 2021-01-02     75  189.          0                NA\n3 2021-01-03     80   63           3                NA\n4 2021-01-04     44   20           0                NA\n5 2021-01-05     18    0           2                NA\n6 2021-01-06     27  850           2                NA\n\n\nFinally, lets get the striking worker data ready:\n\n\nCode\nstrike_data &lt;- read.csv('data/strikes.csv')\n\n# clean date format\nstrike_data$start = mdy(strike_data$Work.stoppage.beginning.date)\nstrike_data$end = mdy(strike_data$Work.stoppage.ending.date)\nstrike_data$workers = as.numeric(str_replace(strike_data$Number.of.workers.2., \",\", \"\" ))\n\n\nhead(strike_data)\n\n\n            ï..Organizations.involved Industry.code.1.\n1          Hunts Point Produce Market           445200\n2                 Columbia University            61131\n3 Allegheny Technologies Incorporated           332710\n4              Warrior Met Coal, Inc.           212112\n5                 New York University            61131\n6                         Cook County           622110\n  Work.stoppage.beginning.date Work.stoppage.ending.date Number.of.workers.2.\n1                    1/17/2021                 1/23/2021                1,400\n2                    3/15/2021                  1/7/2022                3,000\n3                    3/30/2021                 7/13/2021                1,300\n4                     4/1/2021                 3/31/2022                1,100\n5                    4/26/2021                 5/15/2021                2,200\n6                    6/25/2021                 7/12/2021                2,000\n  Days.idle..cumulative.for.this.work.stoppage.3.      start        end workers\n1                                           5,600 2021-01-17 2021-01-23    1400\n2                                         177,000 2021-03-15 2022-01-07    3000\n3                                          94,900 2021-03-30 2021-07-13    1300\n4                                         273,900 2021-04-01 2022-03-31    1100\n5                                          33,000 2021-04-26 2021-05-15    2200\n6                                          22,000 2021-06-25 2021-07-12    2000\n\n\nCode\ntarget_dates &lt;- ymd(index(spyIn))\n\ndaily_workers &lt;- vector(mode = \"numeric\", length = length(target_dates))\n\nfor(i in seq_along(target_dates)) {\n    tempDat &lt;- strike_data %&gt;%\n        filter(start &lt;= target_dates[i]) %&gt;%\n        filter(end &gt;= target_dates[i])\n\n    daily_workers[i] = sum(tempDat$workers)\n}\n\nworkerDF &lt;- data.frame('workers' = daily_workers, 'date' = target_dates)\nplot(daily_workers, type = 'l')\n\n\n\n\n\nNow, we can combined all of these datasets into one dataframe, joining on the date columns\n\n\nCode\n# Convert TS objects to df, and fix the date column\nvixDF &lt;- data.frame(vixIn)\nvixDF$date &lt;- ymd(index(vixIn))\nspyDF &lt;- data.frame(spyIn)\nspyDF$date &lt;- ymd(index(spyIn))\nqqqDF &lt;- data.frame(qqqIn)\nqqqDF$date &lt;- ymd(index(qqqIn))\niwmDF &lt;- data.frame(iwmIn)\niwmDF$date &lt;- ymd(index(iwmIn))\n\n# Join symbols together\ntickers &lt;- left_join(spyDF, vixDF, by = 'date')\ntickers &lt;- left_join(tickers, qqqDF, by = 'date')\ntickers &lt;- left_join(tickers, iwmDF, by = 'date')\n\n# Join weather data\ncombinedData &lt;- left_join(tickers, weather_merged, by = c(\"date\" = \"realdate\"))\n\n# Join Bond Yields\ncombinedData &lt;- left_join(combinedData, yieldCurve, by = c(\"date\" = \"Date\"))\n\n# Join Labor Data\ncombinedData &lt;- left_join(combinedData, workerDF, by = 'date')\n\nhead(combinedData)\n\n\n  SPY.Open SPY.High SPY.Low SPY.Close SPY.Volume SPY.Adjusted    spyRange\n1   375.31   375.45  364.82    368.79  110210800     354.1974 0.028323266\n2   368.10   372.50  368.05    371.33   66426200     356.6369 0.012089139\n3   369.71   376.98  369.12    373.55  107997700     358.7691 0.021259950\n4   376.10   379.90  375.91    379.10   68766800     364.0994 0.010608854\n5   380.59   381.49  377.10    381.26   71677200     366.1740 0.011534681\n6   377.85   380.58  377.72    378.69   51034700     363.7057 0.007569102\n        date VIX.Open VIX.High VIX.Low VIX.Close VIX.Volume VIX.Adjusted\n1 2021-01-04    23.04    29.19   22.56     26.97          0        26.97\n2 2021-01-05    26.94    28.60   24.80     25.34          0        25.34\n3 2021-01-06    25.48    26.77   22.14     25.07          0        25.07\n4 2021-01-07    23.67    23.91   22.25     22.37          0        22.37\n5 2021-01-08    22.43    23.34   21.42     21.56          0        21.56\n6 2021-01-11    23.31    24.81   23.23     24.08          0        24.08\n  dailyChange QQQ.Open QQQ.High QQQ.Low QQQ.Close QQQ.Volume QQQ.Adjusted\n1          NA   315.11   315.29  305.18    309.31   45305900     304.2443\n2  -1.6299992   308.29   312.14  308.29    311.86   29323400     306.7526\n3  -0.2700005   307.00   311.88  305.98    307.54   52809600     302.5033\n4  -2.6999989   310.28   315.84  310.25    314.98   30394800     309.8215\n5  -0.8100014   317.34   319.39  315.08    319.03   33955800     313.8052\n6   2.5200005   315.98   317.19  313.75    314.42   32746400     309.2707\n    qqqRange IWM.Open IWM.High IWM.Low IWM.Close IWM.Volume IWM.Adjusted\n1 0.03208409   197.54   197.89  190.94    193.50   33664200     186.8583\n2 0.01248826   193.09   197.62  193.07    196.49   27442900     189.7457\n3 0.01921822   199.48   206.78  199.16    204.53   52952200     197.5097\n4 0.01801597   205.71   208.52  205.70    208.17   24031400     201.0247\n5 0.01358174   209.32   209.77  204.66    207.72   29017000     200.5902\n6 0.01088677   205.09   208.12  204.83    207.54   20945100     200.4164\n    iwmRange events pdam casualties hurricaneWarnings X1.Mo X2.Mo X3.Mo X4.Mo\n1 0.03518273     44   20          0                NA  0.09  0.09  0.09    NA\n2 0.02356408     18    0          2                NA  0.08  0.09  0.09    NA\n3 0.03819929     27  850          2                NA  0.09  0.09  0.09    NA\n4 0.01370865     74    0          0                NA  0.09  0.09  0.09    NA\n5 0.02441239     29   20          2                NA  0.08  0.08  0.08    NA\n6 0.01604171     68    0          0                NA  0.09  0.08  0.08    NA\n  X6.Mo X1.Yr X2.Yr X3.Yr X5.Yr X7.Yr X10.Yr X20.Yr X30.Yr mo3delta mo6delta\n1  0.09  0.10  0.11  0.16  0.36  0.64   0.93   1.46   1.66     0.00     0.00\n2  0.09  0.10  0.13  0.17  0.38  0.66   0.96   1.49   1.70     0.00     0.00\n3  0.09  0.11  0.14  0.20  0.43  0.74   1.04   1.60   1.81     0.00     0.00\n4  0.09  0.11  0.14  0.22  0.46  0.78   1.08   1.64   1.85     0.01     0.00\n5  0.09  0.10  0.14  0.24  0.49  0.81   1.13   1.67   1.87     0.00    -0.01\n6  0.10  0.10  0.14  0.22  0.50  0.84   1.15   1.68   1.88    -0.01     0.01\n  mo12delta workers\n1      0.00       0\n2     -0.01       0\n3      0.00       0\n4      0.01       0\n5      0.00       0\n6     -0.01       0\n\n\n\n\n\nWe will combine these 12 predictors into 5 models, for SPY, QQQ, and IWM intraday volatility:\n\n\nThis model is selected based on the literature review, which suggested that weather events and investor expecations could affect stock prices. This is the “kitchen sink” model, where I am throwing in variables from all data sources. However, looking at the variables individually, such as daily property damage vs. SPY daily price range, we don’t nessecarily see clear correlation (see plot below which resembles white noise). But I am interested to see how these variables are related when taking many different contextual factors into account in the same model.\n\n\nCode\nggplot(combinedData, aes(x = log(spyRange), y = log(pdam)) ) + geom_point() + labs(title = \"3 Month Interest Rate Changes vs. VIX Change \", x = \"Log SPY Daily Spread\", y = \"Log Property Damage From Storms\")\n\n\nWarning: Removed 63 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nThis model is based on a belief that there is an interrelationship between VIX prices and bond yields. This is because both would increase and decrease based on investor expectations for macroeconomic performance in upcoming months. If investors feel the economy will perform poorly, then this might predict bond yields lowering, as well as increased volatility which would be reflected by increases in the VIX. We also see a weak linear correlation in these daily values, as pictured in the plot below.\n\n\nCode\nggplot(combinedData, aes(x = mo3delta, y = dailyChange ) ) + geom_point() + labs(title = \"3 Month Interest Rate Changes vs. VIX Change \", x = \"Change in 3-Month Interest Rates\", y = \"Change in VIX Price\")\n\n\nWarning: Removed 30 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nThis model is all about exogenous shocks. New strikes beggining and hurricane warnings are infrequent but extreme events, which have been grouped together with short-term interest rates (3 month window) to try and capture extreme-but-short-termm influences on volatility.\n\n\n\n\n\n\nModels 4 and 5 follow the same logic as model 1, being selected based off of the literature review, but looking at QQQ as oppposed to SPY to see whether large-cap tech companies are more likely to be affected by this kind of volatility."
  },
  {
    "objectID": "Data Sources.html#vix-data-yahoo-finance",
    "href": "Data Sources.html#vix-data-yahoo-finance",
    "title": "Data Sources",
    "section": "VIX Data, Yahoo Finance",
    "text": "VIX Data, Yahoo Finance\nhttps://finance.yahoo.com/quote/%5EVIX/history?p=%255EVIX\nThe Chicago Board of Exchange (CBOE) VIX index is a widely-used tool to measure investor sentiment. The indicator itself represents the degree of volatility perceived by investors in the next month. It ranges from near zero up to about 60 in recent years, with scores at different intervals representing different levels of perceived volatility."
  },
  {
    "objectID": "Data Sources.html#work-stoppages",
    "href": "Data Sources.html#work-stoppages",
    "title": "Data Sources",
    "section": "Work Stoppages",
    "text": "Work Stoppages\nThis dataset contains all of the major strikes recorded by the bureau of labor statistics, such that I will be able to use the number of striking workers on any given day as a variable for analysis.\n\nSources\n\nLabor Deparment Data"
  },
  {
    "objectID": "Data Sources.html#daily-yield-curve-next-3-months",
    "href": "Data Sources.html#daily-yield-curve-next-3-months",
    "title": "Data Sources",
    "section": "Daily Yield Curve Next 3 Months",
    "text": "Daily Yield Curve Next 3 Months\nWhile I originally considered the actual federal funds rate, the values did not change much day to day. Instead I would like to use the expectation of yield rates, measured through the pricing of bonds in the yield curve over the next few months. The data includes the prices of treasury bills 1, 2, and 3 months out on each date.\n\nSources\n\nTreasury Department Data"
  },
  {
    "objectID": "Data Sources.html#storm-events-by-day",
    "href": "Data Sources.html#storm-events-by-day",
    "title": "Data Sources",
    "section": "Storm Events by Day",
    "text": "Storm Events by Day\nThe National Oceanic and Atmospheric Administration (NOAA) maintains records on each storm warning they send out for the entire country. This dataset includes many types of storm warnings, such as for hurricanes, blizzards, flash floods, and tornadoes. Importantly, the dataset includes the day in which each weather event took place, as well as the property damage and bodily injury caused by the weather event.\n\nNOAA Data"
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html#model-selection",
    "href": "ARIMAX SARIMAX VAR.html#model-selection",
    "title": "ARIMAX SARIMAX VAR",
    "section": "Model Selection",
    "text": "Model Selection\nIn this section, I will begin by identifying the candidate model structures for each of the 5 overarching models outlined above. I will identify candidate models through auto.arima for ARIMAX models, plus hand-selected values. For VAR models, I will identify 2 candidates for each overall model with the autoVAR function.\n\nModel Selection for ARIMAX Models\n\nModel 1: SPY ~ Interest Rate 1-Year + Daily Weather Property Damage + Daily VIX Change + Daily Striking Worker Total\n\n\nCode\nxMatrix = combinedData[,c('mo12delta', 'pdam', 'dailyChange', 'workers')]\nxMatrix[is.na(xMatrix)] &lt;- 0\nxMatrix = scale(xMatrix)\nxMatrix = as.matrix(xMatrix)\n\n#xMatrix\n\nmod1candidate1 = auto.arima(scale(combinedData$spyRange), xreg = xMatrix, trace = TRUE)\n\n\n\n Fitting models using approximations to speed things up...\n\n Regression with ARIMA(2,1,2) errors : 1599.925\n Regression with ARIMA(0,1,0) errors : 1880.267\n Regression with ARIMA(1,1,0) errors : 1699.179\n Regression with ARIMA(0,1,1) errors : 1616.148\n Regression with ARIMA(0,1,0) errors : 1878.238\n Regression with ARIMA(1,1,2) errors : 1604.247\n Regression with ARIMA(2,1,1) errors : 1604.271\n Regression with ARIMA(3,1,2) errors : 1599.17\n Regression with ARIMA(3,1,1) errors : 1597.175\n Regression with ARIMA(3,1,0) errors : 1626.681\n Regression with ARIMA(4,1,1) errors : 1591.027\n Regression with ARIMA(4,1,0) errors : 1623.139\n Regression with ARIMA(5,1,1) errors : 1597.41\n Regression with ARIMA(4,1,2) errors : 1593.09\n Regression with ARIMA(5,1,0) errors : 1617.939\n Regression with ARIMA(5,1,2) errors : Inf\n Regression with ARIMA(4,1,1) errors : 1588.981\n Regression with ARIMA(3,1,1) errors : 1595.121\n Regression with ARIMA(4,1,0) errors : 1621.08\n Regression with ARIMA(5,1,1) errors : 1595.37\n Regression with ARIMA(4,1,2) errors : 1591.037\n Regression with ARIMA(3,1,0) errors : 1624.63\n Regression with ARIMA(3,1,2) errors : 1597.106\n Regression with ARIMA(5,1,0) errors : 1615.874\n Regression with ARIMA(5,1,2) errors : Inf\n\n Now re-fitting the best model(s) without approximations...\n\n Regression with ARIMA(4,1,1) errors : 1592.116\n\n Best model: Regression with ARIMA(4,1,1) errors \n\n\nCode\ncheckresiduals(mod1candidate1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(4,1,1) errors\nQ* = 3.619, df = 5, p-value = 0.6055\n\nModel df: 5.   Total lags used: 10\n\n\nAuto ARIMA picks out the model (4,1,1) for the standardized data. The residual diagnostic plots look good, with the residuals normally distributed.\n\n\nCode\nxMatrix = as.data.frame(xMatrix)\n\n# Lets examine the residuals directly to identify \nmod1candidate2 &lt;- lm(scale(combinedData$spyRange) ~ mo12delta + pdam + dailyChange + workers, data = xMatrix )\nsummary(mod1candidate2)\n\n\n\nCall:\nlm(formula = scale(combinedData$spyRange) ~ mo12delta + pdam + \n    dailyChange + workers, data = xMatrix)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5121 -0.7147 -0.2584  0.4739  5.6401 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.420e-16  3.766e-02   0.000 1.000000    \nmo12delta    1.479e-02  3.778e-02   0.392 0.695544    \npdam        -2.214e-02  3.788e-02  -0.584 0.559186    \ndailyChange  1.046e-01  3.772e-02   2.772 0.005731 ** \nworkers     -1.272e-01  3.791e-02  -3.355 0.000838 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9894 on 685 degrees of freedom\nMultiple R-squared:  0.02684,   Adjusted R-squared:  0.02116 \nF-statistic: 4.723 on 4 and 685 DF,  p-value: 0.0009156\n\n\nCode\nresid1 &lt;- mod1candidate2$residuals\npacf(resid1)\n\n\n\n\n\nCode\nacf(resid1)\n\n\n\n\n\nBased on the PACF and ACF of the residuals from the regression, it seems we should definitely difference the series, as we have many significant lag terms in the ACF. On the PACF, we can see 4 terms clearly signfificant. Based on these charts, I might try the model (4,1,0). I will try up through (4,2,2) and look for the lowest aic.\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \ntemp=c()\nd=1\nD=1\ns=12\n \ni=1\ntemp= data.frame()\nls=matrix(rep(NA,9*378),nrow=378)\n \n \nfor (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\nSARIMA.c(p1=1,p2=5,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=resid1) %&gt;% filter(!is.na(p))\n\n\n   p d q P D Q      AIC      BIC     AICc\n1  0 0 0 0 1 0 1942.363 1951.437 1942.381\n2  0 1 0 0 1 0 1881.332 1885.868 1881.338\n3  0 0 0 0 1 1 1942.363 1951.437 1942.381\n4  0 1 0 0 1 1 1881.332 1885.868 1881.338\n5  0 0 0 1 1 0 1942.363 1951.437 1942.381\n6  0 1 0 1 1 0 1881.332 1885.868 1881.338\n7  0 0 0 1 1 1 1942.363 1951.437 1942.381\n8  0 1 0 1 1 1 1881.332 1885.868 1881.338\n9  0 0 0 2 1 0 1942.363 1951.437 1942.381\n10 0 1 0 2 1 0 1881.332 1885.868 1881.338\n11 0 0 0 2 1 1 1942.363 1951.437 1942.381\n12 0 0 1 0 1 0 1809.358 1822.969 1809.393\n13 0 1 1 0 1 0 1611.588 1620.658 1611.605\n14 0 0 1 0 1 1 1809.358 1822.969 1809.393\n15 0 1 1 0 1 1 1611.588 1620.658 1611.605\n16 0 0 1 1 1 0 1809.358 1822.969 1809.393\n17 0 1 1 1 1 0 1611.588 1620.658 1611.605\n18 0 0 1 1 1 1 1809.358 1822.969 1809.393\n19 0 0 1 2 1 0 1809.358 1822.969 1809.393\n20 0 0 2 0 1 0 1728.571 1746.718 1728.629\n21 0 1 2 0 1 0 1611.137 1624.743 1611.172\n22 0 0 2 0 1 1 1728.571 1746.718 1728.629\n23 0 0 2 1 1 0 1728.571 1746.718 1728.629\n24 0 0 3 0 1 0 1713.789 1736.472 1713.877\n25 1 0 0 0 1 0 1708.277 1721.887 1708.312\n26 1 1 0 0 1 0 1713.364 1722.434 1713.381\n27 1 0 0 0 1 1 1708.277 1721.887 1708.312\n28 1 1 0 0 1 1 1713.364 1722.434 1713.381\n29 1 0 0 1 1 0 1708.277 1721.887 1708.312\n30 1 1 0 1 1 0 1713.364 1722.434 1713.381\n31 1 0 0 1 1 1 1708.277 1721.887 1708.312\n32 1 0 0 2 1 0 1708.277 1721.887 1708.312\n33 1 0 1 0 1 0 1601.649 1619.796 1601.707\n34 1 1 1 0 1 0 1610.324 1623.930 1610.359\n35 1 0 1 0 1 1 1601.649 1619.796 1601.707\n36 1 0 1 1 1 0 1601.649 1619.796 1601.707\n37 1 0 2 0 1 0 1603.227 1625.911 1603.315\n38 2 0 0 0 1 0 1640.939 1659.086 1640.997\n39 2 1 0 0 1 0 1680.368 1693.974 1680.403\n40 2 0 0 0 1 1 1640.939 1659.086 1640.997\n41 2 0 0 1 1 0 1640.939 1659.086 1640.997\n42 2 0 1 0 1 0 1603.142 1625.825 1603.229\n43 3 0 0 0 1 0 1631.451 1654.134 1631.539\n\n\nCode\nmod1candidate2 &lt;- arima(resid1, order = c(1,0,1), seasonal = list(order = c(0,1,0)))\ncheckresiduals(mod1candidate2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1)\nQ* = 13.208, df = 8, p-value = 0.1049\n\nModel df: 2.   Total lags used: 10\n\n\nThe function to evaluate various p,d,q values returns SARIMA(1,0,1)(0,1,0)[12] with the lowest AIC and BIC. The residuals of this second model show clear correlation around lags 2 and 4, which was not present in the 4,1,0 model that auto arima suggested. So overall, I would say the diagnostics look worse for the second model than the first.\n\n\nModel 3: (ARIMAX) IWM ~ Casualties + Hurricanes + Interest Rate 3-Months\n\n\nCode\nxMatrix3 = combinedData[,c('mo3delta', 'casualties', 'hurricaneWarnings')]\nxMatrix3[is.na(xMatrix3)] &lt;- 0\nxMatrix3 = scale(xMatrix3)\nxMatrix3 = as.matrix(xMatrix3)\n\n#xMatrix\n\nmod3candidate1 = auto.arima(scale(combinedData$iwmRange), xreg = xMatrix3, trace = TRUE)\n\n\n\n Fitting models using approximations to speed things up...\n\n Regression with ARIMA(2,1,2) errors : 1751.36\n Regression with ARIMA(0,1,0) errors : 1999.57\n Regression with ARIMA(1,1,0) errors : 1844.166\n Regression with ARIMA(0,1,1) errors : 1765.158\n Regression with ARIMA(0,1,0) errors : 1997.547\n Regression with ARIMA(1,1,2) errors : 1744.072\n Regression with ARIMA(0,1,2) errors : 1761.437\n Regression with ARIMA(1,1,1) errors : 1749.049\n Regression with ARIMA(1,1,3) errors : 1740.657\n Regression with ARIMA(0,1,3) errors : 1751.237\n Regression with ARIMA(2,1,3) errors : 1748.946\n Regression with ARIMA(1,1,4) errors : 1742.521\n Regression with ARIMA(0,1,4) errors : 1752.517\n Regression with ARIMA(2,1,4) errors : 1749.048\n Regression with ARIMA(1,1,3) errors : 1738.675\n Regression with ARIMA(0,1,3) errors : 1749.422\n Regression with ARIMA(1,1,2) errors : 1742.035\n Regression with ARIMA(2,1,3) errors : 1747.358\n Regression with ARIMA(1,1,4) errors : 1740.549\n Regression with ARIMA(0,1,2) errors : 1759.509\n Regression with ARIMA(0,1,4) errors : 1750.738\n Regression with ARIMA(2,1,2) errors : 1749.538\n Regression with ARIMA(2,1,4) errors : 1747.409\n\n Now re-fitting the best model(s) without approximations...\n\n Regression with ARIMA(1,1,3) errors : 1735.245\n\n Best model: Regression with ARIMA(1,1,3) errors \n\n\nCode\ncheckresiduals(mod3candidate1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,1,3) errors\nQ* = 5.873, df = 6, p-value = 0.4376\n\nModel df: 4.   Total lags used: 10\n\n\nAuto.arima identifies (1,1,3) as the best model. The residuals show a low level of correlation in the lags, which is encouraging, and overall the residuals are mostly normally distributed although they are somewhat skewed to the right. Now, let’s see what we manually select, also considering a SARIMAX model.\nPrepare residuals:\n\n\nCode\nxMatrix3 = as.data.frame(xMatrix3)\n\n# Lets examine the residuals directly to identify \nmod3candidate2 &lt;- lm(scale(combinedData$iwmRange) ~ casualties + mo3delta + hurricaneWarnings, data = xMatrix3 )\nsummary(mod3candidate2)\n\n\n\nCall:\nlm(formula = scale(combinedData$iwmRange) ~ casualties + mo3delta + \n    hurricaneWarnings, data = xMatrix3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5534 -0.7062 -0.2568  0.4825  4.7271 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)        2.694e-17  3.804e-02   0.000   1.0000  \ncasualties         1.599e-02  3.830e-02   0.418   0.6764  \nmo3delta          -1.079e-02  3.811e-02  -0.283   0.7771  \nhurricaneWarnings  7.385e-02  3.833e-02   1.927   0.0545 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9992 on 686 degrees of freedom\nMultiple R-squared:  0.006013,  Adjusted R-squared:  0.001666 \nF-statistic: 1.383 on 3 and 686 DF,  p-value: 0.2467\n\n\nCode\nresid3 &lt;- mod3candidate2$residuals\npacf(resid3)\n\n\n\n\n\nCode\nacf(resid3)\n\n\n\n\n\nThe ACF and PACF plots of the residuals from linear regression are mixed, but there is clear correlation through value 5 in the PACF plot. The ACF plot has many significant terms, suggesting the series should be differenced. Now, I’ll loop through all the options to see if there is a suitable SARIMA model for the residuals:\n\n\nCode\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=resid3)\noutput %&gt;% filter(!is.na(p))\n\n\n   p d q P D Q      AIC      BIC     AICc\n1  0 0 0 0 1 0 1956.973 1966.046 1956.991\n2  0 1 0 0 1 0 1997.404 2001.939 1997.410\n3  0 0 0 0 1 1 1956.973 1966.046 1956.991\n4  0 1 0 0 1 1 1997.404 2001.939 1997.410\n5  0 0 0 1 1 0 1956.973 1966.046 1956.991\n6  0 1 0 1 1 0 1997.404 2001.939 1997.410\n7  0 0 0 1 1 1 1956.973 1966.046 1956.991\n8  0 1 0 1 1 1 1997.404 2001.939 1997.410\n9  0 0 0 2 1 0 1956.973 1966.046 1956.991\n10 0 1 0 2 1 0 1997.404 2001.939 1997.410\n11 0 0 0 2 1 1 1956.973 1966.046 1956.991\n12 0 0 1 0 1 0 1854.645 1868.255 1854.680\n13 0 1 1 0 1 0 1758.768 1767.838 1758.785\n14 0 0 1 0 1 1 1854.645 1868.255 1854.680\n15 0 1 1 0 1 1 1758.768 1767.838 1758.785\n16 0 0 1 1 1 0 1854.645 1868.255 1854.680\n17 0 1 1 1 1 0 1758.768 1767.838 1758.785\n18 0 0 1 1 1 1 1854.645 1868.255 1854.680\n19 0 0 1 2 1 0 1854.645 1868.255 1854.680\n20 0 0 2 0 1 0 1792.008 1810.155 1792.066\n21 0 1 2 0 1 0 1754.136 1767.742 1754.171\n22 0 0 2 0 1 1 1792.008 1810.155 1792.066\n23 0 0 2 1 1 0 1792.008 1810.155 1792.066\n24 0 0 3 0 1 0 1781.413 1804.096 1781.500\n25 1 0 0 0 1 0 1790.787 1804.397 1790.822\n26 1 1 0 0 1 0 1839.888 1848.958 1839.905\n27 1 0 0 0 1 1 1790.787 1804.397 1790.822\n28 1 1 0 0 1 1 1839.888 1848.958 1839.905\n29 1 0 0 1 1 0 1790.787 1804.397 1790.822\n30 1 1 0 1 1 0 1839.888 1848.958 1839.905\n31 1 0 0 1 1 1 1790.787 1804.397 1790.822\n32 1 0 0 2 1 0 1790.787 1804.397 1790.822\n33 1 0 1 0 1 0 1734.079 1752.226 1734.137\n34 1 1 1 0 1 0 1748.894 1762.499 1748.929\n35 1 0 1 0 1 1 1734.079 1752.226 1734.137\n36 1 0 1 1 1 0 1734.079 1752.226 1734.137\n37 1 0 2 0 1 0 1736.034 1758.717 1736.121\n38 2 0 0 0 1 0 1745.835 1763.981 1745.893\n39 2 1 0 0 1 0 1811.995 1825.600 1812.030\n40 2 0 0 0 1 1 1745.835 1763.981 1745.893\n41 2 0 0 1 1 0 1745.835 1763.981 1745.893\n42 2 0 1 0 1 0 1736.014 1758.697 1736.102\n\n\nThe best model identified by a small margin is SARIMA(1,0,1)(1,1,0). Let’s check the diagnostics\n\n\nCode\nresidualsMod3Can2 &lt;- arima(resid3, order = c(1,0,1), seasonal = list(order = c(1,1,0)))\ncheckresiduals(residualsMod3Can2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1)\nQ* = 17.291, df = 8, p-value = 0.02722\n\nModel df: 2.   Total lags used: 10\n\n\nThe residuals for this model arent encouraging, as the Ljung Box test returns a p value of 0.03. THe residuals also do not look perfectly normally distributed.\n\n\nModel 4: (ARIMAX) QQQ ~ All Interest Rates + All Extreme Weather Events + Daily VIX Change + Daily Striking Workers\nRunning an auto arima:\n\n\nCode\nxMatrix4 = combinedData[,c('mo3delta', 'mo6delta', 'mo12delta', 'events', 'dailyChange', 'workers')]\nxMatrix4[is.na(xMatrix4)] &lt;- 0\nxMatrix4 = scale(xMatrix4)\nxMatrix4 = as.matrix(xMatrix4)\n\n#xMatrix\n\nmod4candidate1 = auto.arima(scale(combinedData$qqqRange), xreg = xMatrix4, trace = TRUE)\n\n\n\n Fitting models using approximations to speed things up...\n\n Regression with ARIMA(2,1,2) errors : 1646.527\n Regression with ARIMA(0,1,0) errors : 1985.591\n Regression with ARIMA(1,1,0) errors : 1789.889\n Regression with ARIMA(0,1,1) errors : 1653.554\n Regression with ARIMA(0,1,0) errors : 1983.547\n Regression with ARIMA(1,1,2) errors : 1643.615\n Regression with ARIMA(0,1,2) errors : 1655.565\n Regression with ARIMA(1,1,1) errors : 1641.759\n Regression with ARIMA(2,1,1) errors : 1644.847\n Regression with ARIMA(2,1,0) errors : 1723.561\n Regression with ARIMA(1,1,1) errors : 1639.7\n Regression with ARIMA(0,1,1) errors : 1651.558\n Regression with ARIMA(1,1,0) errors : 1787.836\n Regression with ARIMA(2,1,1) errors : 1642.787\n Regression with ARIMA(1,1,2) errors : 1641.551\n Regression with ARIMA(0,1,2) errors : 1653.561\n Regression with ARIMA(2,1,0) errors : 1721.502\n Regression with ARIMA(2,1,2) errors : 1644.457\n\n Now re-fitting the best model(s) without approximations...\n\n Regression with ARIMA(1,1,1) errors : 1645.163\n\n Best model: Regression with ARIMA(1,1,1) errors \n\n\nCode\ncheckresiduals(mod4candidate1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,1,1) errors\nQ* = 2.722, df = 8, p-value = 0.9506\n\nModel df: 2.   Total lags used: 10\n\n\nAuto Arima returns (1,1,1). The diagnostics look acceptable, although there is clustered volatility in the residual plot. The Ljung-Box test returns p = 0.95, suggesting there is not autocorrelation in the residuals. However, the residual lag plot has high correlation around lag 20, and the correlation of the residuals slightly increases as the lags get greater.\nNow let’s select a candidate manually, including from SARIMA models. First we calculate and review the residuals from the linear regression:\n\n\nCode\nxMatrix4 = as.data.frame(xMatrix4)\n\n# Lets examine the residuals directly to identify \nmod4candidate2 &lt;- lm(scale(combinedData$qqqRange) ~ mo3delta + mo6delta + mo12delta + events + dailyChange, data = xMatrix4 )\nsummary(mod1candidate2)\n\n\n\nCall:\narima(x = resid1, order = c(1, 0, 1), seasonal = list(order = c(0, 1, 0)))\n\nCoefficients:\n         ar1      ma1\n      0.1099  -0.8103\ns.e.  0.0612   0.0446\n\nsigma^2 estimated as 0.6001:  log likelihood = -802.16,  aic = 1610.32\n\nTraining set error measures:\n                       ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.008908356 0.7740864 0.5740772 -314.3173 543.1463 0.8413363\n                     ACF1\nTraining set -0.003214659\n\n\nCode\nresid4 &lt;- mod4candidate2$residuals\npacf(resid4)\n\n\n\n\n\nCode\nacf(resid4)\n\n\n\n\n\nThe ACF plot has many significant lags (&gt;10) which suggests we may need to difference the residuals. The PACF plot has high significance through lag 5. Let’s run a function to check all of the values up through p=2 and q=5.\n\n\nCode\nmod4candidate2fit =SARIMA.c(p1=1,p2=2,q1=1,q2=5,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=1,data=resid4)\nmod4candidate2fit %&gt;% filter(!is.na(p))\n\n\n   p d q P D Q      AIC      BIC     AICc\n1  0 0 0 0 1 0 1942.736 1951.809 1942.753\n2  0 1 0 0 1 0 1984.792 1989.327 1984.798\n3  0 0 0 0 1 1 1942.736 1951.809 1942.753\n4  0 1 0 0 1 1 1984.792 1989.327 1984.798\n5  0 0 0 1 1 0 1942.736 1951.809 1942.753\n6  0 1 0 1 1 0 1984.792 1989.327 1984.798\n7  0 0 0 1 1 1 1942.736 1951.809 1942.753\n8  0 1 0 1 1 1 1984.792 1989.327 1984.798\n9  0 0 0 2 1 0 1942.736 1951.809 1942.753\n10 0 1 0 2 1 0 1984.792 1989.327 1984.798\n11 0 0 0 2 1 1 1942.736 1951.809 1942.753\n12 0 0 1 0 1 0 1844.520 1858.130 1844.555\n13 0 1 1 0 1 0 1642.996 1652.067 1643.014\n14 0 0 1 0 1 1 1844.520 1858.130 1844.555\n15 0 1 1 0 1 1 1642.996 1652.067 1643.014\n16 0 0 1 1 1 0 1844.520 1858.130 1844.555\n17 0 1 1 1 1 0 1642.996 1652.067 1643.014\n18 0 0 1 1 1 1 1844.520 1858.130 1844.555\n19 0 0 1 2 1 0 1844.520 1858.130 1844.555\n20 0 0 2 0 1 0 1788.823 1806.970 1788.881\n21 0 1 2 0 1 0 1644.979 1658.584 1645.014\n22 0 0 2 0 1 1 1788.823 1806.970 1788.881\n23 0 0 2 1 1 0 1788.823 1806.970 1788.881\n24 0 0 3 0 1 0 1761.530 1784.214 1761.618\n25 1 0 0 0 1 0 1777.225 1790.835 1777.260\n26 1 1 0 0 1 0 1788.204 1797.275 1788.222\n27 1 0 0 0 1 1 1777.225 1790.835 1777.260\n28 1 1 0 0 1 1 1788.204 1797.275 1788.222\n29 1 0 0 1 1 0 1777.225 1790.835 1777.260\n30 1 1 0 1 1 0 1788.204 1797.275 1788.222\n31 1 0 0 1 1 1 1777.225 1790.835 1777.260\n32 1 0 0 2 1 0 1777.225 1790.835 1777.260\n33 1 0 1 0 1 0 1638.017 1656.164 1638.075\n34 1 1 1 0 1 0 1644.977 1658.583 1645.012\n35 1 0 1 0 1 1 1638.017 1656.164 1638.075\n36 1 0 1 1 1 0 1638.017 1656.164 1638.075\n37 1 0 2 0 1 0 1639.444 1662.128 1639.532\n\n\nThe best AIC and BIC scores returned by the function are for the model SARIMA(1,0,1)(1,1,0). Let’s look at the diagnostic plots to see how well this model captures the data:\n\n\nCode\nmod4candidate2fit = arima(resid4, order = c(1,0,1), seasonal = list(order = c(1,1,0)))\ncheckresiduals(mod4candidate2fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1)\nQ* = 4.6414, df = 8, p-value = 0.7951\n\nModel df: 2.   Total lags used: 10\n\n\nThe residuals for this model look similar to the auto.arima model, so it will be interesting to compare them with cross validation. Otherwise, it is notable that the residuals display clustered volatility, while the lag plot shows significant correlations at some values, although the Ljung-Box test returns 0.795 so we can conclude there is no autocorrelation in the residuals.\n\n\nModel 5: (ARIMAX) QQQ ~ Interest Rate 3 months + Daily Property Damage + Daily VIX Change + Daily New Striking Workers\nI am leaving this model to fit after the homework.\n\nModel Selection for VAR Models\n\n\nModel 2: (VAR) SPY ~ Interest Rate 3-Months + Daily VIX Value\n\nStep 1, let’s fit VAR with p=1 just to see the relationship between our 3 variables (SPY intraday range, 3-month interest rate changes, and the real daily VIX values).\n\n\nCode\nxMatrix2 = combinedData[,c('VIX.Adjusted', 'mo3delta', 'spyRange') ]\nxMatrix2[is.na(xMatrix2)] &lt;- 0\nxMatrix2 = scale(xMatrix2)\nxMatrix2 = as.matrix(xMatrix2)\n\n#xMatrix\n\nsummary(vars::VAR(xMatrix2, p = 1, type = 'both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: VIX.Adjusted, mo3delta, spyRange \nDeterministic variables: both \nSample size: 689 \nLog Likelihood: -1891.17 \nRoots of the characteristic polynomial:\n0.9396 0.05755 0.05755\nCall:\nvars::VAR(y = xMatrix2, p = 1, type = \"both\")\n\n\nEstimation results for equation VIX.Adjusted: \n============================================= \nVIX.Adjusted = VIX.Adjusted.l1 + mo3delta.l1 + spyRange.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nVIX.Adjusted.l1  9.330e-01  1.915e-02  48.710   &lt;2e-16 ***\nmo3delta.l1     -2.685e-02  1.301e-02  -2.064   0.0394 *  \nspyRange.l1      4.189e-03  1.888e-02   0.222   0.8245    \nconst            1.716e-02  2.680e-02   0.641   0.5221    \ntrend           -5.707e-05  6.794e-05  -0.840   0.4013    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3372 on 684 degrees of freedom\nMultiple R-Squared: 0.8869, Adjusted R-squared: 0.8862 \nF-statistic:  1341 on 4 and 684 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation mo3delta: \n========================================= \nmo3delta = VIX.Adjusted.l1 + mo3delta.l1 + spyRange.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)   \nVIX.Adjusted.l1 -0.0429432  0.0558806  -0.768  0.44247   \nmo3delta.l1     -0.1137505  0.0379419  -2.998  0.00282 **\nspyRange.l1     -0.1245585  0.0550961  -2.261  0.02409 * \nconst            0.1330199  0.0781812   1.701  0.08932 . \ntrend           -0.0003844  0.0001982  -1.939  0.05287 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9838 on 684 degrees of freedom\nMultiple R-Squared: 0.03914,    Adjusted R-squared: 0.03352 \nF-statistic: 6.965 on 4 and 684 DF,  p-value: 1.69e-05 \n\n\nEstimation results for equation spyRange: \n========================================= \nspyRange = VIX.Adjusted.l1 + mo3delta.l1 + spyRange.l1 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nVIX.Adjusted.l1  0.6616598  0.0403536  16.397  &lt; 2e-16 ***\nmo3delta.l1      0.0662097  0.0273994   2.416   0.0159 *  \nspyRange.l1      0.0661674  0.0397871   1.663   0.0968 .  \nconst           -0.3143373  0.0564577  -5.568 3.71e-08 ***\ntrend            0.0008977  0.0001431   6.271 6.34e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7104 on 684 degrees of freedom\nMultiple R-Squared: 0.496,  Adjusted R-squared: 0.4931 \nF-statistic: 168.3 on 4 and 684 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             VIX.Adjusted  mo3delta spyRange\nVIX.Adjusted      0.11371 -0.019172 0.076451\nmo3delta         -0.01917  0.967841 0.006324\nspyRange          0.07645  0.006324 0.504715\n\nCorrelation matrix of residuals:\n             VIX.Adjusted  mo3delta spyRange\nVIX.Adjusted      1.00000 -0.057792 0.319129\nmo3delta         -0.05779  1.000000 0.009049\nspyRange          0.31913  0.009049 1.000000\n\n\nThe initial VAR fit is encouraging, as the 3 variables are all significant. SPY range has a p value of 0.09, which is slightly above the 0.05 threshold that would be ideal, but still suggests it helps explain the variance in the other variables in the model. The overall R squared and adjusted R squared are also encouraging, at 0.5, which is exceptionally high for a model concerning stock prices.\nNow lets use VAR select to identify some preferrable p values.\n\n\nCode\nvars::VARselect(xMatrix2, lag.max = 10, type = 'both')\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      1      1      5 \n\n$criteria\n                 1           2           3          4           5           6\nAIC(n) -2.95967636 -2.96412325 -2.96215454 -2.9562924 -3.00144103 -2.99099019\nHQ(n)  -2.92106481 -2.90234476 -2.87720911 -2.8481801 -2.87016173 -2.83654396\nSC(n)  -2.85992432 -2.80451997 -2.74270003 -2.6769867 -2.66228407 -2.59198201\nFPE(n)  0.05183573  0.05160586  0.05170783  0.0520123  0.04971693  0.05024022\n                 7           8           9          10\nAIC(n) -2.98502423 -2.96721214 -2.95685167 -2.95666268\nHQ(n)  -2.80741107 -2.76643204 -2.73290464 -2.70954871\nSC(n)  -2.52616482 -2.44850150 -2.37828980 -2.31824958\nFPE(n)  0.05054219  0.05145229  0.05199039  0.05200299\n\n\nVAR select returns either p =5 or p = 1 as the best fits, with AIC and FPE selecting p=5, and HQ and SC selecting p =1. We will use cross validation to compare these options."
  },
  {
    "objectID": "ARIMAX SARIMAX VAR.html#model-evaluation",
    "href": "ARIMAX SARIMAX VAR.html#model-evaluation",
    "title": "ARIMAX SARIMAX VAR",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nIn this section, I will use cross validation to select the best candidate model for each of the 5 overaching model designs. First, lets define a cross validation function\n\n\nCode\n#######\n\ncrossVal &lt;- function(arima1order, arima2order, sarima2order, data) {\n\n    # window is always 1\n    test &lt;- 30\n    trainnum &lt;- length(data) - test\n    rmse1 &lt;- vector(mode = 'numeric', length = 30)\n    rmse2 &lt;- vector(mode = 'numeric', length = 30)\n\n    for(i in 1:30) {\n        #print(trainnum + ((i-1) * 4))\n        #print(trainnum + (i*4))\n        #print(trainnum + ((i-1) * 4) +1)\n        \n        xtrain &lt;- data[c(1:(trainnum + i - 1))]\n        xtest &lt;-  data[c((trainnum+1):(trainnum+i+1))]\n        \n        \n        \n        ######## first Model ############\n        fit &lt;- arima(xtrain, order = arima1order)\n        fcast &lt;- predict(fit, n.ahead = 1)$pred[1]\n        \n        \n        ######## second model ###########\n        fit2 &lt;- arima(xtrain, order = arima2order, seasonal = sarima2order)\n        fcast2 &lt;- predict(fit2, n.ahead = 1)$pred[1]\n        \n        # Errors\n\n        rmse1[i]  &lt;-sqrt((fcast-xtest[1])^2)\n        rmse2[i]  &lt;-sqrt((fcast2-xtest[1])^2)\n        \n    }\n    \n    outputs = data.frame(\"rmse1\" = rmse1, 'rmse2' = rmse2)\n    return(outputs)\n\n}\n\n\n\nModel 1 (ARIMAX)\nLets test the function out on the first model, comparing ARIMA(4,1,1) vs. SARIMA(1,0,1)(0,1,0) on the residuals of the linear regression.\n\n\nCode\nmodel1comparison = crossVal(c(4,1,1), c(1,0,1), list(order = c(0,1,0)), resid1)\n\nmean(model1comparison$rmse1)\n\n\n[1] 0.2690243\n\n\nCode\nmean(model1comparison$rmse2)\n\n\n[1] 0.2696219\n\n\nBetween the two models, model 1, selected by auto.arima, beats out the sarima model with a mean RMSE of 0.2690 vs. 0.2696. However, the models do perform similarly to eachother. Let’s look at a graph to make the difference more clear:\n\n\nCode\nindex1 = c(1:nrow(model1comparison))\nggplot(data = model1comparison, aes(x = index1, y = rmse1), color = 'blue') + geom_line() + geom_line(aes(x = index1, y = rmse2), color = 'red') + labs(title = 'Comparing RMSE of ARIMAX(4,1,1) Black and SARIMAX(1,0,1)(0,1,0) Red')\n\n\n\n\n\nAs we can see in the cross-validation chart, the two models perform similarly, and tend to have higher and lower errors at the same time, with both of the models performing poorly near the middle of the cross validation data sets. But overall, the ARIMA (ARIMAX) model performs the best at predicting the residuals. As such, my chosen model for Model 1 is ARIMAX(4,1,1).\n\nPredictions\nNow, let’s make predictions:\n\n\nCode\n#mod1candidate2 &lt;- lm(scale(combinedData$spyRange) ~ mo12delta + pdam + dailyChange + workers, data = xMatrix )\n\n\nmod1pdam &lt;- forecast(auto.arima(combinedData$pdam), 10)\nmod1delta12 &lt;- forecast(auto.arima(combinedData$mo12delta), 10)\nmod1dailyChange &lt;- forecast(auto.arima(combinedData$dailyChange), 10)\nmod1workers &lt;- forecast(auto.arima(combinedData$workers), 10)\n\npredictors1 &lt;- data.frame(cbind(pdam = mod1pdam$mean, mo12delta = mod1delta12$mean, dailyChange = mod1dailyChange$mean, workers = mod1workers$mean))\n\nfit = arima(combinedData$spyRange, order = c(4,1,1), xreg = xMatrix)\n\n#summary(fit)\n#forecast(fit)\nmod1 = predict(fit, newxreg = predictors1)\n\nautoplot(mod1$pred)\n\n\n\n\n\nHere we can see the models predictions for the SPY daily upcoming range, using auto.arima generated models to predict the external regressors. Overall, the model predicts a precipitous drop in the daily range in SPY prices in the upcoming 10 days.\n\n\n\nModel 3 (ARIMAX) IWM ~ Casualties + Hurricanes + Interest Rate 3-Months\nLet’s compare the model returned by auto.arima, ARIMA(1,1,3), vs the model I found by hand, SARIMA(1,0,1)(1,1,0), on the residuals of the linear regression for model 3.\n\n\nCode\nmodel3comparison = crossVal(c(1,1,3), c(1,0,1), list(order = c(1,1,0)), resid3)\n\nmean(model3comparison$rmse1)\n\n\n[1] 0.5537179\n\n\nCode\nmean(model3comparison$rmse2)\n\n\n[1] 0.6006214\n\n\nOnce again, the simple ARIMA model beats out the SARIMA architecture in terms of average RMSE. The average for ARIMA(1,1,3) was 0.554, while for SARIMA(1,0,1)(1,1,0) it was 0.601. Let’s look at a plot of the rmse values to see how the models fared:\n\n\nCode\nindex3 = c(1:nrow(model3comparison))\nggplot(data = model3comparison, aes(x = index3, y = rmse1), color = 'blue') + geom_line() + geom_line(aes(x = index3, y = rmse2), color = 'red') + labs(title = 'Comparing RMSE of ARIMAX(1,1,3) Black and SARIMAX(1,0,1)(1,1,0) Red')\n\n\n\n\n\nJust like for model 1, the two approaches performed similarly. If one model had a high RMSE for a particular value, the other model was likely to perform poorly as well. Overall however, we can see that for a given value the black line (ARIMA) tended to perform better. In the end, the ARIMAX models seems to fit the data well. As such, my chosen model for Model 3 is: ARIMAX(1,1,3)/.\n\nPredictions\nNow, let’s make predictions:\n\n\nCode\n#xMatrix3 = combinedData[,c('mo3delta', 'casualties', 'hurricaneWarnings')]\n\nmod3cas &lt;- forecast(auto.arima(combinedData$casualties), 10)\nmod3delta3 &lt;- forecast(auto.arima(combinedData$mo3delta), 10)\nmod3hurricane &lt;- forecast(auto.arima(combinedData$hurricaneWarnings), 10)\n\npredictors3 &lt;- data.frame(cbind(casualties = mod3cas$mean, mo3delta = mod3delta3$mean, hurricaneWarnings = mod3hurricane$mean))\n\nfit3 = arima(combinedData$iwmRange, order = c(1,1,3), xreg = xMatrix3)\n\n#summary(fit)\n#forecast(fit)\nmod3 = predict(fit3, newxreg = predictors3)\n\nautoplot(mod3$pred)\n\n\n\n\n\nHere we can see the models predictions for the IWM daily upcoming range, using auto.arima generated models to predict the external regressors. Overall, the model predicts a steep incline in IWM intraday ranges in the upcoming 10 days, especially in the first 5 before leveling off.\n\n\n\nModel 4 ARIMAX\nLeaving for future work. ### Model 5 ARIMAX Leaving for future work.\n\n\nModel 2 VAR\nLet’s forecast our VAR model, which used 3-month interest rate changes and daily VIX values to predict SPY’s intraday range. For this model we wanted to compare the p values of 1 and 5 to find the best model.\nLets, run our CV function:\n\n\nCode\ndata = xMatrix2\n\n    # window is always 1\n    test &lt;- 30\n    trainnum &lt;- nrow(data) - test\n    rmse1 &lt;- matrix(NA, 30,3)\n    rmse1 &lt;- data.frame(rmse1)\n    rmse2 &lt;- matrix(NA, 30,3)\n    rmse2 &lt;- data.frame(rmse2)\n\n    for(i in 1:29) {\n\n        xtrain &lt;- data[c(1:(trainnum + i - 1)),]\n        xtest &lt;-  data[c((trainnum+i):(trainnum+i+1)),]\n        \n        \n        \n        ######## first Model ############\n        fit &lt;- vars::VAR(xtrain, p = 1, type = 'both')\n        \n        fcast &lt;- predict(fit, n.ahead = 1)$fcst\n        \n        ff&lt;-data.frame(fcast$VIX.Adjusted[,1],fcast$mo3delta[,1],fcast$spyRange[,1])\n        \n        ######## second model ###########\n        fit2 &lt;- vars::VAR(xtrain,p =5, type = 'both')\n        fcast2 &lt;- predict(fit2, n.ahead = 1)$fcst\n        \n        ff2&lt;-data.frame(fcast2$VIX.Adjusted[,1],fcast2$mo3delta[,1],fcast2$spyRange[,1])\n\n        # Errors\n\n        rmse1[i,]  = sqrt((ff-xtest)^2)\n        rmse2[i,]  = sqrt((ff2-xtest)^2)\n        \n    }\n    \n    \n#print(rmse1)\n\n\nnames(rmse1) =c(\"VIXPrice\", \"3mo\",\"SPYDailyRange\")\nnames(rmse2) =c(\"VIXPrice\", \"3mo\",\"SPYDailyRange\")\n\ncolMeans(rmse1, na.rm = TRUE)\n\n\n     VIXPrice           3mo SPYDailyRange \n    0.1477893     1.1090823     0.6958689 \n\n\nCode\ncolMeans(rmse2, na.rm = TRUE)\n\n\n     VIXPrice           3mo SPYDailyRange \n    0.1387773     1.1899464     0.6822915 \n\n\nAfter cross validating the 2 VAR models across 30 1-day intervals, we obtain the following average RMSE for the different variables: VIX Price: p=1 -&gt; 0.148, p=5 -&gt; 0.139. 3 Month Interest Rate Changes: p=1 -&gt; 1.109, p=5 -&gt; 1.190. SPY Daily Range: p=1 -&gt; 0.696, p=5 -&gt; 0.682.\nOverall, the p=1 VAR model was better at predicting the interest rate changes variable, and the p=5 VAR model performed better on the VIX Price and SPY daily range variables.\nLet’s plot their performance:\n\n\nCode\nindex2 = c(1:nrow(rmse1))\n\nggplot() + \n  geom_line(data = rmse1, aes(x = index2, y = VIXPrice),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = index2, y = VIXPrice),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Vix Prices, Blue = (p=1), Red = (p=5)\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nCode\nggplot() + \n  geom_line(data = rmse1, aes(x = index2, y = SPYDailyRange),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = index2, y = SPYDailyRange),color = \"red\") +\n  labs(\n    title = \"CV RMSE for SPY Daily Range, Blue = (p=1), Red = (p=5)\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\nRemoved 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nOverall, the charts show what we confirmed with the average values: That the mean performance of the p=5 model was better on average.\nNow let’s predict:\n\n\nCode\nfinalmod2 = vars::VAR(xMatrix2, p = 5, type = 'both')\n        \nmod2forecast &lt;- predict(finalmod2, n.ahead = 10)$fcst\n\nindexmod2 = c(1:10)\nggplot() +\n    geom_line(aes(x = indexmod2, y = mod2forecast$spyRange[1:10]),color = \"blue\") +\n    geom_line(aes(x = indexmod2, y = mod2forecast$mo3delta[1:10]),color = \"red\") +\n    geom_line(aes(x = indexmod2, y = mod2forecast$VIX.Adjusted[1:10]),color = \"green\")\n\n\n\n\n\nHere we can see the scale forecasts for the 3 key variables, which are created with the predict function for the VAR model with p=5. Overall, the model predicts the SPY range to raise slightly for the next 10 days, and the 3 month interest rates to change sharply."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "About Me",
    "section": "",
    "text": "Hiking in the Cascades circa. 2020\n\n\nHi Everyone, I am Corwin Dark and I am a first year master’s student in the Data Science and Analytics Program here at Georgetown. I grew up in Seattle, Washington before coming to DC for university. I attended American University for my undergraduate degree, which was in international relations. After graduating during COVID, I worked for a small consulting firm on data-related topics for international development programs. After a few years of working, I realized I had a strong curiosity to learn more about cutting edge data science methods, and so I have come back to school. Within data science, I am really interested in how algorithms and techniques can be combined to perform better than they would alone, which is why I really enjoyed creating this project for class and drawing on so many different techniques. I am super excited to be in the Georgetown program with you all, and I hope you enjoy my portfolio! Start reading with the introduction here."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "About Me",
    "section": "Interests",
    "text": "Interests\n\nDeep learning and neural networks\nEnsembling and meta learning\nFinancial data science"
  },
  {
    "objectID": "Deep Learning for TS.html#declare-useful-functions",
    "href": "Deep Learning for TS.html#declare-useful-functions",
    "title": "Deep Learning for TS",
    "section": "Declare Useful Functions",
    "text": "Declare Useful Functions\n\n\nCode\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "Deep Learning for TS.html#read-in-data",
    "href": "Deep Learning for TS.html#read-in-data",
    "title": "Deep Learning for TS",
    "section": "Read in Data",
    "text": "Read in Data\n\n\nCode\nimport yfinance as yf\n\nspy = yf.download('SPY',start = \"2021-01-01\", end = \"2023-09-30\")\niwm = yf.download('QQQ',start = \"2021-01-01\", end = \"2023-09-30\")\nqqq = yf.download('IWM',start = \"2021-01-01\", end = \"2023-09-30\")\n\nspyRange = (spy['High'] - spy['Low']) / spy['Close']\niwmRange = (iwm['High'] - iwm['Low']) / iwm['Close']\nqqqRange = (qqq['High'] - qqq['Low']) / qqq['Close']\n\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed"
  },
  {
    "objectID": "Deep Learning for TS.html#normalize",
    "href": "Deep Learning for TS.html#normalize",
    "title": "Deep Learning for TS",
    "section": "Normalize",
    "text": "Normalize\n\n\nCode\n# Slice training set\nspy_train = pd.DataFrame( spyRange[range(0,490)] )\nspy_train = spy_train.reset_index(drop = True)\n\n# Slice validation set\nspy_val =  pd.DataFrame( spyRange[range(490,590)] )\nspy_val = spy_val.reset_index(drop = True)\n\n# Slice test set\nspy_test =  pd.DataFrame( spyRange[range(590,690)] )\nspy_test = spy_test.reset_index(drop = True)\n\n# Save training normalization to use on test and validation sets\nspy_train_mean = spy_train.mean()\nspy_train_std = spy_train.std()\n\n# Normalize\nspy_train=(spy_train-spy_train_mean)/spy_train_std\nspy_val=(spy_val-spy_train_mean)/spy_train_std\nspy_test=(spy_test-spy_train_mean)/spy_train_std"
  },
  {
    "objectID": "Deep Learning for TS.html#fit-helper-function-to-create-data-in-desired-format",
    "href": "Deep Learning for TS.html#fit-helper-function-to-create-data-in-desired-format",
    "title": "Deep Learning for TS",
    "section": "Fit Helper Function to Create Data in Desired Format",
    "text": "Fit Helper Function to Create Data in Desired Format\n\n\nCode\n# code from lab\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n\n    # verbose=True --&gt; report and plot for debugging\n    # unique=True --&gt; don't re-sample:\n    # x1,x2,x3 --&gt; x4 then x4,x5,x6 --&gt; x7 instead of x2,x3,x4 --&gt; x5\n    # initialize\n    i_start=0; count=0;\n\n    # initialize output arrays with samples\n    x_out=[]\n    y_out=[]\n\n    # sequentially build mini-batch samples\n    while i_start+lookback+delay&lt; x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n    \n        # report if desired\n        if verbose and count&lt;2: print(\"indice range:\",i_start,i_stop,\"--&gt;\", i_pred)\n\n        # define arrays:\n        # method-1: buggy due to indexing from left\n        # numpy's slicing --&gt; start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while j&gt;=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n        \n        # create mini-batch sample\n        xtmp=x.iloc[indices_to_keep,:] # isolate relevant indices\n        \n        xtmp=xtmp.iloc[:,feature_columns] # isolate desire features\n        ytmp=x.iloc[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp);\n        \n        # report if desired\n        if verbose and count&lt;2: print(xtmp, \"--&gt;\",ytmp)\n        if verbose and count&lt;2: print(\"shape:\",xtmp.shape, \"--&gt;\",ytmp.shape)\n        \n        if verbose and count&lt;2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n        \n        # UPDATE START POINT\n        if unique: i_start+=lookback\n        i_start+=1; count+=1\n\n    return np.array(x_out),np.array(y_out)\n\n\nDiagnostic plotting functions from the lab\n\n\nCode\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n\n# UTILITY FUNCTION\ndef regression_report(yt,ytp,yv,yvp):\n\n    print(\"---------- Regression report ----------\")\n\n    print(\"TRAINING:\")\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n    title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]);\n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n    print(\"VALIDATION:\")\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n    title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\n\nUse function to reformat data. Train will be 1-490, Validation 491-590, and Test 591-690\n\n\nCode\n#spy_train = generator(spyRange, lookback=lookback, delay=delay, min_index=0, max_index=489, shuffle=True, step=step, batch_size=batch_size)\n#spy_val = generator(spyRange, lookback=lookback, delay=delay, min_index=490, max_index=589, step=step, batch_size=batch_size)\n#spy_test = generator(spyRange, lookback=lookback, delay=delay, min_index=590, max_index=690, step=step, batch_size=batch_size)\n\n\n\nL = 30\nS = 1\nD = 1\nbatch_size = 10 \n\n\nspy_x,spy_y=form_arrays(spy_train,lookback=L,delay=D,step=S,unique=False,verbose=True)\n\nspy_val_x, spy_val_y = form_arrays(spy_val,lookback=L,delay=D,step=S,unique=False,verbose=True)\n\n\nindice range: 0 30 --&gt; 31\n           0\n30 -0.671401\n29 -0.885386\n28 -0.734102\n27 -0.585913\n26 -0.212548\n25 -1.168214\n24 -1.015748\n23 -0.973075\n22 -0.358360\n21 -0.677961\n20  0.497549\n19  0.548116\n18  1.059558\n17  0.243034\n16  0.998227\n15 -0.971943\n14  0.294332\n13 -0.976559\n12 -1.166153\n11 -0.404222\n10 -0.907305\n9  -0.447196\n8  -0.729908\n7  -0.739743\n6  -0.579574\n5  -0.785181\n4  -0.302614\n3  -0.423132\n2   0.857734\n1  -0.245422\n0   1.805622 --&gt; 0   -0.526714\nName: 31, dtype: float64\nshape: (31, 1) --&gt; (1,)\nindice range: 1 31 --&gt; 32\n           0\n31 -0.526714\n30 -0.671401\n29 -0.885386\n28 -0.734102\n27 -0.585913\n26 -0.212548\n25 -1.168214\n24 -1.015748\n23 -0.973075\n22 -0.358360\n21 -0.677961\n20  0.497549\n19  0.548116\n18  1.059558\n17  0.243034\n16  0.998227\n15 -0.971943\n14  0.294332\n13 -0.976559\n12 -1.166153\n11 -0.404222\n10 -0.907305\n9  -0.447196\n8  -0.729908\n7  -0.739743\n6  -0.579574\n5  -0.785181\n4  -0.302614\n3  -0.423132\n2   0.857734\n1  -0.245422 --&gt; 0   -0.821283\nName: 32, dtype: float64\nshape: (31, 1) --&gt; (1,)\nindice range: 0 30 --&gt; 31\n           0\n30 -0.287952\n29 -0.233536\n28  0.467138\n27 -0.636803\n26  0.401771\n25  0.651644\n24 -0.507879\n23  1.045176\n22 -0.734373\n21  0.055444\n20  0.157378\n19 -0.405520\n18 -0.339190\n17  0.188520\n16  1.383184\n15 -0.716225\n14  0.161146\n13  1.045477\n12 -0.383336\n11 -0.031053\n10  0.548781\n9  -0.587335\n8  -0.105081\n7   1.954741\n6  -0.216591\n5  -0.303154\n4   0.070543\n3   0.055478\n2   0.595663\n1   1.097443\n0   1.755203 --&gt; 0   -0.23042\nName: 31, dtype: float64\nshape: (31, 1) --&gt; (1,)\nindice range: 1 31 --&gt; 32\n           0\n31 -0.230420\n30 -0.287952\n29 -0.233536\n28  0.467138\n27 -0.636803\n26  0.401771\n25  0.651644\n24 -0.507879\n23  1.045176\n22 -0.734373\n21  0.055444\n20  0.157378\n19 -0.405520\n18 -0.339190\n17  0.188520\n16  1.383184\n15 -0.716225\n14  0.161146\n13  1.045477\n12 -0.383336\n11 -0.031053\n10  0.548781\n9  -0.587335\n8  -0.105081\n7   1.954741\n6  -0.216591\n5  -0.303154\n4   0.070543\n3   0.055478\n2   0.595663\n1   1.097443 --&gt; 0    0.020882\nName: 32, dtype: float64\nshape: (31, 1) --&gt; (1,)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReshape data\n\n\nCode\n# RESHAPE INTO A DATA FRAME\nspy_t_1 = spy_x.reshape(spy_x.shape[0],spy_x.shape[1]*spy_x.shape[2])\nspy_v_1 = spy_val_x.reshape(spy_val_x.shape[0],spy_val_x.shape[1]*spy_val_x.shape[2])\n\n\ninput_shape = (spy_t_1.shape[1],)\nrnn_input_shape = (spy_x.shape[1], spy_x.shape[2])\n\n# NEW SIZES\nprint(\"train: \", spy_x.shape,\"--&gt;\",spy_t_1.shape)\nprint(\"validation: \", spy_val_x.shape,\"--&gt;\",spy_v_1.shape)\n\n\ntrain:  (459, 31, 1) --&gt; (459, 31)\nvalidation:  (69, 31, 1) --&gt; (69, 31)"
  },
  {
    "objectID": "Deep Learning for TS.html#fit-gru",
    "href": "Deep Learning for TS.html#fit-gru",
    "title": "Deep Learning for TS",
    "section": "Fit GRU",
    "text": "Fit GRU\nNo regularlization\n\n\nCode\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_1_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=30, validation_data= (spy_val_x, spy_val_y), validation_steps=10)\n\n\nWARNING:tensorflow:From C:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nEpoch 1/30\nWARNING:tensorflow:From C:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n 1/20 [&gt;.............................] - ETA: 1:20 - loss: 0.5548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7631  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7686\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.7694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7774\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7722\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7526\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 5s 47ms/step - loss: 0.7395 - val_loss: 0.5307\nEpoch 2/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.9408\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7476\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7262\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.7455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7249\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.7255\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.7255 - val_loss: 0.5363\nEpoch 3/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7118\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7394\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.7084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7190\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7191\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7311\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.7280 - val_loss: 0.5508\nEpoch 4/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7277\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6981\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6965\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7278\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7260\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.7197 - val_loss: 0.5784\nEpoch 5/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 1.0293\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.8250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7671\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.7393\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7261\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7215\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.7194 - val_loss: 0.5481\nEpoch 6/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7102\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7268\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.7199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7317\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7229\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7195\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.7121 - val_loss: 0.5702\nEpoch 7/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7263\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7269\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.7200\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7249\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 22ms/step - loss: 0.7126 - val_loss: 0.5774\nEpoch 8/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4888\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6971\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6975\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.7002 - val_loss: 0.6068\nEpoch 9/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6690\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6948\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6476\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6788\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6788 - val_loss: 0.6124\nEpoch 10/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6329\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6755\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6633 - val_loss: 0.5173\nEpoch 11/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4952\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6798\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6394\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6114\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6299\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6366 - val_loss: 0.5933\nEpoch 12/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7076\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.5998\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.6716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6609\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6386 - val_loss: 0.5496\nEpoch 13/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.9449\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.6749\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6795\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6459\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6422 - val_loss: 0.5310\nEpoch 14/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6919\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6604\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 22ms/step - loss: 0.6344 - val_loss: 0.5129\nEpoch 15/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5514\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.5519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6032\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6280\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6280 - val_loss: 0.5253\nEpoch 16/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6107\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6203\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6336\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6186\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6186 - val_loss: 0.5353\nEpoch 17/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4749\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6246\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6177\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6248\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6374\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6395\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6260 - val_loss: 0.5501\nEpoch 18/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4490\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.6272\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6197\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6164\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6164 - val_loss: 0.6235\nEpoch 19/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6058\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6099\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6255\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6235\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6211 - val_loss: 0.5644\nEpoch 20/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4737\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6308\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6506\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6161\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6151\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6084 - val_loss: 0.5442\nEpoch 21/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6733\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6235\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6264\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6232\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6189\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 22ms/step - loss: 0.6181 - val_loss: 0.6058\nEpoch 22/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4851\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.5952\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5762\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5900\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.5921\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6133 - val_loss: 0.6396\nEpoch 23/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5522\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6427\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6104\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6130\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.5996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.5996 - val_loss: 0.6000\nEpoch 24/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 22ms/step - loss: 0.6014 - val_loss: 0.5293\nEpoch 25/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.5530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6045\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6052\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6131\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.5987\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6106 - val_loss: 0.7323\nEpoch 26/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6216\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6002\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.6125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6134 - val_loss: 0.5173\nEpoch 27/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6433\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6310\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6215\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6191 - val_loss: 0.5556\nEpoch 28/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4803\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.5302\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5249\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.5774\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.5935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.5935 - val_loss: 0.6026\nEpoch 29/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6495\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.5850\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6020\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.5950 - val_loss: 0.5986\nEpoch 30/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.3834\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.4578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5304\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5566\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.5926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.5936\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.5919 - val_loss: 0.6036\n\n\nLet’s plot results\n\n\nCode\nhistory_plot(history_spy_1_noreg)\n\n\n\n\n\nRegularlization\n\n\nCode\nfrom tensorflow.keras import regularizers\nL1=0\nL2=1e-3\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape,kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n# layer that reads the recurent layer\nmodel.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5,kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_1_reg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=30, validation_data= (spy_val_x, spy_val_y), validation_steps=10)\n\n\nEpoch 1/30\n 1/20 [&gt;.............................] - ETA: 1:19 - loss: 0.7256\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.8191  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.8315\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.8297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.8231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 5s 38ms/step - loss: 0.8230 - val_loss: 0.6097\nEpoch 2/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.8212\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7982\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.8140\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7977\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 19ms/step - loss: 0.7973 - val_loss: 0.6024\nEpoch 3/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.8349\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.8084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7934\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 19ms/step - loss: 0.7918 - val_loss: 0.6123\nEpoch 4/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6283\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7565\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.8019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 18ms/step - loss: 0.7775 - val_loss: 0.5833\nEpoch 5/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8295\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7545\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 19ms/step - loss: 0.7683 - val_loss: 0.5798\nEpoch 6/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6774\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7461\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7621\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.7623\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 20ms/step - loss: 0.7623 - val_loss: 0.6368\nEpoch 7/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8800\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7394\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.7182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.7692\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7669\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.7587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 19ms/step - loss: 0.7587 - val_loss: 0.6096\nEpoch 8/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8067\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7472\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7681\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.7442\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.7541\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7440\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 19ms/step - loss: 0.7428 - val_loss: 0.6354\nEpoch 9/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.6982\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.7328\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.7398\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 20ms/step - loss: 0.7350 - val_loss: 0.6368\nEpoch 10/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5953\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7277\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.7007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7282\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.7397\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.7402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 20ms/step - loss: 0.7258 - val_loss: 0.5796\nEpoch 11/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7470\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.7914\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.7254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.7000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.7000 - val_loss: 0.6170\nEpoch 12/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7660\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6949\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7148\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6968\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6897 - val_loss: 0.5914\nEpoch 13/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7686\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.6391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6843\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6655 - val_loss: 0.5706\nEpoch 14/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6808\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.7035\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6546\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6519 - val_loss: 0.5858\nEpoch 15/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6298\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6257\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6172\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6433\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6487 - val_loss: 0.5604\nEpoch 16/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7176\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7311\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6474\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6609\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6551 - val_loss: 0.5618\nEpoch 17/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.6869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7078\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6953\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6591 - val_loss: 0.5751\nEpoch 18/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7323\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6810\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.6558\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6586\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6436\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6351\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6351 - val_loss: 0.5892\nEpoch 19/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7150\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6953\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6698\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6423\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6356 - val_loss: 0.5710\nEpoch 20/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7900\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7112\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6778\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6431\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6549\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6486 - val_loss: 0.5601\nEpoch 21/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6203\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6376\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6113\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6126\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6311\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6371\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6364 - val_loss: 0.5651\nEpoch 22/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6946\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6360\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6458\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6274\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6256 - val_loss: 0.5667\nEpoch 23/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7790\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7304\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6570\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6507\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6457 - val_loss: 0.5488\nEpoch 24/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5290\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.5546\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/20 [===========&gt;..................] - ETA: 0s - loss: 0.6066\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.6373\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6385\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.6529\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6369 - val_loss: 0.5621\nEpoch 25/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7230\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6214\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6314\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6212\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6503\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6426 - val_loss: 0.5975\nEpoch 26/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6187\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6395\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6262\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6166\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6033\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6266\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6378\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6378 - val_loss: 0.6495\nEpoch 27/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.5857\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6131\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6120\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6306\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6292\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6410 - val_loss: 0.5961\nEpoch 28/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6309\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5899\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5886\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.5995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6091\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6246 - val_loss: 0.5950\nEpoch 29/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.7269\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6561\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6516\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6385\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6408\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6379 - val_loss: 0.5586\nEpoch 30/30\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/20 [=====&gt;........................] - ETA: 0s - loss: 0.6625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6200\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6382\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6241\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6197\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6255\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 21ms/step - loss: 0.6343 - val_loss: 0.5557\n\n\nLet’s look at the diagnostic plot\n\n\nCode\nhistory_plot(history_spy_1_reg)\n\n\n\n\n\nCompared to the model without regularization, we can see that the training set error decreases more steadily, without regressing higher very often. The consequence of this is that the raining error goes lower than the validation error faster than without regularization, suggesting regularization is helping the model to learn more purposefully and successfully."
  },
  {
    "objectID": "Deep Learning for TS.html#fit-rnn",
    "href": "Deep Learning for TS.html#fit-rnn",
    "title": "Deep Learning for TS",
    "section": "Fit RNN",
    "text": "Fit RNN\n\n\nCode\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.SimpleRNN(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_2_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10)\n\n\nEpoch 1/40\n 1/20 [&gt;.............................] - ETA: 18s - loss: 1.0789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.9296 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 17ms/step - loss: 0.8798 - val_loss: 0.5963\nEpoch 2/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8186\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.7987\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.7811 - val_loss: 0.6081\nEpoch 3/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7701\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.7252\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.7433 - val_loss: 0.6222\nEpoch 4/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.7260\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.7198 - val_loss: 0.6047\nEpoch 5/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7436\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.6789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.7045 - val_loss: 0.6365\nEpoch 6/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7314\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.6979 - val_loss: 0.6021\nEpoch 7/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6767\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.6916 - val_loss: 0.6089\nEpoch 8/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6493\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6817 - val_loss: 0.6581\nEpoch 9/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6462\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6770 - val_loss: 0.7033\nEpoch 10/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6939\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6800\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6746 - val_loss: 0.6072\nEpoch 11/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.6543 - val_loss: 0.6226\nEpoch 12/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5933\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6424\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.6472 - val_loss: 0.6897\nEpoch 13/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6895\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6452 - val_loss: 0.6262\nEpoch 14/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7677\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6737\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6392 - val_loss: 0.5982\nEpoch 15/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6739\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6434\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6338 - val_loss: 0.7201\nEpoch 16/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5189\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6285\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6314 - val_loss: 0.6580\nEpoch 17/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6047\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6251\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6251 - val_loss: 0.6079\nEpoch 18/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7059\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6297 - val_loss: 0.5988\nEpoch 19/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5263\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6552\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6346\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 9ms/step - loss: 0.6256 - val_loss: 0.5952\nEpoch 20/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7430\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5890\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6128 - val_loss: 0.6089\nEpoch 21/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5202\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6058\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.5971\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6040 - val_loss: 0.6477\nEpoch 22/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6096\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6265 - val_loss: 0.6031\nEpoch 23/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.9461\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6171\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6186\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6150 - val_loss: 0.5730\nEpoch 24/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4095\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5677\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6100\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6100 - val_loss: 0.5558\nEpoch 25/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7707\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6068\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6060 - val_loss: 0.5620\nEpoch 26/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6338\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6069\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6069 - val_loss: 0.5775\nEpoch 27/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5161\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5934\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6092 - val_loss: 0.5863\nEpoch 28/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 7ms/step - loss: 0.6076 - val_loss: 0.5754\nEpoch 29/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6091 - val_loss: 0.5681\nEpoch 30/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4845\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6135\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6135 - val_loss: 0.5532\nEpoch 31/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5905\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6109 - val_loss: 0.5416\nEpoch 32/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5974\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6079\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6098 - val_loss: 0.5758\nEpoch 33/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.6011\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 9ms/step - loss: 0.6081 - val_loss: 0.5309\nEpoch 34/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6110 - val_loss: 0.5876\nEpoch 35/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7584\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6109\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6078\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6078 - val_loss: 0.6195\nEpoch 36/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6028 - val_loss: 0.5826\nEpoch 37/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6251\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6019 - val_loss: 0.5842\nEpoch 38/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6102\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6102 - val_loss: 0.5432\nEpoch 39/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8066\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.5935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.5935 - val_loss: 0.5806\nEpoch 40/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7397\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6247\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 8ms/step - loss: 0.6062 - val_loss: 0.5837\n\n\n\n\nCode\nhistory_plot(history_spy_2_noreg)\n\n\n\n\n\nWe can see the validation loss of the simple RNN model is actually almost equivalent to both the regularized and non-regularized GRU models above. Specifically, all 3 models tended to initialize around 0.6 loss for the validation set, and then slowly work down to about 0.55. If anything, the RNN model might perform slightly better, with validation loss decreasing below 0.55 at the later-stage epochs."
  },
  {
    "objectID": "Deep Learning for TS.html#fit-lstm",
    "href": "Deep Learning for TS.html#fit-lstm",
    "title": "Deep Learning for TS",
    "section": "Fit LSTM",
    "text": "Fit LSTM\n\n\nCode\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.LSTM(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_3_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10)\n\n\nEpoch 1/40\n 1/20 [&gt;.............................] - ETA: 43s - loss: 0.7877\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.7603 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 3s 27ms/step - loss: 0.6872 - val_loss: 0.5454\nEpoch 2/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7942\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 13ms/step - loss: 0.6526 - val_loss: 0.5501\nEpoch 3/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.7185\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 14ms/step - loss: 0.6419 - val_loss: 0.5392\nEpoch 4/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5006\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6195\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6423\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6310\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6314\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 17ms/step - loss: 0.6376 - val_loss: 0.5396\nEpoch 5/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8413\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6427\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6404\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 14ms/step - loss: 0.6348 - val_loss: 0.5340\nEpoch 6/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6472\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 14ms/step - loss: 0.6290 - val_loss: 0.5336\nEpoch 7/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6678\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5937\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6464\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6205\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 14ms/step - loss: 0.6210 - val_loss: 0.5432\nEpoch 8/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6196\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5932\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6498\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6201\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 13ms/step - loss: 0.6211 - val_loss: 0.5688\nEpoch 9/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6385\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6450\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.6135\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.6208\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 13ms/step - loss: 0.6220 - val_loss: 0.5806\nEpoch 10/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7201\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6249\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6283\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 12ms/step - loss: 0.6175 - val_loss: 0.5604\nEpoch 11/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5906\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.6144\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 13ms/step - loss: 0.6156 - val_loss: 0.5762\nEpoch 12/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5794\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.5728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 15ms/step - loss: 0.6116 - val_loss: 0.5549\nEpoch 13/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7079\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.6095\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6236\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 13ms/step - loss: 0.6122 - val_loss: 0.5362\nEpoch 14/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6223\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 12ms/step - loss: 0.6104 - val_loss: 0.5471\nEpoch 15/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6308\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/20 [=================&gt;............] - ETA: 0s - loss: 0.6415\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6219\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 13ms/step - loss: 0.6115 - val_loss: 0.5530\nEpoch 16/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5780\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 12ms/step - loss: 0.6096 - val_loss: 0.5351\nEpoch 17/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.5961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 11ms/step - loss: 0.5991 - val_loss: 0.5613\nEpoch 18/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6404\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6089\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.5970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 11ms/step - loss: 0.5970 - val_loss: 0.5522\nEpoch 19/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6673\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6078\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6048\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.5993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 12ms/step - loss: 0.6012 - val_loss: 0.5341\nEpoch 20/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6258\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5978\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.5789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 14ms/step - loss: 0.5999 - val_loss: 0.5657\nEpoch 21/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7228\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.5996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 14ms/step - loss: 0.6041 - val_loss: 0.5588\nEpoch 22/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5966\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.5893\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 14ms/step - loss: 0.6014 - val_loss: 0.5582\nEpoch 23/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6150\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6148\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.5984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 14ms/step - loss: 0.5998 - val_loss: 0.5483\nEpoch 24/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5683\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5887\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5898\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.5893\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.5984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 15ms/step - loss: 0.5984 - val_loss: 0.5511\nEpoch 25/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6835\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6355\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.5957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 15ms/step - loss: 0.6003 - val_loss: 0.5481\nEpoch 26/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6784\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5867\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.6014\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 15ms/step - loss: 0.6014 - val_loss: 0.5668\nEpoch 27/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6421\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5753\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5671\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5894\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5984 - val_loss: 0.5369\nEpoch 28/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5834\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5925\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.5880\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.6028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5952 - val_loss: 0.5602\nEpoch 29/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6160\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6146\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5978 - val_loss: 0.5516\nEpoch 30/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5806\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6118\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5948\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.6007 - val_loss: 0.5601\nEpoch 31/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.3989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5292\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.5744\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.5986\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5906 - val_loss: 0.5474\nEpoch 32/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.3922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.5737\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.5931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.5884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5907 - val_loss: 0.5686\nEpoch 33/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6421\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.5843\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.5936\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 15ms/step - loss: 0.5954 - val_loss: 0.5354\nEpoch 34/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.9139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6357\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5959\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5910 - val_loss: 0.5526\nEpoch 35/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.6863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.6304\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.6025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5948 - val_loss: 0.5326\nEpoch 36/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5954\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5940 - val_loss: 0.5486\nEpoch 37/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5323\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5557\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5910\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/20 [=======================&gt;......] - ETA: 0s - loss: 0.6033\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - ETA: 0s - loss: 0.5940\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5940 - val_loss: 0.5550\nEpoch 38/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6002\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5954\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5954 - val_loss: 0.5345\nEpoch 39/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6374\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/20 [========&gt;.....................] - ETA: 0s - loss: 0.5810\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/20 [==============&gt;...............] - ETA: 0s - loss: 0.6263\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/20 [====================&gt;.........] - ETA: 0s - loss: 0.5847\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/20 [==========================&gt;...] - ETA: 0s - loss: 0.5740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5841 - val_loss: 0.5608\nEpoch 40/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5815\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6136\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5965\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5754\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 0s 16ms/step - loss: 0.5916 - val_loss: 0.5547\n\n\n\n\nCode\nhistory_plot(history_spy_3_noreg)\n\n\n\n\n\nWith one LSTM layer, the model already seems better than the RNN and GRU competitiors. The validation loss starts out around 0.56 and stays in the lower end of the 0.55-0.6 range. Let’s try adding another layer:\n\n\nCode\n#from sklearn.neural_network import MLPRegressor\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer= regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer = regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer = regularizers.L1L2(l1=L1, l2=L2)))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(32, activation='relu',kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.Dense(1))\n#model.add(MLPRegressor())\n\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\n#  trying diff shape\n\nhistory_spy_4_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10)\n\n\nEpoch 1/40\n 1/20 [&gt;.............................] - ETA: 2:01 - loss: 0.9702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.9227  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.9035\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.8942\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.9107\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.9312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.9103\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.8930\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.9109\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.9050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 8s 60ms/step - loss: 0.8936 - val_loss: 0.7515\nEpoch 2/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7675\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.8306\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.8582\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.8970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.8705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.8795\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.8585\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.8330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.8335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.8348\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 36ms/step - loss: 0.8341 - val_loss: 0.7288\nEpoch 3/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7627\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.8584\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.8263\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.8263\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.8182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.8017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.8159\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.8298\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.8156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.8050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 33ms/step - loss: 0.8021 - val_loss: 0.7174\nEpoch 4/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6915\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6982\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7510\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7557\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.7689\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 34ms/step - loss: 0.7770 - val_loss: 0.6923\nEpoch 5/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.9002\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.8370\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.8809\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.8350\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7953\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.8034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7856\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.7780\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7682\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 36ms/step - loss: 0.7605 - val_loss: 0.6633\nEpoch 6/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6912\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7422\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7265\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7098\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7240\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.7356\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7308\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 35ms/step - loss: 0.7342 - val_loss: 0.7457\nEpoch 7/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7273\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.7286\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7280\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 33ms/step - loss: 0.7286 - val_loss: 0.6483\nEpoch 8/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7305\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6752\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6382\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6958\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7087\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6862\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.7139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.7226\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.7224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 32ms/step - loss: 0.7231 - val_loss: 0.6213\nEpoch 9/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6411\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6673\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6509\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6840\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6752\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6691\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6784\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6897\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6946\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 32ms/step - loss: 0.7017 - val_loss: 0.6281\nEpoch 10/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6331\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6482\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7190\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7165\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.7069\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.7061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.7027\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6999\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 32ms/step - loss: 0.6953 - val_loss: 0.6136\nEpoch 11/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6416\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6473\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 34ms/step - loss: 0.6878 - val_loss: 0.6329\nEpoch 12/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6817\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6484\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6795\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6824\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6815\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 36ms/step - loss: 0.6722 - val_loss: 0.5766\nEpoch 13/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8347\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.8735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.8098\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7321\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.7122\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 36ms/step - loss: 0.6735 - val_loss: 0.5790\nEpoch 14/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5201\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7035\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6828\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6808\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6827\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 34ms/step - loss: 0.6557 - val_loss: 0.5943\nEpoch 15/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7952\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6166\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5791\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5570\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5836\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6126\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6237\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6338\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6343\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 34ms/step - loss: 0.6418 - val_loss: 0.6089\nEpoch 16/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6338\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6264\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6232\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6411\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6315\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6552\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6481\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6421\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6377\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 35ms/step - loss: 0.6439 - val_loss: 0.5948\nEpoch 17/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7144\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6316\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6417\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6419\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 36ms/step - loss: 0.6413 - val_loss: 0.5656\nEpoch 18/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7367\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7217\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6614\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6212\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6371\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6265\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6205\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6293\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6476\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 35ms/step - loss: 0.6454 - val_loss: 0.5686\nEpoch 19/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6038\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5871\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6226\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6329\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6558\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6430\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6416\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6289\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6347 - val_loss: 0.6075\nEpoch 20/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5419\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6233\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6280\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6205\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6076\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6106\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6167\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6142\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6217\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 38ms/step - loss: 0.6339 - val_loss: 0.6081\nEpoch 21/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5417\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6647\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6533\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6504\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6598\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6337\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6306\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 38ms/step - loss: 0.6383 - val_loss: 0.5646\nEpoch 22/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.7242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6397\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6355\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 37ms/step - loss: 0.6346 - val_loss: 0.5717\nEpoch 23/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6613\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6288\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6319\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6290\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6295\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6314\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6342\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6245\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 40ms/step - loss: 0.6254 - val_loss: 0.6151\nEpoch 24/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8120\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.8504\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6397\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6413\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6398\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6338\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6323 - val_loss: 0.5940\nEpoch 25/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8496\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.7566\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.7237\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6809\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6460\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6346\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6350\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6360\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 40ms/step - loss: 0.6279 - val_loss: 0.5931\nEpoch 26/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6706\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6673\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6398\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6472\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6338\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6319\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 42ms/step - loss: 0.6227 - val_loss: 0.5534\nEpoch 27/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5503\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.4878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5121\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5640\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5622\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6222\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6213\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6183\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6277\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 40ms/step - loss: 0.6250 - val_loss: 0.5528\nEpoch 28/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5616\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5365\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5781\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6045\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6176\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6230\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6237\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6207 - val_loss: 0.5964\nEpoch 29/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5272\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5351\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5543\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5589\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5753\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6012\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6292\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 40ms/step - loss: 0.6253 - val_loss: 0.5589\nEpoch 30/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6350\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6394\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6467\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6466\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6363\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6207 - val_loss: 0.5622\nEpoch 31/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5119\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5395\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5486\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5710\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6114\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6225\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6150 - val_loss: 0.5762\nEpoch 32/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6557\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6126\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6361\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6179\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6143\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6194 - val_loss: 0.5594\nEpoch 33/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.5470\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6713\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6497\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6340\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6319\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6322\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6198\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6167\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6095 - val_loss: 0.5765\nEpoch 34/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6364\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6230\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6331\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6081\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6338\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6266\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6121 - val_loss: 0.5806\nEpoch 35/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.7443\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6474\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6584\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6700\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6474\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6271\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6187\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6154 - val_loss: 0.5783\nEpoch 36/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6817\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6639\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6158\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5967\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.5954\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 40ms/step - loss: 0.6114 - val_loss: 0.5445\nEpoch 37/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.8976\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.6577\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6309\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6141\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5786\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5962\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.5996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.5990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 40ms/step - loss: 0.6026 - val_loss: 0.5567\nEpoch 38/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6589\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5445\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6073\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5818\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.5910\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6152\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6086 - val_loss: 0.5461\nEpoch 39/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.6587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5656\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.6076\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.6009\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.5970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.5927\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.5904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6002\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.5993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6096 - val_loss: 0.5226\nEpoch 40/40\n 1/20 [&gt;.............................] - ETA: 0s - loss: 0.4451\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/20 [===&gt;..........................] - ETA: 0s - loss: 0.5174\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/20 [======&gt;.......................] - ETA: 0s - loss: 0.5639\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/20 [=========&gt;....................] - ETA: 0s - loss: 0.5768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/20 [============&gt;.................] - ETA: 0s - loss: 0.6064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/20 [===============&gt;..............] - ETA: 0s - loss: 0.6041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/20 [==================&gt;...........] - ETA: 0s - loss: 0.6012\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/20 [=====================&gt;........] - ETA: 0s - loss: 0.6196\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/20 [========================&gt;.....] - ETA: 0s - loss: 0.6099\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/20 [===========================&gt;..] - ETA: 0s - loss: 0.6094\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/20 [==============================] - 1s 39ms/step - loss: 0.6070 - val_loss: 0.5725\n\n\n\n\nCode\nspy_4_t_pred = model.predict(spy_x)\nspy_4_t_pred = spy_4_t_pred\n\nspy_4_v_pred = model.predict(spy_val_x)\nspy_4_v_pred.shape\n\n\n 1/15 [=&gt;............................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/15 [=================&gt;............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/15 [==============================] - 1s 7ms/step\n1/3 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3/3 [==============================] - 0s 7ms/step\n\n\n(69, 31, 1)\n\n\n\n\nCode\nhistory_plot(history_spy_4_noreg)\n\n\n\n\n\nWith a much larger model of 3x64 neuron LSTM layers, we can see that the validation loss is still around 55. However, the training loss has a consistent curve down to the validation loss. It is interesting that even in the later epochs, the validation loss remains lower than the training."
  },
  {
    "objectID": "Deep Learning for TS.html#comparing-results-of-deep-learning-models",
    "href": "Deep Learning for TS.html#comparing-results-of-deep-learning-models",
    "title": "Deep Learning for TS",
    "section": "Comparing Results of Deep Learning Models",
    "text": "Comparing Results of Deep Learning Models\nOf the three neural network models, the LSTM provided the most consistent performance, while the RNN provided the best performance. The GRU model performed slightly worse than the other two, as measured by its validation set performance. Interestingly, the complexity required for the model to avoid underfitting was much higher with the LSTM model, where 3 hidden layers of 64 neurons each were needed before I felt the training plot showed excess training performance over validation performance. With smaller models, the LSTM performed somewhat poorly on the training set in comparison to the validation set. In terms of accuracy, the RNN model was notable for sometimes reaching below 0.55 loss in performance on the validation set, while the LSTM model usually reached around 0.56 and the GRU model between 0.56 and 0.6. In terms of predictive power (extrapolating outside the validation set) I would tend to trust the LSTM model, because it had the most consistent performance once it was trained over a number of epochs, and even boasted the lowest loss on a particular epoch, sometimes reaching as low as 0.51 in the validation set, although these results were not the norm.\nRegularization was an important element across the models. It had two main benefits: First, the models achieved better performance faster because they had more consistent improvement and less variation in performance between epochs. Models which included regularization (both L1L2 and dropout regularization) had almost monotonically decreasing loss in the training set, and clear trends toward improvement in the validation set.\nThe deep learning models will only have the performance described above when they predict 1 observation into the future. This is because they take in the past 30 observations and return 1 prediction. All of the loss estimates are for this 1-step ahead prediction. If we were to predict further into the future with the models, there would be drift or compound error, where the models loss from a previous prediction would be inherited in future predictions that relied upon that past prediction as an input into the model.\nThe deep learning modelling is more complicated than the univariate modeling from HW3, because there are a range of hyperparameters introduced that make the process of selecting the best model more complicated. Unlike a simple ARIMA model, it is not possible to iterate through all combinations of hidden layer sizes and complexities. As such, I think there is more subjective elements introduced to neural network modeling."
  },
  {
    "objectID": "Deep Learning for TS.html#comparing-deep-learning-and-traditional-ts-models",
    "href": "Deep Learning for TS.html#comparing-deep-learning-and-traditional-ts-models",
    "title": "Deep Learning for TS",
    "section": "Comparing Deep Learning and Traditional TS Models",
    "text": "Comparing Deep Learning and Traditional TS Models\nWe identified ARIMA(3,0,3) as a good model for the SPY data. Lets look at the one step ahead precition of the ARIMA and best-performing (LSTM) neural network model in the validation data:\n\n\nCode\nfrom statsmodels.tsa.arima.model import ARIMA\n\nmodelA = ARIMA(spy_train, order = (3,0,3))\nmodelA = modelA.fit()\n\nforecasts_arima = modelA.forecast(100)\n\npredict_arima = ARIMA(spy_val, order = (3,0,3))\npredict_arima = predict_arima.fit()\n\narimapreds = list()\n\n\n\nindexvals = pd.array(range(0,69))\narimavals = pd.array(predict_arima.get_prediction().predicted_mean)\narimavals = arimavals[0:69]\nnnvals = spy_4_v_pred[:,0,0]\n\n\nplt.plot(indexvals, arimavals, label='ARIMA Prediction')\nplt.plot(indexvals, nnvals, label='NN Prediction')\nplt.plot(indexvals, spy_val_y, label='Real Values')\nplt.legend()\nplt.title(\"One Step Ahead Forecasts in Validation (NN vs. ARIMA)\")\nplt.show()\n\nrealVals = spy_val_y\n\ndef rmse(pred, tar):\n    return np.sqrt(((pred - tar) ** 2).mean())\n\n\nprint(\"arima: \", rmse(arimavals, spy_val_y))\nprint(\"Neural Net: \", rmse(nnvals, spy_val_y))\n\n\nC:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\nC:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n\n\n\n\n\narima:  0.776142384126778\nNeural Net:  0.7071590843066736\n\n\nLooking at a chart of our best-performing Neural Network Model (LSTM) vs. my best performing ARIMA model (3,0,3) from earlier work, vs. the real observed values, a few things become clear. One, neither model type does a perfect job, especially of capturing extremely high or low values. Two, the ARIMA predictions increase and decrease more (have a higher range) thant he NN predictions. However, the ARIMA predictions tend to vary further from 0, while the NN predictions stay close to zero and only alter slightly. Because of this, the RSME of the neural network is actually lower than the ARIMA model, with the neural network having 0.71 RMSE and the ARIMA model having 0.78 RMSE.\nHowever, the neural network is still not the best model I have tested in this project, because the VAR model achieved an RMSE of 0.68 on the same dataset. This suggests that while Neural networks are powerful tools for univariate prediction, the most accurate model predictions may still be those that include exogenous variables."
  },
  {
    "objectID": "Data Sources.html#united-states-stock-indices---intraday-volatility",
    "href": "Data Sources.html#united-states-stock-indices---intraday-volatility",
    "title": "Data Sources",
    "section": "",
    "text": "Financial data on the prices of different stocks are readily available in both R and Python through various packages, and amongst these the Yahoo Finance package is a particularly popular option. To approximate the daily range in value of major indices, I chose to look at popular low-cost ETFs which attempt to track the value of stocks contained in these indices. For the S&P 500, this is SPY, run by State Street, for the Russel 2000 this is iShare’s IWM, and for the NASDAQ 100 it is the ever-popular Invesco QQQ fund. The data I haave gathered for these tickers includes the open and close of stock prices for each day, as well as the highest and lowest price recorded for the day. These indices are widely used and will be a relevant outcome variable for the project to study. The main output value that will be calculated is the high-low range of the day.\n\n\n\nSPY Historical Data\nIWM Historical Data\nQQQ Historical Data"
  },
  {
    "objectID": "Data Sources.html#investor-confidence---cboes-vix",
    "href": "Data Sources.html#investor-confidence---cboes-vix",
    "title": "Data Sources",
    "section": "Investor Confidence - CBOE’s VIX",
    "text": "Investor Confidence - CBOE’s VIX\nhttps://finance.yahoo.com/quote/%5EVIX/history?p=%255EVIX\nThe Chicago Board of Exchange (CBOE) VIX index is a widely-used tool to measure investor sentiment. The indicator itself represents the degree of volatility perceived by investors in the next month. It ranges from near zero up to about 60 in recent years, with scores at different intervals representing different levels of perceived volatility.\n\nSources\n\nMethodology of the VIX\nVIX Historical Data"
  },
  {
    "objectID": "Data Sources.html#daily-3-month-yield-curve",
    "href": "Data Sources.html#daily-3-month-yield-curve",
    "title": "Data Sources",
    "section": "Daily 3 Month Yield Curve",
    "text": "Daily 3 Month Yield Curve\nWhile I originally considered the actual federal funds rate, the values did not change much day to day. Instead I would like to use the expectation of yield rates, measured through the pricing of bonds in the yield curve over the next few months. The data includes the prices of treasury bills 1, 2, and 3 months out on each date.\n\nSources\n\nTreasury Department Data"
  },
  {
    "objectID": "Data Visualization.html#include-if-desired",
    "href": "Data Visualization.html#include-if-desired",
    "title": "Data Visualization",
    "section": "Include if desired",
    "text": "Include if desired"
  },
  {
    "objectID": "Data Visualization.html#introduction",
    "href": "Data Visualization.html#introduction",
    "title": "Data Visualization",
    "section": "",
    "text": "The central question of this research project is about stock market volatility. As such the following visualizations will explore this subject in different areas. To start with, let’s define the outcome variables in question:"
  },
  {
    "objectID": "Data Visualization.html#financial-data",
    "href": "Data Visualization.html#financial-data",
    "title": "Data Visualization",
    "section": "Financial Data",
    "text": "Financial Data\nFor each security: S&P 500, QQQ Pro-Shares ETF, and Russell 200 index, we have several metrics 1. Price 2. Standard deviation (10 day window) 3. True range (intraday difference between highest and lowest price/previous closing value) 4. Trading volume\nThe window of our investigation is January 1st 2021 - September 30th 2023.\nLet’s start by simply seeing the absolute prices of these securities over time:\n\n\n\n\n\nIn these charts, we can see that the overall trend for all three securities has been somewhat of a decline since the start of the time window. We can also see that QQQ and IWM have moved further in their trends than SPY, which makes sense as that is the most stable index of the three.\nNow, let us look at the intraday range measure of volatility, which is expressed as a percent of the opening price:\n\n\nNULL\n\n\n\n\n\nIn this chart, we can see that the three indices tended to move together, with the midpoint of the time window having a higher average range in daily prices than the beginning or end for all of the securities. However, we can also see again that volatility is not even between them, with SPY and IWM having higher daily ranges than QQQ on average (at least according to the naked eye)."
  },
  {
    "objectID": "Data Visualization.html#year-bond-yields",
    "href": "Data Visualization.html#year-bond-yields",
    "title": "Data Visualization",
    "section": "3-Year Bond Yields",
    "text": "3-Year Bond Yields\nNow, let’s review the trends in the federal funds rate:\n\n\n\n\n\n\nBy reviewing the data, we can see that the 1, 3, and 6 month rates on bonds tend to move together. However, the shorter length bonds, namely the 1 month rate, have the greatest volatility. We can also see that these variables will be tricky to include in our analsis, as they remained near zero for the first half of the dataset, before jumping up quickly in mid 2022."
  },
  {
    "objectID": "Data Visualization.html#labor-data",
    "href": "Data Visualization.html#labor-data",
    "title": "Data Visualization",
    "section": "Labor Data",
    "text": "Labor Data\nLet’s examine the strikes that we have contained in our labor dataset. First let’s look at the number of strikes by year:\n\n\n\n\n\nWe can see that the number of strikes varied year-to-year, although 2023 was not complete in the dataset. But what if we looked at the overlap in strikes, and the number of workers striking concurrently:\n\n\n\n\n\n\nWe can see that the number of striking concurrent workers has varied overtime, but it should be an interesting variable to study as there are definite peaks and troughs in the dataset. A few particular strikes had a big impact but lasted only a few days, such as the teachers strikes in Portland and Los Angeles, which each lasted less than a week."
  },
  {
    "objectID": "Data Visualization.html#weather-data",
    "href": "Data Visualization.html#weather-data",
    "title": "Data Visualization",
    "section": "Weather Data",
    "text": "Weather Data\nFinally, let’s see what our weather data for this period looks like:\n\n\n\n\n\n\n\n\nLooking at the first weather data chart, we can see that the total number of weather events appears to have some weak seasonality (which makes sense since weather is seasonal). The lowest months are September through November, and the highest are December through August (winter + hurricane season).\nIn the second chart, we can see that total property damage and casualties move together. Unlike the total number of storms, the data has more heteroskedacticity and less seasonality, suggesting these numbers are more volatile and linked to individual, severe storms. However, the two trends are associated, with periods of elevated property damage having higher casualties and vice versa."
  },
  {
    "objectID": "Exploratory Data Analysis.html#time-series-analysis",
    "href": "Exploratory Data Analysis.html#time-series-analysis",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Let’s start with reading in the intraday range data. First with the S&P 500.\n\n\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(forecast)\nlibrary(ggplot2)\n\n\n\n\nCode\nspyIn &lt;- quantmod::getSymbols(\"SPY\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\nqqqIn &lt;- quantmod::getSymbols(\"QQQ\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\niwmIn &lt;- quantmod::getSymbols(\"IWM\", from = as.Date(\"2021/01/01\"), to = as.Date(\"2023/09/30\"), periodicity = \"daily\", src = \"yahoo\", auto.assign = FALSE)\n\n\nspyIn$spyRange &lt;- (spyIn$SPY.High - spyIn$SPY.Low)/ spyIn$SPY.Open\nqqqIn$qqqRange &lt;- (qqqIn$QQQ.High - qqqIn$QQQ.Low)/ qqqIn$QQQ.Open\niwmIn$iwmRange &lt;- (iwmIn$IWM.High - iwmIn$IWM.Low)/ iwmIn$IWM.Open\n\n\n#decomposedSPY &lt;- decompose(spyIn$spyRange)\n\n##decompedSPY = HoltWinters(spyIn$spyRange,beta = FALSE,gamma = FALSE)\n#plot(decompedSPY)\n\nautoplot(spyIn$spyRange)\n\n\n\n\n\nCode\nacf(spyIn$spyRange)\n\n\n\n\n\nCode\npacf(spyIn$spyRange)\n\n\n\n\n\nUpon initial review of the SPY intraday range data, it appears that there is some trend, but no seasonality. The decomposition function would not work on the data as it could not recognize periodicity. Let’s review the lag plots of the data for SPY, QQQ, and IWM:\n\n\nCode\nspyLagPlot &lt;- gglagplot(spyIn$spyRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, SPY Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n\nqqqLagPlot &lt;- gglagplot(qqqIn$qqqRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, QQQ Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n  \n\niwmLagPlot &lt;- gglagplot(iwmIn$iwmRange, do.lines = FALSE, set.lags = c(1, 5, 10, 20)) + \n  ggtitle('Lag Plots, IWM Daily Range') + labs(x = 'Intraday Range Lagged', y = 'Intraday Range Current')  +  theme(text = element_text(size=12))\n\n\nspyLagPlot\n\n\n\n\n\nCode\nqqqLagPlot\n\n\n\n\n\nCode\niwmLagPlot\n\n\n\n\n\nWhile these lag plots do not show a clear trend, they do show clear heteroskedacticity. We can see that the ranges are always positive, so this already means the distribution is one-sided. However, the positive ends of the spectrum (a higher percent range for the day) are much further spread out than those values closer to zero. If the data didn’t always have to be positive, I could see the plots looking like Gaussian white noise (i.e. imagine the lag plots visualize, but with each one rotated to all 4 quadrants instead of +X/+Y and you would have a near-Gaussian distribution).\nAs a next step, to study these relationships with the lag plots further, we can difference the data.\n\n\nCode\ndiff1 &lt;- diff(spyIn$spyRange)\ndiffqqq &lt;- diff(qqqIn$qqqRange)\ndiffiwm &lt;- diff(iwmIn$iwmRange)\n\n\nggtsdisplay(diff1, main = \"Differenced SPY Intraday Daily Range\")\n\n\n\n\n\nCode\nggtsdisplay(diffqqq, main = \"Differenced QQQ Intraday Daily Range\")\n\n\n\n\n\nCode\nggtsdisplay(diffiwm, main = \"Differenced IWM Intraday Daily Range\")\n\n\n\n\n\nAfter differencing, the data appears to have lost its trend. Moverover, the first lag in the ACF plot has very strong correlation, suggesting we may have over-differenced. However, there is still visible heteroskedacticity in the differences between intraday ranges. These patterns are essentially the same between the three charts.\n\n\nCode\nacf(diff1,    na.action = na.exclude)\n\n\n\n\n\nCode\npacf(diff1,    na.action = na.exclude)\n\n\n\n\n\nAfter differencing, we see 2 lags being significant in the ACF plot. In the PACF plot, we see about 4 lags being significant. This is a marked departure from the original plots, where the ACF showed clear non-stationarity and the PACF had many significant lags.\n\n\nCode\nlibrary(tseries)\n\ndiffnoNA &lt;- diff1$spyRange[!is.na(diff1$spyRange)]\nadf.test(diffnoNA)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diffnoNA\nDickey-Fuller = -12.455, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\n#\n\n\nWith dickey fuller test result of 0.01, we can reject the null hypothesis and conclude the series is stationary.\n\n\n\n\nCode\nspyDiff1 &lt;- as.ts(diffnoNA)\n\nspyAvg3Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/3, 3))\nspyAvg5Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/5, 5))\nspyAvg30Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/30, 30))\nspyAvg50Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/50, 50))\n\nspyAvg100Diff1 &lt;- stats::filter(spyDiff1, sides = 1, rep(1/100, 100))\n\n\nautoplot(spyAvg3Diff1)\n\n\n\n\n\nCode\nautoplot(spyAvg5Diff1)\n\n\n\n\n\nCode\nautoplot(spyAvg30Diff1)\n\n\n\n\n\nCode\nautoplot(spyAvg50Diff1)\n\n\n\n\n\nCode\nautoplot(spyAvg100Diff1)\n\n\n\n\n\nOverall the effect of the moving average at higher numbers is to reveal seasonality in the data. At lower smoothing levels (3 and 5 days), the time series looked almost unchanged, and still highly variant. Only at a 30 day average window did the seasonality start to appear, with clear periods in the data and a repeating pattern. Notably, even with a repeating pattern the data still showed obvious heteroskedasticity, with periods of increased varianced. The 50 and 100 day moving average windows began to obfuscate the periods, while keeping the heteroskedacticity, although they did have smaller variations and were nearer to 0 on average. In adition, the 50 and 100 days had discernable trends in the data."
  }
]