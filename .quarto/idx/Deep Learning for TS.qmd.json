{"title":"Deep Learning for TS","markdown":{"yaml":{"title":"Deep Learning for TS"},"headingText":"Declare Useful Functions","containsRefs":false,"markdown":"\n\n\n```{python}\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nimport subprocess\n\nimport sys\nimport os \n\ndef install(name):\n    subprocess.call([sys.executable, '-m', 'pip', 'install', name])\n\n#install('yfinance')\n```\n\n\n\nHere, I would like to mention that this code is based on the deep learning lab code we went over in class. I have adapted it here to apply it to my data, with modifications in some areas as my data functions slightly differently than the examples in the lab.\n\n```{python}\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\n```\n\n\n\n## Read in Data\n\n```{python}\n\nimport yfinance as yf\n\nspy = yf.download('SPY',start = \"2021-01-01\", end = \"2023-09-30\")\niwm = yf.download('QQQ',start = \"2021-01-01\", end = \"2023-09-30\")\nqqq = yf.download('IWM',start = \"2021-01-01\", end = \"2023-09-30\")\n\nspyRange = (spy['High'] - spy['Low']) / spy['Close']\niwmRange = (iwm['High'] - iwm['Low']) / iwm['Close']\nqqqRange = (qqq['High'] - qqq['Low']) / qqq['Close']\n\n```\n\n## Normalize\n\n```{python}\n\n\n\n# Slice training set\nspy_train = pd.DataFrame( spyRange[range(0,490)] )\nspy_train = spy_train.reset_index(drop = True)\n\n# Slice validation set\nspy_val =  pd.DataFrame( spyRange[range(490,590)] )\nspy_val = spy_val.reset_index(drop = True)\n\n# Slice test set\nspy_test =  pd.DataFrame( spyRange[range(590,690)] )\nspy_test = spy_test.reset_index(drop = True)\n\n# Save training normalization to use on test and validation sets\nspy_train_mean = spy_train.mean()\nspy_train_std = spy_train.std()\n\n# Normalize\nspy_train=(spy_train-spy_train_mean)/spy_train_std\nspy_val=(spy_val-spy_train_mean)/spy_train_std\nspy_test=(spy_test-spy_train_mean)/spy_train_std\n\n\n\n```\n\n\n## Fit Helper Function to Create Data in Desired Format \n\n```{python}\n# code from lab\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample:\n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n    # initialize\n    i_start=0; count=0;\n\n    # initialize output arrays with samples\n    x_out=[]\n    y_out=[]\n\n    # sequentially build mini-batch samples\n    while i_start+lookback+delay< x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n    \n        # report if desired\n        if verbose and count<2: print(\"indice range:\",i_start,i_stop,\"-->\", i_pred)\n\n        # define arrays:\n        # method-1: buggy due to indexing from left\n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while j>=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n        \n        # create mini-batch sample\n        xtmp=x.iloc[indices_to_keep,:] # isolate relevant indices\n        \n        xtmp=xtmp.iloc[:,feature_columns] # isolate desire features\n        ytmp=x.iloc[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp);\n        \n        # report if desired\n        if verbose and count<2: print(xtmp, \"-->\",ytmp)\n        if verbose and count<2: print(\"shape:\",xtmp.shape, \"-->\",ytmp.shape)\n        \n        if verbose and count<2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n        \n        # UPDATE START POINT\n        if unique: i_start+=lookback\n        i_start+=1; count+=1\n\n    return np.array(x_out),np.array(y_out)\n\n```\n\n\nDiagnostic plotting functions from the lab\n```{python}\n\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n\n# UTILITY FUNCTION\ndef regression_report(yt,ytp,yv,yvp):\n\n    print(\"---------- Regression report ----------\")\n\n    print(\"TRAINING:\")\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n    title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]);\n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n    print(\"VALIDATION:\")\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n    title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\n\n\n```\n\n\nUse function to reformat data. Train will be 1-490, Validation 491-590, and Test 591-690\n\n```{python}\n\n\n\n#spy_train = generator(spyRange, lookback=lookback, delay=delay, min_index=0, max_index=489, shuffle=True, step=step, batch_size=batch_size)\n#spy_val = generator(spyRange, lookback=lookback, delay=delay, min_index=490, max_index=589, step=step, batch_size=batch_size)\n#spy_test = generator(spyRange, lookback=lookback, delay=delay, min_index=590, max_index=690, step=step, batch_size=batch_size)\n\n\n\nL = 30\nS = 1\nD = 1\nbatch_size = 10 \n\n\nspy_x,spy_y=form_arrays(spy_train,lookback=L,delay=D,step=S,unique=False,verbose=False)\n\nspy_val_x, spy_val_y = form_arrays(spy_val,lookback=L,delay=D,step=S,unique=False,verbose=False)\n\n\n```\n\nReshape data\n```{python}\n\n# RESHAPE INTO A DATA FRAME\nspy_t_1 = spy_x.reshape(spy_x.shape[0],spy_x.shape[1]*spy_x.shape[2])\nspy_v_1 = spy_val_x.reshape(spy_val_x.shape[0],spy_val_x.shape[1]*spy_val_x.shape[2])\n\n\ninput_shape = (spy_t_1.shape[1],)\nrnn_input_shape = (spy_x.shape[1], spy_x.shape[2])\n\n# NEW SIZES\nprint(\"train: \", spy_x.shape,\"-->\",spy_t_1.shape)\nprint(\"validation: \", spy_val_x.shape,\"-->\",spy_v_1.shape)\n\n```\n\n\n\n\n\n## Fit GRU \n\nNo regularlization\n\n```{python}\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_1_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=30, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n```\n\nLet's plot results\n\n```{python}\nhistory_plot(history_spy_1_noreg)\n\n```\n\nRegularlization\n\n```{python}\n\nfrom tensorflow.keras import regularizers\nL1=0\nL2=1e-3\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape,kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n# layer that reads the recurent layer\nmodel.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5,kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_1_reg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=30, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n```\n\nLet's look at the diagnostic plot\n```{python}\n\nhistory_plot(history_spy_1_reg)\n```\n\n\nCompared to the model without regularization, we can see that the training set error decreases more steadily, without regressing higher very often. The consequence of this is that the raining error goes lower than the validation error faster than without regularization, suggesting regularization is helping the model to learn more purposefully and successfully.\n\n\n## Fit RNN \n\n```{python}\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.SimpleRNN(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_2_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n```\n\n\n```{python}\n\nhistory_plot(history_spy_2_noreg)\n```\n\nWe can see the validation loss of the simple RNN model is actually almost equivalent to both the regularized and non-regularized GRU models above. Specifically, all 3 models tended to initialize around 0.6 loss for the validation set, and then slowly work down to about 0.55. If anything, the RNN model might perform slightly better, with validation loss decreasing below 0.55 at the later-stage epochs.\n\n\n\n## Fit LSTM \n\n```{python}\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.LSTM(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_3_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n```\n\n```{python}\n\nhistory_plot(history_spy_3_noreg)\n\n```\n\nWith one LSTM layer, the model already seems better than the RNN and GRU competitiors. The validation loss starts out around 0.56 and stays in the lower end of the 0.55-0.6 range. Let's try adding another layer:\n\n```{python}\n#from sklearn.neural_network import MLPRegressor\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer= regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer = regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer = regularizers.L1L2(l1=L1, l2=L2)))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(32, activation='relu',kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.Dense(1))\n#model.add(MLPRegressor())\n\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\n#  trying diff shape\n\nhistory_spy_4_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n```\n\n```{python}\n\nspy_4_t_pred = model.predict(spy_x)\nspy_4_t_pred = spy_4_t_pred\n\nspy_4_v_pred = model.predict(spy_val_x)\nspy_4_v_pred.shape\n\n\n```\n\n```{python}\nhistory_plot(history_spy_4_noreg)\n```\n\n\nWith a much larger model of 3x64 neuron LSTM layers, we can see that the validation loss is still around 55. However, the training loss has a consistent curve down to the validation loss. It is interesting that even in the later epochs, the validation loss remains lower than the training.\n\n\n## Comparing Results of Deep Learning Models \n\nOf the three neural network models, the LSTM provided the most consistent performance, while the RNN provided the best performance. The GRU model performed slightly worse than the other two, as measured by its validation set performance. Interestingly, the complexity required for the model to avoid underfitting was much higher with the LSTM model, where 3 hidden layers of 64 neurons each were needed before I felt the training plot showed excess training performance over validation performance. With smaller models, the LSTM performed somewhat poorly on the training set in comparison to the validation set. In terms of accuracy, the RNN model was notable for sometimes reaching below 0.55 loss in performance on the validation set, while the LSTM model usually reached around 0.56 and the GRU model between 0.56 and 0.6. In terms of predictive power (extrapolating outside the validation set) I would tend to trust the LSTM model, because it had the most consistent performance once it was trained over a number of epochs, and even boasted the lowest loss on a particular epoch, sometimes reaching as low as 0.51 in the validation set, although these results were not the norm.\n\nRegularization was an important element across the models. It had two main benefits: First, the models achieved better performance faster because they had more consistent improvement and less variation in performance between epochs. Models which included regularization (both L1L2 and dropout regularization) had almost monotonically decreasing loss in the training set, and clear trends toward improvement in the validation set.\n\nThe deep learning models will only have the performance described above when they predict 1 observation into the future. This is because they take in the past 30 observations and return 1 prediction. All of the loss estimates are for this 1-step ahead prediction. If we were to predict further into the future with the models, there would be drift or compound error, where the models loss from a previous prediction would be inherited in future predictions that relied upon that past prediction as an input into the model.\n\nThe deep learning modelling is more complicated than the univariate modeling from  HW3, because there are a range of hyperparameters introduced that make the process of selecting the best model more complicated. Unlike a simple ARIMA model, it is not possible to iterate through all combinations of hidden layer sizes and complexities. As such, I think there is more subjective elements introduced to neural network modeling.\n\n\n\n## Comparing Deep Learning and Traditional TS Models \n\n\nWe identified ARIMA(3,0,3) as a good model for the SPY data. Lets look at the one step ahead precition of the ARIMA and best-performing (LSTM) neural network model in the validation data:\n\n```{python}\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nmodelA = ARIMA(spy_train, order = (3,0,3))\nmodelA = modelA.fit()\n\nforecasts_arima = modelA.forecast(100)\n\npredict_arima = ARIMA(spy_val, order = (3,0,3))\npredict_arima = predict_arima.fit()\n\narimapreds = list()\n\n\n\nindexvals = pd.array(range(0,69))\narimavals = pd.array(predict_arima.get_prediction().predicted_mean)\narimavals = arimavals[0:69]\nnnvals = spy_4_v_pred[:,0,0]\n\n\nplt.plot(indexvals, arimavals, label='ARIMA Prediction')\nplt.plot(indexvals, nnvals, label='NN Prediction')\nplt.plot(indexvals, spy_val_y, label='Real Values')\nplt.legend()\nplt.title(\"One Step Ahead Forecasts in Validation (NN vs. ARIMA)\")\nplt.show()\n\nrealVals = spy_val_y\n\ndef rmse(pred, tar):\n    return np.sqrt(((pred - tar) ** 2).mean())\n\n\nprint(\"arima: \", rmse(arimavals, spy_val_y))\nprint(\"Neural Net: \", rmse(nnvals, spy_val_y))\n```\n\n\nLooking at a chart of our best-performing Neural Network Model (LSTM) vs. my best performing ARIMA model (3,0,3) from earlier work, vs. the real observed values, a few things become clear. One, neither model type does a perfect job, especially of capturing extremely high or low values. Two, the ARIMA predictions increase and decrease more (have a higher range) thant he NN predictions. However, the ARIMA predictions tend to vary further from 0, while the NN predictions stay close to zero and only alter slightly. Because of this, the RSME of the neural network is actually lower than the ARIMA model, with the neural network having 0.71 RMSE and the ARIMA model having 0.78 RMSE.\n\nHowever, the neural network is still not the best model I have tested in this project, because the VAR model achieved an RMSE of 0.68 on the same dataset. This suggests that while Neural networks are powerful tools for univariate prediction, the most accurate model predictions may still be those that include exogenous variables.\n\n\n\n","srcMarkdownNoYaml":"\n\n\n```{python}\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nimport subprocess\n\nimport sys\nimport os \n\ndef install(name):\n    subprocess.call([sys.executable, '-m', 'pip', 'install', name])\n\n#install('yfinance')\n```\n\n\n## Declare Useful Functions \n\nHere, I would like to mention that this code is based on the deep learning lab code we went over in class. I have adapted it here to apply it to my data, with modifications in some areas as my data functions slightly differently than the examples in the lab.\n\n```{python}\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\n```\n\n\n\n## Read in Data\n\n```{python}\n\nimport yfinance as yf\n\nspy = yf.download('SPY',start = \"2021-01-01\", end = \"2023-09-30\")\niwm = yf.download('QQQ',start = \"2021-01-01\", end = \"2023-09-30\")\nqqq = yf.download('IWM',start = \"2021-01-01\", end = \"2023-09-30\")\n\nspyRange = (spy['High'] - spy['Low']) / spy['Close']\niwmRange = (iwm['High'] - iwm['Low']) / iwm['Close']\nqqqRange = (qqq['High'] - qqq['Low']) / qqq['Close']\n\n```\n\n## Normalize\n\n```{python}\n\n\n\n# Slice training set\nspy_train = pd.DataFrame( spyRange[range(0,490)] )\nspy_train = spy_train.reset_index(drop = True)\n\n# Slice validation set\nspy_val =  pd.DataFrame( spyRange[range(490,590)] )\nspy_val = spy_val.reset_index(drop = True)\n\n# Slice test set\nspy_test =  pd.DataFrame( spyRange[range(590,690)] )\nspy_test = spy_test.reset_index(drop = True)\n\n# Save training normalization to use on test and validation sets\nspy_train_mean = spy_train.mean()\nspy_train_std = spy_train.std()\n\n# Normalize\nspy_train=(spy_train-spy_train_mean)/spy_train_std\nspy_val=(spy_val-spy_train_mean)/spy_train_std\nspy_test=(spy_test-spy_train_mean)/spy_train_std\n\n\n\n```\n\n\n## Fit Helper Function to Create Data in Desired Format \n\n```{python}\n# code from lab\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample:\n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n    # initialize\n    i_start=0; count=0;\n\n    # initialize output arrays with samples\n    x_out=[]\n    y_out=[]\n\n    # sequentially build mini-batch samples\n    while i_start+lookback+delay< x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n    \n        # report if desired\n        if verbose and count<2: print(\"indice range:\",i_start,i_stop,\"-->\", i_pred)\n\n        # define arrays:\n        # method-1: buggy due to indexing from left\n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while j>=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n        \n        # create mini-batch sample\n        xtmp=x.iloc[indices_to_keep,:] # isolate relevant indices\n        \n        xtmp=xtmp.iloc[:,feature_columns] # isolate desire features\n        ytmp=x.iloc[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp);\n        \n        # report if desired\n        if verbose and count<2: print(xtmp, \"-->\",ytmp)\n        if verbose and count<2: print(\"shape:\",xtmp.shape, \"-->\",ytmp.shape)\n        \n        if verbose and count<2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n        \n        # UPDATE START POINT\n        if unique: i_start+=lookback\n        i_start+=1; count+=1\n\n    return np.array(x_out),np.array(y_out)\n\n```\n\n\nDiagnostic plotting functions from the lab\n```{python}\n\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n\n# UTILITY FUNCTION\ndef regression_report(yt,ytp,yv,yvp):\n\n    print(\"---------- Regression report ----------\")\n\n    print(\"TRAINING:\")\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n    title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]);\n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n    print(\"VALIDATION:\")\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n    title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\n\n\n```\n\n\nUse function to reformat data. Train will be 1-490, Validation 491-590, and Test 591-690\n\n```{python}\n\n\n\n#spy_train = generator(spyRange, lookback=lookback, delay=delay, min_index=0, max_index=489, shuffle=True, step=step, batch_size=batch_size)\n#spy_val = generator(spyRange, lookback=lookback, delay=delay, min_index=490, max_index=589, step=step, batch_size=batch_size)\n#spy_test = generator(spyRange, lookback=lookback, delay=delay, min_index=590, max_index=690, step=step, batch_size=batch_size)\n\n\n\nL = 30\nS = 1\nD = 1\nbatch_size = 10 \n\n\nspy_x,spy_y=form_arrays(spy_train,lookback=L,delay=D,step=S,unique=False,verbose=False)\n\nspy_val_x, spy_val_y = form_arrays(spy_val,lookback=L,delay=D,step=S,unique=False,verbose=False)\n\n\n```\n\nReshape data\n```{python}\n\n# RESHAPE INTO A DATA FRAME\nspy_t_1 = spy_x.reshape(spy_x.shape[0],spy_x.shape[1]*spy_x.shape[2])\nspy_v_1 = spy_val_x.reshape(spy_val_x.shape[0],spy_val_x.shape[1]*spy_val_x.shape[2])\n\n\ninput_shape = (spy_t_1.shape[1],)\nrnn_input_shape = (spy_x.shape[1], spy_x.shape[2])\n\n# NEW SIZES\nprint(\"train: \", spy_x.shape,\"-->\",spy_t_1.shape)\nprint(\"validation: \", spy_val_x.shape,\"-->\",spy_v_1.shape)\n\n```\n\n\n\n\n\n## Fit GRU \n\nNo regularlization\n\n```{python}\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_1_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=30, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n```\n\nLet's plot results\n\n```{python}\nhistory_plot(history_spy_1_noreg)\n\n```\n\nRegularlization\n\n```{python}\n\nfrom tensorflow.keras import regularizers\nL1=0\nL2=1e-3\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape,kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\n# layer that reads the recurent layer\nmodel.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5,kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_1_reg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=30, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n```\n\nLet's look at the diagnostic plot\n```{python}\n\nhistory_plot(history_spy_1_reg)\n```\n\n\nCompared to the model without regularization, we can see that the training set error decreases more steadily, without regressing higher very often. The consequence of this is that the raining error goes lower than the validation error faster than without regularization, suggesting regularization is helping the model to learn more purposefully and successfully.\n\n\n## Fit RNN \n\n```{python}\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.SimpleRNN(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_2_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n```\n\n\n```{python}\n\nhistory_plot(history_spy_2_noreg)\n```\n\nWe can see the validation loss of the simple RNN model is actually almost equivalent to both the regularized and non-regularized GRU models above. Specifically, all 3 models tended to initialize around 0.6 loss for the validation set, and then slowly work down to about 0.55. If anything, the RNN model might perform slightly better, with validation loss decreasing below 0.55 at the later-stage epochs.\n\n\n\n## Fit LSTM \n\n```{python}\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.LSTM(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\nhistory_spy_3_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n```\n\n```{python}\n\nhistory_plot(history_spy_3_noreg)\n\n```\n\nWith one LSTM layer, the model already seems better than the RNN and GRU competitiors. The validation loss starts out around 0.56 and stays in the lower end of the 0.55-0.6 range. Let's try adding another layer:\n\n```{python}\n#from sklearn.neural_network import MLPRegressor\n\n\n# create a sequential model once again\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu',input_shape=rnn_input_shape))\n# This layer is the recurent layer, which returns all previous data\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer= regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer = regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, recurrent_regularizer = regularizers.L1L2(l1=L1, l2=L2)))\n# layer that reads the recurent layer\nmodel.add(layers.Dense(32, activation='relu',kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2)))\nmodel.add(layers.Dense(1))\n#model.add(MLPRegressor())\n\nmodel.compile(optimizer=RMSprop(), loss='mae')\n\n\n#  trying diff shape\n\nhistory_spy_4_noreg = model.fit(x = spy_x, y = spy_y, steps_per_epoch=20, epochs=40, validation_data= (spy_val_x, spy_val_y), validation_steps=10, verbose = 0)\n\n\n```\n\n```{python}\n\nspy_4_t_pred = model.predict(spy_x)\nspy_4_t_pred = spy_4_t_pred\n\nspy_4_v_pred = model.predict(spy_val_x)\nspy_4_v_pred.shape\n\n\n```\n\n```{python}\nhistory_plot(history_spy_4_noreg)\n```\n\n\nWith a much larger model of 3x64 neuron LSTM layers, we can see that the validation loss is still around 55. However, the training loss has a consistent curve down to the validation loss. It is interesting that even in the later epochs, the validation loss remains lower than the training.\n\n\n## Comparing Results of Deep Learning Models \n\nOf the three neural network models, the LSTM provided the most consistent performance, while the RNN provided the best performance. The GRU model performed slightly worse than the other two, as measured by its validation set performance. Interestingly, the complexity required for the model to avoid underfitting was much higher with the LSTM model, where 3 hidden layers of 64 neurons each were needed before I felt the training plot showed excess training performance over validation performance. With smaller models, the LSTM performed somewhat poorly on the training set in comparison to the validation set. In terms of accuracy, the RNN model was notable for sometimes reaching below 0.55 loss in performance on the validation set, while the LSTM model usually reached around 0.56 and the GRU model between 0.56 and 0.6. In terms of predictive power (extrapolating outside the validation set) I would tend to trust the LSTM model, because it had the most consistent performance once it was trained over a number of epochs, and even boasted the lowest loss on a particular epoch, sometimes reaching as low as 0.51 in the validation set, although these results were not the norm.\n\nRegularization was an important element across the models. It had two main benefits: First, the models achieved better performance faster because they had more consistent improvement and less variation in performance between epochs. Models which included regularization (both L1L2 and dropout regularization) had almost monotonically decreasing loss in the training set, and clear trends toward improvement in the validation set.\n\nThe deep learning models will only have the performance described above when they predict 1 observation into the future. This is because they take in the past 30 observations and return 1 prediction. All of the loss estimates are for this 1-step ahead prediction. If we were to predict further into the future with the models, there would be drift or compound error, where the models loss from a previous prediction would be inherited in future predictions that relied upon that past prediction as an input into the model.\n\nThe deep learning modelling is more complicated than the univariate modeling from  HW3, because there are a range of hyperparameters introduced that make the process of selecting the best model more complicated. Unlike a simple ARIMA model, it is not possible to iterate through all combinations of hidden layer sizes and complexities. As such, I think there is more subjective elements introduced to neural network modeling.\n\n\n\n## Comparing Deep Learning and Traditional TS Models \n\n\nWe identified ARIMA(3,0,3) as a good model for the SPY data. Lets look at the one step ahead precition of the ARIMA and best-performing (LSTM) neural network model in the validation data:\n\n```{python}\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nmodelA = ARIMA(spy_train, order = (3,0,3))\nmodelA = modelA.fit()\n\nforecasts_arima = modelA.forecast(100)\n\npredict_arima = ARIMA(spy_val, order = (3,0,3))\npredict_arima = predict_arima.fit()\n\narimapreds = list()\n\n\n\nindexvals = pd.array(range(0,69))\narimavals = pd.array(predict_arima.get_prediction().predicted_mean)\narimavals = arimavals[0:69]\nnnvals = spy_4_v_pred[:,0,0]\n\n\nplt.plot(indexvals, arimavals, label='ARIMA Prediction')\nplt.plot(indexvals, nnvals, label='NN Prediction')\nplt.plot(indexvals, spy_val_y, label='Real Values')\nplt.legend()\nplt.title(\"One Step Ahead Forecasts in Validation (NN vs. ARIMA)\")\nplt.show()\n\nrealVals = spy_val_y\n\ndef rmse(pred, tar):\n    return np.sqrt(((pred - tar) ** 2).mean())\n\n\nprint(\"arima: \", rmse(arimavals, spy_val_y))\nprint(\"Neural Net: \", rmse(nnvals, spy_val_y))\n```\n\n\nLooking at a chart of our best-performing Neural Network Model (LSTM) vs. my best performing ARIMA model (3,0,3) from earlier work, vs. the real observed values, a few things become clear. One, neither model type does a perfect job, especially of capturing extremely high or low values. Two, the ARIMA predictions increase and decrease more (have a higher range) thant he NN predictions. However, the ARIMA predictions tend to vary further from 0, while the NN predictions stay close to zero and only alter slightly. Because of this, the RSME of the neural network is actually lower than the ARIMA model, with the neural network having 0.71 RMSE and the ARIMA model having 0.78 RMSE.\n\nHowever, the neural network is still not the best model I have tested in this project, because the VAR model achieved an RMSE of 0.68 on the same dataset. This suggests that while Neural networks are powerful tools for univariate prediction, the most accurate model predictions may still be those that include exogenous variables.\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["styles.css"],"embed-resources":true,"output-file":"Deep Learning for TS.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"sandstone","code-summary":"Show Code","title":"Deep Learning for TS"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}